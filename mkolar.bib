% Encoding: ISO-8859-1

@Article{Kim2019Two,
  author        = {Kim, Byol and Liu, Song and Kolar, Mladen},
  journal       = {Journal of the Royal Statistical Society.~Series B.},
  title         = {Two-sample inference for high-dimensional {M}arkov networks},
  year          = {2021},
  issn          = {1369-7412},
  number        = {5},
  pages         = {939--962},
  volume        = {83},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  doi           = {10.1111/rssb.12446},
  eprint        = {1905.00466},
  mrclass       = {62F40},
  mrnumber      = {4349123},
  primaryclass  = {stat.ME},
  url           = {https://doi.org/10.1111/rssb.12446},
}

@Article{Na2019Estimating,
  author      = {Na, S. and Kolar, M. and Koyejo, O.},
  journal     = {Biometrika},
  title       = {Estimating differential latent variable graphical models with applications to brain connectivity},
  year        = {2021},
  issn        = {0006-3444},
  number      = {2},
  pages       = {425--442},
  volume      = {108},
  author+an   = {1=std; 2=hl},
  doi         = {10.1093/biomet/asaa066},
  eprint      = {1909.05892},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  fjournal    = {Biometrika},
  mrclass     = {62H22 (92B20)},
  mrnumber    = {4259141},
  url         = {https://doi.org/10.1093/biomet/asaa066},
}

@Article{Dai2021Inference,
  author        = {Ran Dai and Mladen Kolar},
  journal       = {Electronic Journal of Statistics},
  title         = {Inference for high-dimensional varying-coefficient quantile regression},
  year          = {2021},
  issn          = {1935-7524},
  month         = dec,
  number        = {2},
  pages         = {5696-5757},
  volume        = {15},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=hl},
  doi           = {10.1214/21-EJS1919},
  eprint        = {2002.07370v1},
  file          = {:http\://arxiv.org/pdf/2002.07370v1:PDF},
  groups        = {MyPapers},
  primaryclass  = {stat.ME},
  sici          = {1935-7524(2021)15:2<5696:IFHDVC>2.0.CO;2-N},
}

@Article{Hanzely2021Personalized,
  author        = {Filip Hanzely and Boxin Zhao and Mladen Kolar},
  journal       = {Technical report},
  title         = {Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques},
  year          = {2021},
  month         = feb,
  abstract      = {We study the optimization aspects of personalized Federated Learning (FL). We develop a universal optimization theory applicable to all convex personalized FL models in the literature. In particular, we propose a general personalized objective capable of recovering essentially any existing personalized FL objective as a special case. We design several optimization techniques to minimize the general objective, namely a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. We demonstrate the practicality and/or optimality of our methods both in terms of communication and local computation. Lastly, we argue about the implications of our general optimization theory when applied to solve specific personalized FL objectives.},
  archiveprefix = {arXiv},
  author+an     = {1=f;2=f,std; 3=hl},
  eprint        = {2102.09743},
  file          = {:http\://arxiv.org/pdf/2102.09743v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Na2021Adaptive,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  title         = {An adaptive stochastic sequential quadratic programming with differentiable exact augmented lagrangians},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  date          = {2022-06},
  doi           = {10.1007/s10107-022-01846-z},
  eprint        = {2102.05320},
  file          = {:http\://arxiv.org/pdf/2102.05320v1:PDF},
  groups        = {MyPapers},
  journaltitle  = {Mathematical Programming},
  publisher     = {Springer Science and Business Media {LLC}},
}

@Article{Wang2021Robust,
  author        = {Y. Samuel Wang and Si Kai Lee and Panos Toulis and Mladen Kolar},
  journal       = {International Conference on Machine Learning (ICML)},
  title         = {Robust Inference for High-Dimensional Linear Models via Residual Randomization},
  year          = {2021},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 4=hl},
  eprint        = {2106.07717},
  file          = {:http\://arxiv.org/pdf/2106.07717v2:PDF},
  keywords      = {stat.ME, stat.ML},
  primaryclass  = {stat.ME},
  timestamp     = {Wed, 25 Aug 2021 17:11:17 +0200},
  url           = {http://proceedings.mlr.press/v139/wang21m.html},
}

@Article{Zhao2021High,
  author        = {Boxin Zhao and Shengjun Zhai and Y. Samuel Wang and Mladen Kolar},
  journal       = {Technical report},
  title         = {High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach},
  year          = {2021},
  month         = may,
  abstract      = {Undirected graphical models have been widely used to model the conditional independence structure of high-dimensional random vector data for years. In many modern applications such as EEG and fMRI data, the observations are multivariate random functions rather than scalars. To model the conditional independence of this type of data, functional graphical models are proposed and have attracted an increasing attention in recent years. In this paper, we propose a neighborhood selection approach to estimate Gaussian functional graphical models. We first estimate the neighborhood of all nodes via function-on-function regression, and then we can recover the whole graph structure based on the neighborhood information. By estimating conditional structure directly, we can circumvent the need of a well-defined precision operator which generally does not exist. Besides, we can better explore the effect of the choice of function basis for dimension reduction. We give a criterion for choosing the best function basis and motivate two practically useful choices, which we justified by both theory and experiments and show that they are better than expanding each function onto its own FPCA basis as in previous literature. In addition, the neighborhood selection approach is computationally more efficient than fglasso as it is more easy to do parallel computing. The statistical consistency of our proposed methods in high-dimensional setting are supported by both theory and experiment.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 3=std; 4=hl},
  eprint        = {2105.02487},
  file          = {:http\://arxiv.org/pdf/2105.02487v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, stat.ME},
  primaryclass  = {stat.ML},
}

@Article{Na2021Inequality,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  journal       = {Technical report},
  title         = {Inequality Constrained Stochastic Nonlinear Optimization via Active-Set Sequential Quadratic Programming},
  year          = {2021},
  month         = sep,
  abstract      = {We study nonlinear optimization problems with stochastic objective and deterministic equality and inequality constraints, which emerge in numerous applications including finance, manufacturing, power systems and, recently, deep neural networks. We propose an active-set stochastic sequential quadratic programming algorithm, using a differentiable exact augmented Lagrangian as the merit function. The algorithm adaptively selects the penalty parameters of augmented Lagrangian and performs stochastic line search to decide the stepsize. The global convergence is established: for any initialization, the "liminf" of the KKT residuals converges to zero almost surely. Our algorithm and analysis further develop the prior work \cite{Na2021Adaptive} by allowing nonlinear inequality constraints. We demonstrate the performance of the algorithm on a subset of nonlinear problems collected in the CUTEst test set.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {2109.11502},
  file          = {:http\://arxiv.org/pdf/2109.11502v1:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC, cs.LG, cs.NA, math.NA, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Na2021Fast,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  journal       = {Technical report},
  title         = {A Fast Temporal Decomposition Procedure for Long-horizon Nonlinear Dynamic Programming},
  year          = {2021},
  month         = jul,
  abstract      = {We propose a fast temporal decomposition procedure for solving long-horizon nonlinear dynamic programs. The core of the procedure is sequential quadratic programming (SQP), with a differentiable exact augmented Lagrangian being the merit function. Within each SQP iteration, we solve the Newton system approximately using an overlapping temporal decomposition. We show that the approximated search direction is still a descent direction of the augmented Lagrangian, provided the overlap size and penalty parameters are suitably chosen, which allows us to establish global convergence. Moreover, we show that a unit stepsize is accepted locally for the approximated search direction, and further establish a uniform, local linear convergence over stages. Our local convergence rate matches the rate of the recent Schwarz scheme \cite{Na2020Overlapping}. However, the Schwarz scheme has to solve nonlinear subproblems to optimality in each iteration, while we only perform one Newton step instead. Numerical experiments validate our theories and demonstrate the superiority of our method.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {2107.11560},
  file          = {:http\://arxiv.org/pdf/2107.11560v2:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC, math.DS},
  primaryclass  = {math.OC},
}

@Article{Liao2021Local,
  author        = {Luofeng Liao and Li Shen and Jia Duan and Mladen Kolar and Dacheng Tao},
  title         = {Local {AdaGrad}-type algorithm for stochastic convex-concave optimization},
  archiveprefix = {arXiv},
  author+an     = {1=std; 4=hl},
  date          = {2022-11},
  doi           = {10.1007/s10994-022-06239-z},
  eprint        = {2106.10022},
  file          = {:http\://arxiv.org/pdf/2106.10022v1:PDF},
  groups        = {MyPapers},
  journaltitle  = {Machine Learning},
  keywords      = {cs.LG, cs.DC, math.OC},
  primaryclass  = {cs.LG},
  publisher     = {Springer Science and Business Media {LLC}},
}

@Article{Tsai2021Joint,
  author        = {Katherine Tsai and Oluwasanmi Koyejo and Mladen Kolar},
  journal       = {{WIREs} Computational Statistics},
  title         = {Joint Gaussian Graphical Model Estimation: A Survey},
  year          = {2022},
  number        = {6},
  pages         = {e1582},
  volume        = {14},
  abstract      = {Abstract Graphs representing complex systems often share a partial underlying structure across domains while retaining individual features. Thus, identifying common structures can shed light on the underlying signal, for instance, when applied to scientific discovery or clinical diagnoses. Furthermore, growing evidence shows that the shared structure across domains boosts the estimation power of graphs, particularly for high-dimensional data. However, building a joint estimator to extract the common structure may be more complicated than it seems, most often due to data heterogeneity across sources. This manuscript surveys recent work on statistical inference of joint Gaussian graphical models, identifying model structures that fit various data generation processes. This article is categorized under: Data: Types and Structure > Graph and Network Data Statistical Models > Graphical Models},
  archiveprefix = {arXiv},
  author+an     = {3=hl},
  doi           = {10.1002/wics.1582},
  eprint        = {2110.10281},
  file          = {:http\://arxiv.org/pdf/2110.10281v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ME, cs.LG, stat.ML, Gaussian graphical model, graphical lasso, high-dimensional estimation, joint network, sparsity},
  primaryclass  = {stat.ME},
  url           = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1582},
}

@Article{Zhao2021Adaptive,
  author        = {Boxin Zhao and Ziqi Liu and Chaochao Chen and Mladen Kolar and Zhiqiang Zhang and Jun Zhou},
  journal       = {Technical report},
  title         = {Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback},
  year          = {2021},
  month         = dec,
  abstract      = {In federated learning (FL) problems, client sampling plays a key role in the convergence speed of training algorithm. However, while being an important problem in FL, client sampling is lack of study. In this paper, we propose an online learning with bandit feedback framework to understand the client sampling problem in FL. By adapting an Online Stochastic Mirror Descent algorithm to minimize the variance of gradient estimation, we propose a new adaptive client sampling algorithm. Besides, we use online ensemble method and doubling trick to automatically choose the tuning parameters in the algorithm. Theoretically, we show dynamic regret bound with comparator as the theoretically optimal sampling sequence; we also include the total variation of this sequence in our upper bound, which is a natural measure of the intrinsic difficulty of the problem. To the best of our knowledge, these theoretical contributions are novel to existing literature. Moreover, by implementing both synthetic and real data experiments, we show empirical evidence of the advantages of our proposed algorithms over widely-used uniform sampling and also other online learning based sampling strategies in previous studies. We also examine its robustness to the choice of tuning parameters. Finally, we discuss its possible extension to sampling without replacement and personalized FL objective. While the original goal is to solve client sampling problem, this work has more general applications on stochastic gradient descent and stochastic coordinate descent methods.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 4=hl},
  eprint        = {2112.14332},
  file          = {:http\://arxiv.org/pdf/2112.14332v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@InProceedings{Luo2022Dynamic,
  author     = {Yuwei Luo and Varun Gupta and Mladen Kolar},
  title      = {Dynamic Regret Minimization for Control of Non-stationary Linear Dynamical Systems},
  year       = {2022},
  pages      = {1--72},
  publisher  = {{ACM}},
  author+an  = {1=std; 3=hl},
  date       = {2022-06},
  doi        = {10.1145/3489048.3522649},
  eprint     = {2111.03772},
  eprinttype = {arXiv},
  journal    = {{ACM} {SIGMETRICS}},
}

@Article{Yu2019Simultaneous,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {Journal of Machine Learning Research},
  title         = {Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching},
  year          = {2020},
  issn          = {1532-4435},
  number        = {91},
  pages         = {1-51},
  volume        = {21},
  abstract      = {Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is $\sqrt{n}$-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {1905.06261},
  groups        = {MyPapers},
  mrclass       = {62H22},
  mrnumber      = {4119159},
  primaryclass  = {stat.ME},
  url           = {http://jmlr.org/papers/v21/19-383.html},
}

@Article{Yu2018Recovery,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {Electronic Journal of Statistics},
  title         = {Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach},
  year          = {2020},
  issn          = {1935-7524},
  number        = {1},
  pages         = {413-457},
  volume        = {14},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  doi           = {10.1214/19-EJS1658},
  eprint        = {1802.06967},
  primaryclass  = {stat.ML},
  sici          = {1935-7524(2020)14:1<413:ROSLRA>2.0.CO;2-B},
  timestamp     = {2020.01.22},
}

@Article{Yu2017Estimation,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {Journal of Machine Learning Research},
  title         = {Estimation of a Low-rank Topic-Based Model for Information Cascades},
  year          = {2020},
  number        = {71},
  pages         = {1-47},
  volume        = {21},
  abstract      = {We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or {\it cascades}. Here for a given cascade, we only observe the times of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades and prove an analytical upper bound on the estimation error. The estimated model can be used to build recommendation systems based on the receptivity vectors, as well as for marketing based on the influence vectors. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {1709.01919v2},
  file          = {:http\://arxiv.org/pdf/1709.01919v2:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, cs.SI},
  primaryclass  = {stat.ML},
  url           = {http://jmlr.org/papers/v21/19-496.html},
}

@Article{Zhao2020FuDGE,
  author        = {Boxin Zhao and Y. Samuel Wang and Mladen Kolar},
  journal       = {Journal of Machine Learning Research},
  title         = {FuDGE: A Method to Estimate a Functional Differential Graph in a High-Dimensional Setting},
  year          = {2022},
  number        = {82},
  pages         = {1--82},
  volume        = {23},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 3=hl},
  eprint        = {2003.05402v1},
  primaryclass  = {stat.ML},
  url           = {http://jmlr.org/papers/v23/20-231.html},
}

@Article{Zhang2020Posterior,
  author        = {Yulong Zhang and Mingxuan Yi and Song Liu and Mladen Kolar},
  journal       = {Technical report},
  title         = {Posterior Ratio Estimation for Latent Variables},
  year          = {2020},
  month         = feb,
  abstract      = {Density Ratio Estimation has attracted attention from machine learning community due to its ability of comparing the underlying distributions of two datasets. However, in some applications, we want to compare distributions of \emph{latent} random variables that can be only inferred from observations. In this paper, we study the problem of estimating the ratio between two posterior probability density functions of a latent variable. Particularly, we assume the posterior ratio function can be well-approximated by a parametric model, which is then estimated using observed datasets and synthetic prior samples. We prove consistency of our estimator and the asymptotic normality of the estimated parameters as the number of prior samples tending to infinity. Finally, we validate our theories using numerical experiments and demonstrate the usefulness of the proposed method through some real-world applications.},
  archiveprefix = {arXiv},
  author+an     = {4=hl},
  eprint        = {2002.06410v1},
  file          = {:http\://arxiv.org/pdf/2002.06410v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  timestamp     = {2020.03.13},
}

@Article{Yu2019Constrained,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {arXiv:1911.07319},
  title         = {Constrained High Dimensional Statistical Inference},
  year          = {2020},
  month         = nov,
  abstract      = {In typical high dimensional statistical inference problems, confidence intervals and hypothesis tests are performed for a low dimensional subset of model parameters under the assumption that the parameters of interest are unconstrained. However, in many problems, there are natural constraints on model parameters and one is interested in whether the parameters are on the boundary of the constraint or not. e.g. non-negativity constraints for transmission rates in network diffusion. In this paper, we provide algorithms to solve this problem of hypothesis testing in high-dimensional statistical models under constrained parameter space. We show that following our testing procedure we are able to get asymptotic designed Type I error under the null. Numerical experiments demonstrate that our algorithm has greater power than the standard algorithms where the constraints are ignored. We demonstrate the effectiveness of our algorithms on two real datasets where we have {\emph{intrinsic}} constraint on the parameters.},
  archiveprefix = {arXiv},
  eprint        = {1911.07319v1},
  file          = {:http\://arxiv.org/pdf/1911.07319v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ME},
  primaryclass  = {stat.ME},
  timestamp     = {2020.03.13},
}

@Article{Na2020Semiparametric,
  author        = {Sen Na and Yuwei Luo and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
  journal       = {International Conference on Machine Learning (ICML)},
  title         = {Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees},
  year          = {2020},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 5=hl},
  eprint        = {2003.01013},
  groups        = {MyPapers},
  pdf           = {http://proceedings.mlr.press/v119/na20a/na20a.pdf},
  primaryclass  = {stat.ML},
  timestamp     = {2021.03.01},
  url           = {http://proceedings.mlr.press/v119/na20a.html},
}

@Article{Wang2020Statistical,
  author        = {Xu Wang and Mladen Kolar and Ali Shojaie},
  journal       = {Technical report},
  title         = {Statistical Inference for Networks of High-Dimensional Point Processes},
  year          = {2020},
  month         = jul,
  abstract      = {Fueled in part by recent applications in neuroscience, the multivariate Hawkes process has become a popular tool for modeling the network of interactions among high-dimensional point process data. While evaluating the uncertainty of the network estimates is critical in scientific applications, existing methodological and theoretical work has primarily addressed estimation. To bridge this gap, this paper develops a new statistical inference procedure for high-dimensional Hawkes processes. The key ingredient for this inference procedure is a new concentration inequality on the first- and second-order statistics for integrated stochastic processes, which summarize the entire history of the process. Combining recent results on martingale central limit theory with the new concentration inequality, we then characterize the convergence rate of the test statistics. We illustrate finite sample validity of our inferential tools via extensive simulations and demonstrate their utility by applying them to a neuron spike train data set.},
  archiveprefix = {arXiv},
  author+an     = {2=hl},
  eprint        = {2007.07448v1},
  file          = {:http\://arxiv.org/pdf/2007.07448v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Liao2020Provably,
  author        = {Luofeng Liao and You{-}Lin Chen and Zhuoran Yang and Bo Dai and Mladen Kolar and Zhaoran Wang},
  journal       = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {Provably Efficient Neural Estimation of Structural Equation Models: An Adversarial Approach},
  year          = {2020},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 5=hl},
  eprint        = {2007.01290},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  url           = {https://proceedings.neurips.cc/paper/2020/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html},
}

@Article{Tsai2020Nonconvex,
  author        = {Katherine Tsai and Mladen Kolar and Oluwasanmi Koyejo},
  journal       = {Journal of Machine Learning Research},
  title         = {A Nonconvex Framework for Structured Dynamic Covariance Recovery},
  year          = {2022},
  number        = {200},
  pages         = {1--91},
  volume        = {23},
  archiveprefix = {arXiv},
  author+an     = {2=hl},
  eprint        = {2011.05601},
  file          = {:https://www.jmlr.org/papers/volume23/21-0795/21-0795.pdf:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, stat.AP, stat.ME},
  primaryclass  = {stat.ML},
  timestamp     = {2021.03.01},
  url           = {http://jmlr.org/papers/v23/21-0795.html},
}

@Article{Geng2018Joint,
  author    = {Sinong Geng and Mladen Kolar and Oluwasanmi Koyejo},
  journal   = {Uncertainty in Artificial Intelligence (UAI)},
  title     = {Joint Nonparametric Precision Matrix Estimation with Confounding},
  year      = {2019},
  author+an = {1=std; 2=hl},
}

@Article{Na2018High,
  author        = {Sen Na and Mladen Kolar},
  journal       = {Bernoulli},
  title         = {High-dimensional index volatility models via {S}tein's identity},
  year          = {2021},
  issn          = {1350-7265},
  number        = {2},
  pages         = {794-817},
  volume        = {27},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=hl},
  doi           = {10.3150/20-BEJ1238},
  eprint        = {1811.10790},
  file          = {:http\://arxiv.org/pdf/1811.10790v2:PDF},
  fjournal      = {Bernoulli},
  groups        = {MyPapers},
  primaryclass  = {math.ST},
  sici          = {1350-7265(2021)27:2<794:HDIVMV>2.0.CO;2-X},
}

@Article{Liao2021Instrumental,
  author        = {Luofeng Liao and Zuyue Fu and Zhuoran Yang and Mladen Kolar and Zhaoran Wang},
  journal       = {Technical report},
  title         = {Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning},
  year          = {2021},
  month         = feb,
  abstract      = {In offline reinforcement learning (RL) an optimal policy is learnt solely from a priori collected observational data. However, in observational data, actions are often confounded by unobserved variables. Instrumental variables (IVs), in the context of RL, are the variables whose influence on the state variables are all mediated through the action. When a valid instrument is present, we can recover the confounded transition dynamics through observational data. We study a confounded Markov decision process where the transition dynamics admit an additive nonlinear functional form. Using IVs, we derive a conditional moment restriction (CMR) through which we can identify transition dynamics based on observational data. We propose a provably efficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual reformulation of CMR. To the best of our knowledge, this is the first provably efficient algorithm for instrument-aided offline RL.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 4=hl},
  eprint        = {2102.09907},
  file          = {:http\://arxiv.org/pdf/2102.09907v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Chen2019Tensor,
  author        = {Chen, You-Lin and Kolar, Mladen and Tsay, Ruey S.},
  journal       = {Journal of Computational and Graphical Statistics},
  title         = {Tensor {C}anonical {C}orrelation {A}nalysis {W}ith {C}onvergence and {S}tatistical {G}uarantees},
  year          = {2021},
  issn          = {1061-8600},
  number        = {3},
  pages         = {728--744},
  volume        = {30},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=hl},
  doi           = {10.1080/10618600.2020.1856118},
  eprint        = {1906.05358},
  mrclass       = {62H20 (90C26)},
  mrnumber      = {4313472},
  primaryclass  = {stat.ML},
  url           = {https://doi.org/10.1080/10618600.2020.1856118},
}

@Article{Chen2020Convergence,
  author        = {You-Lin Chen and Sen Na and Mladen Kolar},
  journal       = {Technical report},
  title         = {Convergence Analysis of Accelerated Stochastic Gradient Descent under the Growth Condition},
  year          = {2020},
  month         = jun,
  abstract      = {We study the convergence of accelerated stochastic gradient descent for strongly convex objectives under the growth condition, which states that the variance of stochastic gradient is bounded by a multiplicative part that grows with the full gradient, and a constant additive part. Through the lens of the growth condition, we investigate four widely used accelerated methods: Nesterov's accelerated method (NAM), robust momentum method (RMM), accelerated dual averaging method (ADAM), and implicit ADAM (iADAM). While these methods are known to improve the convergence rate of SGD under the condition that the stochastic gradient has bounded variance, it is not well understood how their convergence rates are affected by the multiplicative noise. In this paper, we show that these methods all converge to a neighborhood of the optimum with accelerated convergence rates (compared to SGD) even under the growth condition. In particular, NAM, RMM, iADAM enjoy acceleration only with a mild multiplicative noise, while ADAM enjoys acceleration even with a large multiplicative noise. Furthermore, we propose a generic tail-averaged scheme that allows the accelerated rates of ADAM and iADAM to nearly attain the theoretical lower bound (up to a logarithmic factor in the variance term).},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 3=hl},
  eprint        = {2006.06782},
  file          = {:http\://arxiv.org/pdf/2006.06782v3:PDF},
  keywords      = {math.OC},
  primaryclass  = {math.OC},
}

@Article{Luo2019Natural,
  author        = {Yuwei Luo and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
  journal       = {Technical report},
  title         = {Natural Actor-Critic Converges Globally for Hierarchical Linear Quadratic Regulator},
  year          = {2019},
  month         = dec,
  abstract      = {Multi-agent reinforcement learning has been successfully applied to a number of challenging problems. Despite these empirical successes, theoretical understanding of different algorithms is lacking, primarily due to the curse of dimensionality caused by the exponential growth of the state-action space with the number of agents. We study a fundamental problem of multi-agent linear quadratic regulator in a setting where the agents are partially exchangeable. In this setting, we develop a hierarchical actor-critic algorithm, whose computational complexity is independent of the total number of agents, and prove its global linear convergence to the optimal policy. As linear quadratic regulators are often used to approximate general dynamic systems, this paper provided an important step towards better understanding of general hierarchical mean-field multi-agent reinforcement learning.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 4=hl},
  eprint        = {1912.06875v1},
  file          = {:http\://arxiv.org/pdf/1912.06875v1:PDF},
  keywords      = {cs.LG, math.OC, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Lu2019Kernel,
  author        = {Lu, Junwei and Kolar, Mladen and Liu, Han},
  journal       = {Journal of the American Statistical Association},
  title         = {Kernel meets sieve: post-regularization confidence bands for sparse additive model},
  year          = {2020},
  issn          = {0162-1459},
  number        = {532},
  pages         = {2084--2099},
  volume        = {115},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=hl},
  doi           = {10.1080/01621459.2019.1689984},
  eprint        = {1503.02978},
  mrclass       = {62G15 (62G08 62G20)},
  mrnumber      = {4189778},
  primaryclass  = {stat.ML},
  url           = {https://doi.org/10.1080/01621459.2019.1689984},
}

@Article{Zhao2019Direct,
  author        = {Boxin Zhao and Y. Samuel Wang and Mladen Kolar},
  journal       = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {Direct Estimation of Differential Functional Graphical Models},
  year          = {2019},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 3=hl},
  eprint        = {1910.09701},
  primaryclass  = {stat.ML},
}

@Article{Yu2019Convergent,
  author        = {Yu, Ming and Yang, Zhuoran and Kolar, Mladen and Wang, Zhaoran},
  journal       = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {Convergent Policy Optimization for Safe Reinforcement Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {1910.12156},
  primaryclass  = {cs.LG},
}

@Article{Na2019High,
  author    = {Sen Na and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
  journal   = {Journal of Machine Learning Research},
  title     = {High-dimensional Varying Index Coefficient Models via Stein's Identity},
  year      = {2019},
  number    = {152},
  pages     = {1-44},
  volume    = {20},
  author+an = {1=std; 4=hl},
  groups    = {MyPapers},
  url       = {http://jmlr.org/papers/v20/18-705.html},
}

@Article{Yu2019Learning,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal   = {International Conference on Artificial Intelligence and Statistics (AISTATS) 2019},
  title     = {Learning Influence-Receptivity Network Structure with Guarantee},
  year      = {2019},
  author+an = {1=std; 3=hl},
}

@Article{Geng2019Partially,
  author    = {Sinong Geng and Minhao Yan and Mladen Kolar and Oluwasanmi Koyejo},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Partially Linear Additive Gaussian Graphical Models},
  year      = {2019},
  author+an = {1=std; 3=hl},
}

@Article{Yu2018Provable,
  author    = {Ming Yu and Zhuoran Yang and Tuo Zhao and Mladen Kolar and Zhaoran Wang},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Provable Gaussian Embedding with One Observation},
  year      = {2018},
  author+an = {1=std; 4=hl},
}

@Article{Barber2015ROCKET,
  author    = {Rina Foygel Barber and Mladen Kolar},
  journal   = {Annals of Statistics},
  title     = {ROCKET: Robust confidence intervals via Kendall's tau for transelliptical graphical models},
  year      = {2018},
  issn      = {0090-5364},
  number    = {6B},
  pages     = {3422-3450},
  volume    = {46},
  author+an = {1=ab; 2=ab,hl},
  doi       = {10.1214/17-AOS1663},
  sici      = {0090-5364(2018)46:6B<3422:RURBUI>2.0.CO;2-J},
  timestamp = {2018.09.11},
}

@Article{Lu2015Posta,
  author    = {Junwei Lu and Mladen Kolar and Han Liu},
  journal   = {Journal of Machine Learning Research},
  title     = {Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models},
  year      = {2018},
  number    = {203},
  pages     = {1-78},
  volume    = {18},
  author+an = {1=std; 2=hl},
  groups    = {MyPapers},
  url       = {http://jmlr.org/papers/v18/17-145.html},
}

@Article{Wang2017Sketching,
  author    = {Wang, Jialei and Lee, Jason D. and Mahdavi, Mehrdad and Kolar, Mladen and Srebro, Nathan},
  journal   = {Electronic Journal of Statistics},
  title     = {Sketching meets random projection in the dual: a provable recovery algorithm for big and high-dimensional data},
  year      = {2017},
  number    = {2},
  pages     = {4896--4944},
  volume    = {11},
  author+an = {1=std; 4=hl},
  doi       = {10.1214/17-EJS1334SI},
  mrclass   = {62H12 (68T05 90C06)},
  mrnumber  = {3738201},
  url       = {https://doi.org/10.1214/17-EJS1334SI},
}

@Article{Suggala2017Expxorcist,
  author    = {Arun Sai Suggala and Mladen Kolar and Pradeep Ravikumar},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {{The Expxorcist}: Nonparametric Graphical Models Via Conditional Exponential Densities},
  year      = {2017},
  author+an = {2=hl},
}

@Article{Yu2017Influence,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal   = {{IEEE} International Conference on Data Mining (ICDM)},
  title     = {An Influence-Receptivity Model for Topic based Information Cascades},
  year      = {2017},
  author+an = {1=std; 3=hl},
}

@Article{Balakrishnan2012Recovering,
  author    = {Sivaraman Balakrishnan and Mladen Kolar and Alessandro Rinaldo and Aarti Singh},
  journal   = {Electronic Journal of Statistics},
  title     = {Recovering block-structured activations using compressive measurements},
  year      = {2017},
  issn      = {1935-7524},
  number    = {1},
  pages     = {2647-2678},
  volume    = {11},
  author+an = {2=hl},
  doi       = {10.1214/17-EJS1267},
  sici      = {1935-7524(2017)11:1<2647:RBSAUC>2.0.CO;2-N},
}

@Article{Wang2016Efficient,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srebro and Tong Zhang},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Efficient Distributed Learning with Sparsity},
  year      = {2017},
  author+an = {1=std; 2=hl},
}

@Article{Wang2016Sketching,
  author    = {Jialei Wang and Jason Lee and Mehrdad Mahdavi and Mladen Kolar and Nati Srebro},
  journal   = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {{Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data}},
  year      = {2017},
  author+an = {1=std; 4=hl},
}

@Article{Yu2016Statistical,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Statistical Inference for Pairwise Graphical Models Using Score Matching},
  year      = {2016},
  author+an = {1=std; 3=hl},
}

@Article{Kolar2016Discussion,
  author    = {Kolar, Mladen and Taddy, Matt},
  journal   = {The Annals of Applied Statistics},
  title     = {Discussion of ``{C}oauthorship and citation networks for statisticians'' [ {MR}3592033]},
  year      = {2016},
  issn      = {1932-6157},
  number    = {4},
  pages     = {1835--1841},
  volume    = {10},
  author+an = {1=ab,hl; 2=ab},
  doi       = {10.1214/16-AOAS896D},
  mrclass   = {91D30 (01A80 05C90 62P25)},
  mrnumber  = {3592037},
  url       = {https://doi.org/10.1214/16-AOAS896D},
}

@Article{Wang2016Inference,
  author    = {Jialei Wang and Mladen Kolar},
  journal   = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Inference for High-dimensional Exponential Family Graphical Models},
  year      = {2016},
  author+an = {1=std; 2=hl},
}

@Article{Wang2015Distributed,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srerbo},
  journal   = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Distributed Multi-Task Learning},
  year      = {2016},
  author+an = {1=std; 2=hl},
}

@Article{Sun2015Learning,
  author    = {Siqi Sun and Mladen Kolar and Jinbo Xu},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS},
  title     = {Learning structured densities via infinite dimensional exponential families},
  year      = {2015},
  author+an = {1=std; 2=hl},
}

@Article{Gaynanova2014Optimal,
  author     = {Irina Gaynanova and Mladen Kolar},
  journal    = {Electronic Journal of Statistics},
  title      = {Optimal variable selection in multi-group sparse discriminant analysis},
  year       = {2015},
  issn       = {1935-7524},
  number     = {2},
  pages      = {2007--2034},
  volume     = {9},
  author+an  = {1=ab; 2=ab,hl},
  doi        = {10.1214/15-EJS1064},
  groups     = {MyPapers},
  mrclass    = {62H30 (62J07 68Q32)},
  mrnumber   = {3393602},
  mrreviewer = {Changyi Park},
  url        = {http://dx.doi.org/10.1214/15-EJS1064},
}

@Article{kolar13road,
  author    = {Mladen Kolar and Han Liu},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Feature Selection in High-Dimensional Classification},
  year      = {2013},
  author+an = {1=hl,ab; 2=ab},
}

@Article{Kolar2013Optimal,
  author     = {Kolar, Mladen and Liu, Han},
  journal    = {IEEE Transactions on Information Theory},
  title      = {Optimal feature selection in high-dimensional discriminant analysis},
  year       = {2015},
  issn       = {0018-9448},
  number     = {2},
  pages      = {1063--1083},
  volume     = {61},
  author+an  = {1=ab,hl; 2=ab},
  doi        = {10.1109/TIT.2014.2381241},
  mrclass    = {62H30 (62J07 94A15)},
  mrnumber   = {3332765},
  mrreviewer = {Yanlin Tang},
  url        = {https://doi.org/10.1109/TIT.2014.2381241},
}

@Article{Wasserman2014Berry,
  author    = {Wasserman, Larry and Kolar, Mladen and Rinaldo, Alessandro},
  journal   = {Electronic Journal of Statistics},
  title     = {Berry-{E}sseen bounds for estimating undirected graphs},
  year      = {2014},
  number    = {1},
  pages     = {1188--1224},
  volume    = {8},
  author+an = {2=hl},
  doi       = {10.1214/14-EJS928},
  mrclass   = {62H12 (62G15)},
  mrnumber  = {3263117},
  url       = {https://doi.org/10.1214/14-EJS928},
}

@Article{Kolar2014Graph,
  author     = {Kolar, Mladen and Liu, Han and Xing, Eric P.},
  journal    = {Journal of Machine Learning Research (JMLR)},
  title      = {Graph estimation from multi-attribute data},
  year       = {2014},
  issn       = {1532-4435},
  pages      = {1713--1750},
  volume     = {15},
  author+an  = {1=hl},
  mrclass    = {62H12 (62H99)},
  mrnumber   = {3225245},
  mrreviewer = {Subhajit Dutta},
}

@Article{kolar13multiatticml,
  author    = {Mladen Kolar and Han Liu and Eric Xing},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Markov Network Estimation From Multi-attribute Data},
  year      = {2013},
  author+an = {1=hl},
}

@Article{kolar10estimating,
  author    = {Kolar, Mladen and Xing, Eric P.},
  journal   = {Electronic Journal of Statistics},
  title     = {Estimating networks with jumps},
  year      = {2012},
  pages     = {2069--2106},
  volume    = {6},
  author+an = {1=hl},
  doi       = {10.1214/12-EJS739},
  mrclass   = {62G05 (05C90 62-09 62G20 62M10 91D30)},
  mrnumber  = {3020257},
  url       = {https://doi.org/10.1214/12-EJS739},
}

@Article{Kolar2012Variance,
  author    = {Mladen Kolar and James Sharpnack},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Variance Function Estimation in High-dimensions},
  year      = {2012},
  author+an = {1=hl,ab; 2=ab},
}

@Article{Kolar2012Consistent,
  author    = {Mladen Kolar and Eric P. Xing},
  journal   = {International Conference on Machine Learning (ICML)},
  title     = {Consistent Covariance Selection From Data With Missing Values},
  year      = {2012},
  author+an = {1=hl},
}

@Article{kolar2012marginal,
  author    = {Mladen Kolar and Han Liu},
  journal   = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Marginal Regression For Multitask Learning},
  year      = {2012},
  author+an = {1=hl,ab; 2=ab},
}

@Article{balakrishnan2011statistical,
  author    = {Balakrishnan, Sivaraman and Kolar, Mladen and Rinaldo, Alessandro and Singh, Aarti and Wasserman, Larry},
  journal   = {NeurIPS 2011 Workshop on Computational Trade-offs in Statistical Learning},
  title     = {Statistical and computational tradeoffs in biclustering},
  year      = {2011},
  author+an = {2=hl},
}

@Article{Kolar2011Minimax,
  author    = {Mladen Kolar and Sivaraman Balakrishnan and Alessandro Rinaldo and Aarti Singh},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Minimax Localization of Structural Information in Large Noisy Matrices},
  year      = {2011},
  author+an = {1=hl},
}

@Article{kolar2011time,
  author    = {Mladen Kolar and Eric P. Xing},
  journal   = {International Conference on Artificial Intelligence and Statistics ({AISTATS})},
  title     = {On Time Varying Undirected Graphs},
  year      = {2011},
  author+an = {1=hl},
}

@Article{kolar11union,
  author    = {Kolar, Mladen and Lafferty, John and Wasserman, Larry},
  journal   = {Journal of Machine Learning Research (JMLR)},
  title     = {Union support recovery in multi-task learning},
  year      = {2011},
  issn      = {1532-4435},
  pages     = {2415--2435},
  volume    = {12},
  author+an = {1=hl},
  file      = {:kolar11union.pdf:PDF},
  mrclass   = {62J07 (62G08 62H12)},
  mrnumber  = {2825433},
}

@Article{kolar10nonparametric,
  author    = {Mladen Kolar and Ankur P. Parikh and Eric P. Xing},
  journal   = {International Conference on Machine Learning ({ICML})},
  title     = {On Sparse Nonparametric Conditional Covariance Selection},
  year      = {2010},
  author+an = {1=hl},
}

@Article{kolar10ultrahigh,
  author    = {Mladen Kolar and Eric P. Xing},
  journal   = {International Conference on Artificial Intelligence and Statistics ({AISTATS})},
  title     = {Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach},
  year      = {2010},
  author+an = {1=hl},
}

@Article{song09time,
  author    = {Le Song and Mladen Kolar and Eric P. Xing},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Time-Varying Dynamic Bayesian Networks},
  year      = {2009},
  author+an = {2=hl},
}

@Article{kolar09nips_tv_paper,
  author    = {Mladen Kolar and Le Song and Eric P. Xing},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Sparsistent Learning of Varying-coefficient Models with Structural Changes},
  year      = {2009},
  author+an = {1=hl},
}

@Article{Song2009KELLER,
  author    = {Le Song and Mladen Kolar and Eric P. Xing},
  journal   = {Bioinformatics},
  title     = {{KELLER:} estimating time-varying interactions between genes},
  year      = {2009},
  number    = {12},
  volume    = {25},
  author+an = {2=hl},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/bioinformatics/SongKX09.bib},
  doi       = {10.1093/bioinformatics/btp192},
  timestamp = {Mon, 02 Mar 2020 16:27:17 +0100},
  url       = {https://doi.org/10.1093/bioinformatics/btp192},
}

@Article{Kolar2010Estimating,
  author    = {Kolar, Mladen and Song, Le and Ahmed, Amr and Xing, Eric P.},
  journal   = {The Annals of Applied Statistics},
  title     = {Estimating time-varying networks},
  year      = {2010},
  issn      = {1932-6157},
  number    = {1},
  pages     = {94--123},
  volume    = {4},
  author+an = {1=hl},
  doi       = {10.1214/09-AOAS308},
  mrclass   = {Expansion},
  mrnumber  = {2758086},
  url       = {https://doi.org/10.1214/09-AOAS308},
}

@Article{kolar2008time,
  author    = {Kolar, Mladen and Xing, Eric P.},
  journal   = {NeurIPS 2008 Workshop on Analyzing Graphs: Theory and Applications},
  title     = {Time Varying Ising Models},
  year      = {2008},
  author+an = {1=hl},
}

@Article{Ray2008CSMET,
  author    = {Pradipta Ray and Suyash Shringarpure and Mladen Kolar and Eric P. Xing},
  journal   = {{PLoS} Computational Biology},
  title     = {{CSMET}: Comparative Genomic Motif Detection via Multi-Resolution Phylogenetic Shadowing},
  year      = {2008},
  month     = {6},
  number    = {6},
  pages     = {e1000090},
  volume    = {4},
  author+an = {3=hl},
  doi       = {10.1371/journal.pcbi.1000090},
  editor    = {Uwe Ohler},
  publisher = {Public Library of Science ({PLoS})},
}

@Article{Petrovic2006Comparison,
  author    = {Sasa Petrovic and Jan Snajder and Bojana Dalbelo Basic and Mladen Kolar},
  journal   = {Journal of Computing and Information Technology},
  title     = {Comparison of Collocation Extraction Measures for Document Indexing},
  year      = {2006},
  number    = {4},
  pages     = {321--327},
  volume    = {14},
  author+an = {4=hl},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/cit/PetrovicSBK06.bib},
  timestamp = {Tue, 14 Jul 2020 14:30:07 +0200},
  url       = {http://cit.srce.hr/index.php/CIT/article/view/1617},
}

@Article{Kolar2005Computer,
  author    = {Kolar, Mladen and Vukmirovi{\'c}, Igor and Dalbelo Ba{\v{s}}i{\'c}, Bojana and {\v{S}}najder, Jan},
  journal   = {Journal of computing and information technology},
  title     = {Computer-aided document indexing system},
  year      = {2005},
  number    = {4},
  pages     = {299--305},
  volume    = {13},
  author+an = {1=hl},
  publisher = {SRCE-Sveu{\v{c}}ili{\v{s}}ni ra{\v{c}}unski centar},
}

@Article{Zhao2022LSVRG,
  author        = {Boxin Zhao and Boxiang Lyu and Mladen Kolar},
  journal       = {Technical report},
  title         = {L-SVRG and L-Katyusha with Adaptive Sampling},
  year          = {2022},
  month         = jan,
  abstract      = {Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha [12], are widely used to train machine learning models. Theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling the observations from a non-uniform distribution [17]. However, to design a desired sampling distribution, Qian et al.[17] rely on prior knowledge of smoothness constants that can be computationally intractable to obtain in practice when the dimension of the model parameter is high. We propose an adaptive sampling strategy for L-SVRG and L-Katyusha that learns the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge on the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. These results show that even without prior information, the proposed adaptive sampling strategy matches, and in some cases even surpasses, the performance of the sampling scheme in Qian et al.[17]. Extensive simulations support our theory and the practical utility of the proposed sampling scheme on real data.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 2=std; 3=hl},
  eprint        = {2201.13387},
  file          = {:http\://arxiv.org/pdf/2201.13387v1:PDF},
  keywords      = {cs.LG, math.OC},
  primaryclass  = {cs.LG},
}

@Article{CisnerosVelarde2022One,
  author        = {Pedro Cisneros-Velarde and Boxiang Lyu and Sanmi Koyejo and Mladen Kolar},
  journal       = {Technical report},
  title         = {One Policy is Enough: Parallel Exploration with a Single Policy is Minimax Optimal for Reward-Free Reinforcement Learning},
  year          = {2022},
  month         = may,
  abstract      = {While parallelism has been extensively used in Reinforcement Learning (RL), the quantitative effects of parallel exploration are not well understood theoretically. We study the benefits of simple parallel exploration for reward-free RL for linear Markov decision processes (MDPs) and two-player zero-sum Markov games (MGs). In contrast to the existing literature focused on approaches that encourage agents to explore over a diverse set of policies, we show that using a single policy to guide exploration across all agents is sufficient to obtain an almost-linear speedup in all cases compared to their fully sequential counterpart. Further, we show that this simple procedure is minimax optimal up to logarithmic factors in the reward-free setting for both linear MDPs and two-player zero-sum MGs. From a practical perspective, our paper shows that a single policy is sufficient and provably optimal for incorporating parallelism during the exploration phase.},
  archiveprefix = {arXiv},
  author+an     = {1=f; 2=std,f; 4=hl},
  eprint        = {2205.15891},
  file          = {:CisnerosVelarde2022One.pdf:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Lyu2022Pessimism,
  author        = {Boxiang Lyu and Zhaoran Wang and Mladen Kolar and Zhuoran Yang},
  journal       = {International Conference on Machine Learning (ICML)},
  title         = {Pessimism meets VCG: Learning Dynamic Mechanism Design via Offline Reinforcement Learning},
  year          = {2022},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {2205.02450},
  file          = {:http\://arxiv.org/pdf/2205.02450v1:PDF},
  keywords      = {cs.LG, cs.GT, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Lyu2022Personalized,
  author        = {Boxiang Lyu and Filip Hanzely and Mladen Kolar},
  journal       = {Technical report (arXiv:2204.13619)},
  title         = {Personalized Federated Learning with Multiple Known Clusters},
  year          = {2022},
  month         = apr,
  abstract      = {We consider the problem of personalized federated learning when there are known cluster structures within users. An intuitive approach would be to regularize the parameters so that users in the same cluster share similar model weights. The distances between the clusters can then be regularized to reflect the similarity between different clusters of users. We develop an algorithm that allows each cluster to communicate independently and derive the convergence results. We study a hierarchical linear model to theoretically demonstrate that our approach outperforms agents learning independently and agents learning a single shared weight. Finally, we demonstrate the advantages of our approach using both simulated and real-world data.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  eprint        = {2204.13619},
  file          = {:http\://arxiv.org/pdf/2204.13619v1:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Dettling2022Lasso,
  author        = {Philipp Dettling and Mathias Drton and Mladen Kolar},
  journal       = {Technical report},
  title         = {On the Lasso for Graphical Continuous Lyapunov Models},
  year          = {2022},
  month         = aug,
  abstract      = {Graphical continuous Lyapunov models offer a new perspective on modeling causally interpretable dependence structure in multivariate data by treating each independent observation as a one-time cross-sectional snapshot of a temporal process. Specifically, the models assume the observations to be cross-sections of independent multivariate Ornstein-Uhlenbeck processes in equilibrium. The Gaussian equilibrium exists under a stability assumption on the drift matrix, and the equilibrium covariance matrix is determined by the continuous Lyapunov equation. Each graphical continuous Lyapunov model assumes the drift matrix to be sparse with a support determined by a directed graph. A natural approach to model selection in this setting is to use an $\ell_1$-regularization approach that seeks to find a sparse approximate solution to the Lyapunov equation when given a sample covariance matrix. We study the model selection properties of the resulting lasso technique by applying the primal-dual witness technique for support recovery. Our analysis uses special spectral properties of the Hessian of the considered loss function in order to arrive at a consistency result. While the lasso technique is able to recover useful structure, our results also demonstrate that the relevant irrepresentability condition may be violated in subtle ways, preventing perfect recovery even in seemingly favorable settings.},
  archiveprefix = {arXiv},
  author+an     = {1=ab; 2=ab; 3=ab,hl},
  eprint        = {2208.13572},
  file          = {:http\://arxiv.org/pdf/2208.13572v1:PDF},
  keywords      = {math.ST, stat.TH, 62H22, 62H12},
  primaryclass  = {math.ST},
}

@Article{Tsai2022Latent,
  author        = {Katherine Tsai and Boxin Zhao and Oluwasanmi Koyejo and Mladen Kolar},
  journal       = {Technical report},
  title         = {Latent Multimodal Functional Graphical Model Estimation},
  year          = {2022},
  month         = oct,
  abstract      = {Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is provably efficient, with the estimator converging to a stationary point with quantifiable statistical error. Furthermore, we show recovery of the latent graph under mild conditions. Our work is applied to analyze simultaneously acquired multimodal brain imaging data where the graph indicates functional connectivity of the brain. We present simulation and empirical results that support the benefits of joint estimation.},
  archiveprefix = {arXiv},
  author+an     = {2=std; 4=hl},
  eprint        = {2210.17237},
  file          = {:http\://arxiv.org/pdf/2210.17237v1:PDF},
  keywords      = {stat.ME, cs.LG, stat.ML},
  primaryclass  = {stat.ME},
}

@Article{Chen2020Provably,
  author        = {You-Lin Chen and Zhaoran Wang and Mladen Kolar},
  journal       = {Electronic Journal of Statistics},
  title         = {{Provably training overparameterized neural network classifiers with non-convex constraints}},
  year          = {2022},
  number        = {2},
  pages         = {5812 -- 5851},
  volume        = {16},
  abstract      = {Training a classifier under non-convex constraints has gotten increasing attention in the machine learning community thanks to its wide range of applications such as algorithmic fairness and class-imbalanced classification. However, several recent works addressing non-convex constraints have only focused on simple models such as logistic regression or support vector machines. Neural networks, one of the most popular models for classification nowadays, are precluded and lack theoretical guarantees. In this work, we show that overparameterized neural networks could achieve a near-optimal and near-feasible solution of non-convex constrained optimization problems via the project stochastic gradient descent. Our key ingredient is the no-regret analysis of online learning for neural networks in the overparameterization regime, which may be of independent interest in online learning applications.},
  archiveprefix = {arXiv},
  author+an     = {1=std; 3=hl},
  doi           = {10.1214/22-EJS2036},
  eprint        = {2012.15274},
  file          = {:http\://arxiv.org/pdf/2012.15274v1:PDF},
  keywords      = {stat.ML, cs.LG, math.OC},
  primaryclass  = {stat.ML},
  publisher     = {Institute of Mathematical Statistics and Bernoulli Society},
  url           = {https://doi.org/10.1214/22-EJS2036},
}

@Article{Qiu2020Gradient,
  author      = {Shuang Qiu and Xiaohan Wei and Mladen Kolar},
  journal     = {AAAI Conference on Artificial Intelligence (to appear)},
  title       = {Gradient-Variation Bound for Online Convex Optimization with Constraints},
  abstract    = {We study online convex optimization with constraints consisting of multiple functional constraints and a relatively simple constraint set, such as a Euclidean ball. As enforcing the constraints at each time step through projections is computationally challenging in general, we allow decisions to violate the functional constraints but aim to achieve a low regret and cumulative violation of the constraints over a horizon of $T$ time steps. First-order methods achieve an $\mathcal{O}(\sqrt{T})$ regret and an $\mathcal{O}(1)$ constraint violation, which is the best-known bound, but do not take into account the structural information of the problem. Furthermore, the existing algorithms and analysis are limited to Euclidean space. In this paper, we provide an \emph{instance-dependent} bound for online convex optimization with complex constraints obtained by a novel online primal-dual mirror-prox algorithm. Our instance-dependent regret is quantified by the total gradient variation $V_*(T)$ in the sequence of loss functions. The proposed algorithm works in \emph{general} non-Euclidean spaces and simultaneously achieves an $\mathcal{O}(\sqrt{V_*(T)})$ regret and an $\mathcal{O}(1)$ constraint violation, which is never worse than the best-known $( \mathcal{O}(\sqrt{T}), \mathcal{O}(1) )$ result and improves over previous works that applied mirror-prox-type algorithms for this problem achieving $\mathcal{O}(T^{2/3})$ regret and constraint violation. Finally, our algorithm is computationally efficient, as it only performs mirror descent steps in each iteration instead of solving a general Lagrangian minimization problem.},
  author+an   = {1=std; 3=hl},
  date        = {2022-11},
  eprint      = {2006.12455},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/2006.12455v2:PDF},
  keywords    = {math.OC, cs.LG, stat.ML},
}

@Article{Fang2022FullyStochasticTrust,
  author       = {Yuchen Fang and Sen Na and Michael W. Mahoney and Mladen Kolar},
  title        = {Fully Stochastic Trust-Region Sequential Quadratic Programming for Equality-Constrained Optimization Problems},
  abstract     = {We propose a trust-region stochastic sequential quadratic programming algorithm (TR-StoSQP) to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. We consider a fully stochastic setting, where in each iteration a single sample is generated to estimate the objective gradient. The algorithm adaptively selects the trust-region radius and, compared to the existing line-search StoSQP schemes, allows us to employ indefinite Hessian matrices (i.e., Hessians without modification) in SQP subproblems. As a trust-region method for constrained optimization, our algorithm needs to address an infeasibility issue -- the linearized equality constraints and trust-region constraints might lead to infeasible SQP subproblems. In this regard, we propose an \textit{adaptive relaxation technique} to compute the trial step that consists of a normal step and a tangential step. To control the lengths of the two steps, we adaptively decompose the trust-region radius into two segments based on the proportions of the feasibility and optimality residuals to the full KKT residual. The normal step has a closed form, while the tangential step is solved from a trust-region subproblem, to which a solution ensuring the Cauchy reduction is sufficient for our study. We establish the global almost sure convergence guarantee for TR-StoSQP, and illustrate its empirical performance on both a subset of problems in the CUTEst test set and constrained logistic regression problems using data from the LIBSVM collection.},
  author+an    = {1=std; 4=hl},
  date         = {2022-11-29},
  eprint       = {2211.15943},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/2211.15943v1:PDF},
  journaltitle = {Technical report},
  keywords     = {math.OC, stat.CO, stat.ML},
}

@Comment{jabref-meta: databaseType:bibtex;}
