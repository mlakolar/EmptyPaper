% Encoding: UTF-8

@String { aahmed       = {Amr Ahmed} }
@String { abuja        = {Andreas Buja} }
@String { acha         = {Applied and Computational Harmonic Analysis} }
@String { acha_s       = {Appl. Comput. Harmon. Anal.} }
@String { adalalyan    = {Arnak S. Dalalyan} }
@String { adawid       = {Alexander Philip Dawid} }
@String { aism_s       = {Ann. Inst. Statist. Math.} }
@String { alozano      = {Aur\'elie C. Lozano} }
@String { anobel       = {Andrew B. Nobel} }
@String { aoas         = { Annals of Applied Statistics } }
@String { aoas_s       = { Ann. Appl. Stat. } }
@String { aop          = { Annals of Probability } }
@String { aop_s        = { Ann. Probab. } }
@String { aos          = { Annals of Statistics } }
@String { aos_s        = { Ann. Stat. } }
@String { arothman     = { Adam J. Rothman } }
@String { atsybakov    = { Alexandre B. Tsybakov } }
@String { avdvaart     = {Aad W. {van der Vaart}} }
@String { awillsky     = {Alan S. Willsky} }

@String { bcaffo       = {Brian S. Caffo} }
@String { befron       = { Bradley Efron } }
@String { bpotscher    = {Benedikt M. P\"otscher} }
@String { byu          = { Bin Yu } }
@String { chansen      = {Christian B. Hansen} }
@String{chzhang      = {Cun-Hui Zhang}}
@String { csda         = {Computational Statistics \& Data Analysis} }
@String { csda_s       = {Comput. Stat. Data Anal.} }
@String { ddonoho      = { David L. Donoho } }
@String { dhusmeier    = { Dirk Husmeier } }
@String { dmason       = {David M. Mason} }
@String { druppert     = { David Ruppert } }
@String { dwitten      = { Daniela M. Witten } }
@String { eacastro     = {Ery Arias-Castro} }
@String { ecandes      = {Emmanuel J. Cand\`{e}s} }

@String { ecp          = {Electronic Communications in Probability} }
@String { ecp_s        = {Electron. Commun. Probab.} }
@String { egine        = {Evarist Gin\'{e} } }
@String { ejs          = { Electronic Journal of Statistics } }
@String { ejs_s        = { Electron. J. Stat. } }
@String { elevina      = { Elizaveta Levina } }
@String { epxing       = { Eric P. Xing } }
@String { fbach        = {Francis R. Bach} }
@String { fdondelinger = { Frank Dondelinger } }
@String { gallen       = {Genevera I. Allen} }
@String { graskutti    = { Garvesh Raskutti } }
@String { hhzhang      = {Hao Helen Zhang} }
@String { hleeb        = {Hannes Leeb} }
@String { hliu         = { Han Liu } }
@String { hwang        = { Hansheng Wang } }
@String { hzhou        = { Harrison H. Zhou } }
@String { hzou         = { Hui Zou } }
@String { ICML2010_s   = {27th Int. Conf. Mach. Learn.} }
@String { ICML2012_s   = {29th Int. Conf. Mach. Learn.} }
@String { ICML2013_s   = {30th Int. Conf. Mach. Learn.} }
@String { ICML2014_s   = {31st Int. Conf. Mach. Learn.} }
@String { IEEEit       = { IEEE Transactions on Information Theory } }
@String { IEEEit_s     = { IEEE Trans. Inf. Theory } }
@String { IEEEspm_s    = {IEEE Signal Process. Mag.} }
@String { ijohnstone   = { Iain M. Johnstone } }
@String { jasa         = {Journal of the American Statistical Association } }
@String { jasa_s       = { J. Am. Stat. Assoc. } }
@String { jbes_s       = {J. Bus. Econom. Statist.} }
@String { jcgs         = {Journal of Computational and Graphical Statistics} }
@String { jcgs_s       = {J. Comp. Graph. Stat.} }
@String { jdlee        = {Jason D. Lee} }


@String { jduchi       = { John C. Duchi } }
@String { jfan         = { Jianqing Fan } }
@String { jfried       = { Jerome H. Friedman } }
@String { jhaupt       = {Jarvis D. Haupt} }
@String { jhorowitz    = { Joel L. Horowitz } }
@String { jhuang       = {Jian Huang} }
@String{jjankova     = {Jana Jankov\'a}}
@String { jlafferty    = { John D. Lafferty } }
@String { jlv          = { Jinchi Lv } }
@String { jma          = {Journal of Multivariate Analysis} }
@String { jma_s        = {J. Multivar. Anal.} }
@String { jmlr         = { Journal Of Machine Learning Research } }
@String { jmlr_s       = { J. Mach. Learn. Res. } }
@String { jrssb        = { Journal of the Royal Statistical Society: Series B } }
@String { jrssb_s      = { J. R. Stat. Soc. B } }
@String { jspi_s       = {J. Statist. Plann. Inference} }
@String { jtaylor      = {Jonathan E. Taylor} }
@String { jtropp       = { Joel A. Tropp } }
@String { jwellner     = {Jon A. Wellner} }
@String { jzhu         = { Ji Zhu } }
@String { klange       = {Kenneth L. Lange} }

@String { klounici     = {Karim Lounici } }
@String { lbrown       = {Lawrence D. Brown} }
@String { lsong        = { Le Song } }
@String { lwasser      = { Larry~A. Wasserman } }
@String { machlearn    = { Machine Learning } }
@String { machlearn_s  = { Mach. Learn. } }@String { mbalcan      = {Maria-Florina Balcan} }

@String { mdavenport   = {Mark A. Davenport} }
@String { mdrton       = { Mathias Drton } }
@String { mfarrell     = {Max H. Farrell} }
@String { mgrzegorczyk = { Marco Grzegorczyk } }
@String { mjordan      = { Michael I. Jordan } }
@String { mkolar       = { Mladen Kolar } }
@String { mmalloy      = {Matthew L. Malloy } }
@String { mperlman     = { Michael D. Perlman } }
@String { mwainw       = { Martin J. Wainwright } }
@String { mwakin       = {Michael B. Wakin} }
@String { myuan        = { Ming Yuan } }
@String { NIPS_s       = {Adv. Neural Inf. Process. Syst.} }
@String { nmeins       = { Nicolas Meinshausen } }
@String { nsimon       = {Noah Simon} }@String { nsrebro      = {Nathan Srebro} }
@String { pbartlett    = {Peter L. Bartlett} }

@String { pbickel      = { Peter J. Bickel } }
@String { pbuhl        = { Peter B\"{u}hlmann} }
@String { phall        = {Peter Hall} }
@String { phoff        = {Peter Hoff} }
@String { ploh         = {Po-Ling Loh} }
@String { pnas         = { Proceedings of the National Academy of Sciences of the United States of America } }
@String { pnas_s       = { Proc. Natl. Acad. Sci. U.S.A. } }
@String { pravik       = { Pradeep Ravikumar } }
@String { PROC_s       = {Proc.} }
@String { ptrf         = {Probability Theory and Related Fields} }
@String { ptrf_s       = {Probab. Theory Related Fields} }
@String { rbaraniuk    = {Richard G. Baraniuk} }
@String { rcastro      = {Rui M. Castro} }@String { rdevore      = {Ronald DeVore} }
@String { rfoygel      = {Rina {Foygel Barber}} }

@String { rli          = { Runze Li } }
@String { rmazumder    = { Rahul Mazumder } }
@String { rnowak       = {Robert D. Nowak} }
@String { rsamworth    = {Richard J. Samworth} }
@String { rtibs        = {Robert J. Tibshirani } }
@String { sboyd        = { Stephen P. Boyd } }
@String { sigproc      = { IEEE Transactions on Signal Processing } }
@String { sigproc_s    = { IEEE Trans. Signal Proces. } }
@String { sjs          = {Scandinavian Journal of Statistics} }
@String { sjs_s        = {Scand. J. Stat.} }
@String { slauritzen   = {Steffen L. Lauritzen} }
@String { slebre       = { Sophie L\'{e}bre } }
@String { snegahban    = {Sahand Negahban} }
@String { sportnoy     = {Stephen L. Portnoy} }
@String { statcomp     = { Statistics and Computing } }
@String { statcomp_s   = { Stat. Comput. } }
@String { statprobl    = { Statistics $\&$ Probability Letters } }
@String { statprobl_s  = { Statist. Probab. Lett. } }
@String { statsci      = {Statistical Science} }
@String { statsci_s    = {Stat. Sci.} }
@String { statsin      = { Statistica Sinica } }
@String { statsin_s    = { Stat. Sinica } }
@String { svdgeer      = { Sara A. van de Geer } }
@String { tams         = { The Annals of Mathematical Statistics } }
@String { tams_s       = { Ann. Math. Stat. } }
@String { tcai         = { T. Tony Cai } }
@String { thastie      = { Trevor J. Hastie } }
@String { tzhang       = {Tong Zhang} }
@String { vdlpena      = {Victor {de la Pena}} }
@String { vkoltch      = {Vladimir Koltchinskii} }
@String { wbwu         = { Wei Biao Wu } }
@String { whardle      = {Wolfgang H{\"a}rdle} }
@String { wliu         = {Weidong Liu} }
@String { yingster     = {Yuri I. Ingster} }

@Article{Luo2019Natural,
  author      = {Yuwei Luo and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
  journal     = {arXiv:1912.06875},
  title       = {Natural Actor-Critic Converges Globally for Hierarchical Linear Quadratic Regulator},
  year        = {2019},
  abstract    = {Multi-agent reinforcement learning has been successfully applied to a number of challenging problems. Despite these empirical successes, theoretical understanding of different algorithms is lacking, primarily due to the curse of dimensionality caused by the exponential growth of the state-action space with the number of agents. We study a fundamental problem of multi-agent linear quadratic regulator in a setting where the agents are partially exchangeable. In this setting, we develop a hierarchical actor-critic algorithm, whose computational complexity is independent of the total number of agents, and prove its global linear convergence to the optimal policy. As linear quadratic regulators are often used to approximate general dynamic systems, this paper provided an important step towards better understanding of general hierarchical mean-field multi-agent reinforcement learning.},
  date        = {2019-12-14},
  eprint      = {1912.06875v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1912.06875v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Article{Chen2019Tensor,
  author      = {You-Lin Chen and Mladen Kolar and Ruey S. Tsay},
  journal     = {arXiv:1906.05358},
  title       = {Tensor Canonical Correlation Analysis},
  year        = {2019},
  abstract    = {In many applications, such as classification of images or videos, it is of interest to develop a framework for tensor data instead of ad-hoc way of transforming data to vectors due to the computational and under-sampling issues. In this paper, we study canonical correlation analysis by extending the framework of two dimensional analysis (Lee and Choi, 2007) to tensor-valued data. Instead of adopting the iterative algorithm provided in Lee and Choi (2007), we propose an efficient algorithm, called the higher-order power method, which is commonly used in tensor decomposition and more efficient for large-scale setting. Moreover, we carefully examine theoretical properties of our algorithm and establish a local convergence property via the theory of Lojasiewicz's inequalities. Our results fill a missing, but crucial, part in the literature on tensor data. For practical applications, we further develop (a) an inexact updating scheme which allows us to use the state-of-the-art stochastic gradient descent algorithm, (b) an effective initialization scheme which alleviates the problem of local optimum in non-convex optimization, and (c) an extension for extracting several canonical components. Empirical analyses on challenging data including gene expression, air pollution indexes in Taiwan, and electricity demand in Australia, show the effectiveness and efficiency of the proposed methodology.},
  date        = {2019-06-12},
  eprint      = {1906.05358v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1906.05358v2:PDF},
  keywords    = {stat.ML, cs.LG, stat.ME},
}

@Article{Yu2019Constrained,
  author      = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal     = {arXiv:1911.07319},
  title       = {Constrained High Dimensional Statistical Inference},
  year        = {2019},
  abstract    = {In typical high dimensional statistical inference problems, confidence intervals and hypothesis tests are performed for a low dimensional subset of model parameters under the assumption that the parameters of interest are unconstrained. However, in many problems, there are natural constraints on model parameters and one is interested in whether the parameters are on the boundary of the constraint or not. e.g. non-negativity constraints for transmission rates in network diffusion. In this paper, we provide algorithms to solve this problem of hypothesis testing in high-dimensional statistical models under constrained parameter space. We show that following our testing procedure we are able to get asymptotic designed Type I error under the null. Numerical experiments demonstrate that our algorithm has greater power than the standard algorithms where the constraints are ignored. We demonstrate the effectiveness of our algorithms on two real datasets where we have {\emph{intrinsic}} constraint on the parameters.},
  date        = {2019-11-17},
  eprint      = {1911.07319v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1911.07319v1:PDF},
  keywords    = {stat.ME},
}

@Article{Yu2019Simultaneous,
  author      = {Ming Yu and Varun Gupta and Mladen Kolar},
  title       = {Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching},
  journal     = {arXiv 1905.06261},
  year        = {2019},
  abstract    = {Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyv\"arinen scoring rule. Hyv\"arinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is $\sqrt{n}$-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.},
  date        = {2019-05-15},
  eprint      = {http://arxiv.org/abs/1905.06261v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1905.06261v1:PDF},
  keywords    = {stat.ME},
}

@Article{Na2018High,
  author      = {Sen Na and Mladen Kolar},
  journal     = {arXiv:1811.10790},
  title       = {High-dimensional Index Volatility Models via Stein's Identity},
  year        = {2018},
  abstract    = {We study estimation of the parametric components of single and multiple index volatility models. Using the first- and second-order Stein's identity, we develop methods that are applicable for estimation of the variance index in a high-dimensional setting requiring finite moment condition, which allows for heavy-tailed data. Our approach complements the existing literature in a low-dimensional setting, while relaxing the conditions on estimation, and provides a novel approach in a high-dimensional setting. We prove that the statistical rate of convergence of our variance index estimators consists of a parametric rate and a nonparametric rate, where the latter appears from the estimation of the mean link function. However, under standard assumptions, the parametric rate dominates the rate of convergence and our results match the minimax optimal rate for the mean index estimation. Simulation results illustrate finite sample properties of our methodology and back our theoretical conclusions.},
  date        = {2018-11-27},
  eprint      = {1811.10790v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1811.10790v2:PDF},
  keywords    = {math.ST, cs.LG, stat.ML, stat.TH},
}

@Article{Na2017Scalable,
  author      = {Sen Na and Mingyuan Ma and Mladen Kolar},
  journal     = {arXiv:1711.04955},
  title       = {Scalable Peaceman-Rachford Splitting Method with Proximal Terms},
  year        = {2017},
  abstract    = {Along with developing of Peaceman-Rachford Splittling Method (PRSM), many batch algorithms based on it have been studied very deeply. But almost no algorithm focused on the performance of stochastic version of PRSM. In this paper, we propose a new stochastic algorithm based on PRSM, prove its convergence rate in ergodic sense, and test its performance on both artificial and real data. We show that our proposed algorithm, Stochastic Scalable PRSM (SS-PRSM), enjoys the $O(1/K)$ convergence rate, which is the same as those newest stochastic algorithms that based on ADMM but faster than general Stochastic ADMM (which is $O(1/\sqrt{K})$). Our algorithm also owns wide flexibility, outperforms many state-of-the-art stochastic algorithms coming from ADMM, and has low memory cost in large-scale splitting optimization problems.},
  date        = {2017-11-14},
  eprint      = {1711.04955v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1711.04955v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Wang2018Distributed,
  author      = {Weiran Wang and Jialei Wang and Mladen Kolar and Nathan Srebro},
  journal     = {arXiv:1802.03830},
  title       = {Distributed Stochastic Multi-Task Learning with Graph Regularization},
  year        = {2018},
  abstract    = {We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.},
  date        = {2018-02-11},
  eprint      = {1802.03830v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1802.03830v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Wang2016Distributed,
  author         = {Jialei Wang and Mladen Kolar and Nathan Srebro},
  title          = {Distributed Multi-Task Learning with Shared Representation},
  journal        = {Technical report},
  year           = {2016},
  month          = mar,
  abstract       = {We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.},
  eprint         = {1603.02185},
  oai2identifier = {1603.02185},
}

@Article{Bradic2017Uniform,
  author  = {Bradic, Jelena and Kolar, Mladen},
  title   = {Uniform inference for high-dimensional quantile regression: linear functionals and regression rank scores},
  journal = {arXiv preprint arXiv:1702.06209},
  year    = {2017},
}

@Article{Zhao2014General,
  Title                    = {A General Framework for Robust Testing and Confidence Regions in High-Dimensional Quantile Regression},
  Author                   = {Tianqi Zhao and } # mkolar # { and } # hliu,
  Journal                  = {ArXiv e-prints, arXiv:1412.8724},
  Year                     = {2014},

  Month                    = dec,

  Abstract                 = {We propose a robust inference procedure for high-dimensional linear models, where the dimension $p$ could grow exponentially fast with the sample size $n$. Our method applies the de-biasing technique together with the composite quantile loss function to construct an estimator that weakly converges to a Normal distribution. This estimator can be used to construct uniformly valid confidence intervals and conduct hypothesis tests. Our estimator is robust and does not require the existence of first or second moment of the noise distribution. It also preserves efficiency when the second moment does exist, in the sense that the maximum loss in efficiency is less than 30\% compared to the square-loss-based de-biased Lasso estimator. In many cases our estimator is close to or better than the de-biased Lasso estimator, especially when the noise distribution is heavy-tailed. Although the result is a high-dimensional counterpart of the composite quantile regression (CQR) introduced in Zou and Yuan (2008), our procedure does not require solving the $L_1$-penalized CQR in high dimensions. Instead, we allow any first-stage estimator that has a desired convergence rate and empirical sparsity. Furthermore, we consider a high-dimensional simultaneous test for the regression parameter by applying Gaussian approximation and multiplier bootstrap theories. Lastly we study distributed learning and exploit the divide-and-conquer estimator to reduce computation complexity when the sample size is massive. Our de-biasing method does not require the precision matrix corresponding to the design to be sparse, which is commonly required for constructing confidence intervals in high-dimensional linear models. Finally, our theories are empirically tested on synthetic data.},
  Comments                 = {72 pages, 2 figures, 3 tables},
  Eprint                   = {1412.8724},
  Oai2identifier           = {1412.8724},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@Article{Wang2014Inference,
  Title                    = {Inference for Sparse Conditional Precision Matrices},
  Author                   = {Jialei Wang and } # mkolar,
  Journal                  = {ArXiv e-prints, arXiv:1412.7638},
  Year                     = {2014},

  Month                    = dec,

  Abstract                 = {Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is a high-dimensional vector and $Z$ is a low-dimensional index variable, we study the problem of estimating the conditional inverse covariance matrix $\Omega(z) = (E[(X-E[X \mid Z])(X-E[X \mid Z])^T \mid Z=z])^{-1}$ under the assumption that the set of non-zero elements is small and does not depend on the index variable. We develop a novel procedure that combines the ideas of the local constant smoothing and the group Lasso for estimating the conditional inverse covariance matrix. A proximal iterative smoothing algorithm is used to solve the corresponding convex optimization problems. We prove that our procedure recovers the conditional independence assumptions of the distribution $X \mid Z$ with high probability. This result is established by developing a uniform deviation bound for the high-dimensional conditional covariance matrix from its population counterpart, which may be of independent interest. Furthermore, we develop point-wise confidence intervals for individual elements of the conditional inverse covariance matrix. We perform extensive simulation studies, in which we demonstrate that our proposed procedure outperforms sensible competitors. We illustrate our proposal on a S&P 500 stock price data set.},
  Eprint                   = {1412.7638},
  Oai2identifier           = {1412.7638},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@Article{Sharpnack2014Mean,
  Title                    = {Mean and variance estimation in high-dimensional heteroscedastic models with non-convex penalties},
  Author                   = {James Sharpnack and } # mkolar,
  Journal                  = {ArXiv e-prints, arXiv:1410.7874},
  Year                     = {2014},

  Month                    = oct,

  Abstract                 = {Despite its prevalence in statistical datasets, heteroscedasticity (non-constant sample variances) has been largely ignored in the high-dimensional statistics literature. Recently, studies have shown that the Lasso can accommodate heteroscedastic errors, with minor algorithmic modifications (Belloni et al., 2012; Gautier and Tsybakov, 2013). In this work, we study heteroscedastic regression with linear mean model and log-linear variances model with sparse high-dimensional parameters. In this work, we propose estimating variances in a post-Lasso fashion, which is followed by weighted-least squares mean estimation. These steps employ non-convex penalties as in Fan and Li (2001), which allows us to prove oracle properties for both post-Lasso variance and mean parameter estimates. We reinforce our theoretical findings with experiments.},
  Eprint                   = {1410.7874},
  Oai2identifier           = {1410.7874},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@Article{kolar09sparsistent,
  Title                    = {Sparsistent Estimation Of Time-varying Discrete Markov Random Fields},
  Author                   = mkolar # { and } # epxing,
  Journal                  = {ArXiv e-prints, arXiv:0907.2337},
  Year                     = {2009},

  Month                    = {July},

  Keywords                 = {Statistics - Machine Learning},
  Monthno                  = {7},
  Newspaper                = {ArXiv e-prints, arXiv:0907.2337},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Yu2017Estimation,
  author      = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal     = {To appear in JMLR},
  title       = {Estimation of a Low-rank Topic-Based Model for Information Cascades},
  year        = {2020},
  abstract    = {We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or {\it cascades}. Here for a given cascade, we only observe the times of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades and prove an analytical upper bound on the estimation error. The estimated model can be used to build recommendation systems based on the receptivity vectors, as well as for marketing based on the influence vectors. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.},
  date        = {2017-09-06},
  eprint      = {1709.01919v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1709.01919v2:PDF},
  keywords    = {stat.ML, cs.LG, cs.SI},
}

@InCollection{Zhao2019Direct,
  author    = {Zhao, Boxin and Wang, Y. Samuel and Kolar, Mladen},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {Direct Estimation of Differential Functional Graphical Models},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {2571--2581},
  url       = {http://papers.nips.cc/paper/8526-direct-estimation-of-differential-functional-graphical-models.pdf},
}

@InCollection{Yu2019Convergent,
  author    = {Yu, Ming and Yang, Zhuoran and Kolar, Mladen and Wang, Zhaoran},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {Convergent Policy Optimization for Safe Reinforcement Learning},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {3121--3133},
  url       = {http://papers.nips.cc/paper/8576-convergent-policy-optimization-for-safe-reinforcement-learning.pdf},
}

@Article{Yu2018Recovery,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal   = {Electron. J. Statist.},
  title     = {Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach},
  year      = {2020},
  issn      = {1935-7524},
  number    = {1},
  pages     = {413-457},
  volume    = {14},
  doi       = {10.1214/19-EJS1658},
  fjournal  = {Electronic Journal of Statistics},
  sici      = {1935-7524(2020)14:1<413:ROSLRA>2.0.CO;2-B},
  timestamp = {2020.01.22},
}

@Article{Lu2019Kernel,
  author    = {Junwei Lu and Mladen Kolar and Han Liu},
  journal   = {Journal of the American Statistical Association},
  title     = {Kernel Meets Sieve: Post-Regularization Confidence Bands for Sparse Additive Model},
  year      = {2019},
  month     = {nov},
  pages     = {1--40},
  doi       = {10.1080/01621459.2019.1689984},
  publisher = {Informa {UK} Limited},
}

@Article{Na2019High,
  author  = {Sen Na and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
  journal = {Journal of Machine Learning Research},
  title   = {High-dimensional Varying Index Coefficient Models via Stein's Identity},
  year    = {2019},
  number  = {152},
  pages   = {1-44},
  volume  = {20},
  url     = {http://jmlr.org/papers/v20/18-705.html},
}

@InProceedings{Yu2018Learning,
  author    = {Yu, Ming and Gupta, Varun and Kolar, Mladen},
  title     = {Learning Influence-Receptivity Network Structure with Guarantee},
  booktitle = {Proceedings of Machine Learning Research},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1476--1485},
  month     = {16--18 Apr},
  publisher = {PMLR},
  abstract  = {Traditional works on community detection from observations of information cascade assume that a single adjacency matrix parametrizes all the observed cascades.  However, in reality the connection structure usually does not stay the same across cascades.  For example, different people have different topics of interest, therefore the connection structure depends on the information/topic content of the cascade.  In this paper we consider the case where we observe a sequence of noisy adjacency matrices triggered by information/events with different topic distributions.  We propose a novel latent model using the intuition that a connection is more likely to exist between two nodes if they are interested in similar topics, which are common with the information/event.  Specifically, we endow each node with two node-topic vectors: an influence vector that measures how influential/authoritative they are on each topic; and a receptivity vector that measures how receptive/susceptible they are to each topic.  We show how these two node-topic structures can be estimated from observed adjacency matrices with theoretical guarantee on estimation error, in cases where the topic distributions of the information/events are known, as well as when they are unknown. Experiments on synthetic and real data demonstrate the effectiveness of our model and superior performance compared to state-of-the-art methods.},
  file      = {yu19c.pdf:http\://proceedings.mlr.press/v89/yu19c/yu19c.pdf:PDF},
  timestamp = {2019.05.16},
  url       = {http://proceedings.mlr.press/v89/yu19c.html},
}

@InProceedings{Geng2019Partially,
  author    = {Geng, Sinong and Yan, Minhao and Kolar, Mladen and Koyejo, Sanmi},
  title     = {Partially Linear Additive {G}aussian Graphical Models},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2180--2190},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  abstract  = {We propose a partially linear additive Gaussian graphical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model parameters are estimated using an $L_1$-regularized maximal pseudo-profile likelihood estimator (MaPPLE) for which we prove a $\sqrt{n}$-sparsistency. Importantly, our approach avoids parametric constraints on the effects of confounders on the estimated graphical model structure. Empirically, the PLA-GGM is applied to both synthetic and real-world datasets, demonstrating superior performance compared to competing methods.},
  file      = {geng19a.pdf:http\://proceedings.mlr.press/v97/geng19a/geng19a.pdf:PDF},
  timestamp = {2019.06.05},
  url       = {http://proceedings.mlr.press/v97/geng19a.html},
}

@Article{Geng2018Joint,
  author        = {Sinong Geng and Mladen Kolar and Oluwasanmi Koyejo},
  title         = {Joint Nonparametric Precision Matrix Estimation with Confounding},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1810.07147},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-07147},
  eprint        = {1810.07147},
  timestamp     = {2019.06.05},
  url           = {http://arxiv.org/abs/1810.07147},
}

@InProceedings{Yu2018Provable,
  author    = {Ming Yu and Zhuoran Yang and Tuo Zhao and Mladen Kolar and Zhaoran Wang},
  title     = {Provable Gaussian Embedding with One Observation},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al, Canada.},
  year      = {2018},
  editor    = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  pages     = {6765--6775},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/YuYZKW18},
  timestamp = {Sun, 16 Dec 2018 17:30:05 +0100},
  url       = {http://papers.nips.cc/paper/7910-provable-gaussian-embedding-with-one-observation},
}

@Article{Barber2015ROCKET,
  author    = {Rina Foygel Barber and Mladen Kolar},
  title     = {ROCKET: Robust confidence intervals via Kendall's tau for transelliptical graphical models},
  journal   = {Ann. Statist.},
  year      = {2018},
  volume    = {46},
  number    = {6B},
  pages     = {3422-3450},
  issn      = {0090-5364},
  doi       = {10.1214/17-AOS1663},
  fjournal  = {Annals of Statistics},
  sici      = {0090-5364(2018)46:6B<3422:RURBUI>2.0.CO;2-J},
  timestamp = {2018.09.11},
}

@Article{Lu2015Posta,
  author  = {Junwei Lu and Mladen Kolar and Han Liu},
  title   = {Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {203},
  pages   = {1-78},
  url     = {http://jmlr.org/papers/v18/17-145.html},
}

@InProceedings{Wang2016Sketching,
  author    = {Jialei Wang and Jason Lee and Mehrdad Mahdavi and Mladen Kolar and Nati Srebro},
  title     = {{Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data}},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year      = {2017},
  editor    = {Aarti Singh and Jerry Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1150--1158},
  address   = {Fort Lauderdale, FL, USA},
  month     = {20--22 Apr},
  publisher = {PMLR},
  abstract  = {Sketching techniques scale up machine learning algorithms by reducing the sample size or dimensionality of massive data sets, without sacrificing their statistical properties. In this paper, we study sketching from an optimization point of view. We first show that the iterative Hessian sketch is an optimization process with \emphpreconditioning and develop an \emphaccelerated version using this insight together with conjugate gradient descent. Next, we establish a primal-dual connection between the Hessian sketch and dual random projection, which allows us to develop an \emphaccelerated iterative dual random projection method by applying the preconditioned conjugate gradient descent on the dual problem. Finally, we tackle the problems of large sample size and high-dimensionality in massive data sets by developing the \emphprimal-dual sketch.  The primal-dual sketch iteratively sketches the primal and dual formulations and requires only a logarithmic number of calls to solvers of small sub-problems to recover the optimum of the original problem up to \empharbitrary precision. Extensive experiments on synthetic and real data sets complement our theoretical results.},
  file      = {wang17d.pdf:http\://proceedings.mlr.press/v54/wang17d/wang17d.pdf:PDF},
  url       = {http://proceedings.mlr.press/v54/wang17d.html},
}

@Article{Wang2017Sketching,
  author   = {Wang, Jialei and Lee, Jason D. and Mahdavi, Mehrdad and Kolar, Mladen and Srebro, Nathan},
  journal  = {Electron. J. Stat.},
  title    = {Sketching meets random projection in the dual: a provable recovery algorithm for big and high-dimensional data},
  year     = {2017},
  number   = {2},
  pages    = {4896--4944},
  volume   = {11},
  doi      = {10.1214/17-EJS1334SI},
  fjournal = {Electronic Journal of Statistics},
  mrclass  = {62H12 (68T05 90C06)},
  mrnumber = {3738201},
  url      = {https://doi.org/10.1214/17-EJS1334SI},
}

@InProceedings{Suggala2017Expxorcist,
  author    = {Arun Sai Suggala and Mladen Kolar and Pradeep Ravikumar},
  title     = {{The Expxorcist}: Nonparametric Graphical Models Via Conditional Exponential Densities},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}},
  year      = {2017},
  editor    = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
  pages     = {4449--4459},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/SuggalaKR17},
  timestamp = {2019.05.02},
  url       = {http://papers.nips.cc/paper/7031-the-expxorcist-nonparametric-graphical-models-via-conditional-exponential-densities},
}

@InProceedings{Yu2017Influence,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  title     = {An Influence-Receptivity Model for Topic based Information Cascades},
  booktitle = {2017 {IEEE} International Conference on Data Mining, {ICDM} 2017, New Orleans, LA, USA, November 18-21, 2017},
  year      = {2017},
  editor    = {Vijay Raghavan and Srinivas Aluru and George Karypis and Lucio Miele and Xindong Wu},
  pages     = {1141--1146},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icdm/YuGK17},
  doi       = {10.1109/ICDM.2017.152},
  file      = {online:http\://arxiv.org/pdf/1709.01919v1:PDF},
  url       = {https://doi.org/10.1109/ICDM.2017.152},
}

@Article{Balakrishnan2012Recovering,
  author   = {Sivaraman Balakrishnan and Mladen Kolar and Alessandro Rinaldo and Aarti Singh},
  title    = {Recovering block-structured activations using compressive measurements},
  journal  = {Electron. J. Statist.},
  year     = {2017},
  volume   = {11},
  number   = {1},
  pages    = {2647-2678},
  issn     = {1935-7524},
  doi      = {10.1214/17-EJS1267},
  fjournal = {Electronic Journal of Statistics},
  sici     = {1935-7524(2017)11:1<2647:RBSAUC>2.0.CO;2-N},
}

@InProceedings{Wang2016Efficient,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srebro and Tong Zhang},
  title     = {Efficient Distributed Learning with Sparsity},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3636--3645},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
  abstract  = {We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted $\ell_1$ regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.},
  file      = {wang17f.pdf:http\://proceedings.mlr.press/v70/wang17f/wang17f.pdf:PDF},
  url       = {http://proceedings.mlr.press/v70/wang17f.html},
}

@InCollection{Yu2016Statistical,
  author =    {Ming Yu and Varun Gupta and Mladen Kolar},
  title =     {Statistical Inference for Pairwise Graphical Models Using Score Matching},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year =      {2016}
}

@Article{Kolar2016Discussion,
  author   = {Kolar, Mladen and Taddy, Matt},
  journal  = {Ann. Appl. Stat.},
  title    = {Discussion of ``{C}oauthorship and citation networks for statisticians'' [ {MR}3592033]},
  year     = {2016},
  issn     = {1932-6157},
  number   = {4},
  pages    = {1835--1841},
  volume   = {10},
  doi      = {10.1214/16-AOAS896D},
  fjournal = {The Annals of Applied Statistics},
  mrclass  = {91D30 (01A80 05C90 62P25)},
  mrnumber = {3592037},
  url      = {https://doi.org/10.1214/16-AOAS896D},
}

@InProceedings{Wang2016Inference,
  author    = {Jialei Wang and Mladen Kolar},
  title     = {Inference for High-dimensional Exponential Family Graphical Models},
  booktitle = {Proc. of AISTATS},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  pages     = {751--760},
}

@InCollection{Sun2015Learning,
  author =    {Siqi Sun and Mladen Kolar and Jinbo Xu},
  title =     {Learning structured densities via infinite dimensional exponential families},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {2287--2295},
  url =       {http://papers.nips.cc/paper/6006-learning-structured-densities-via-infinite-dimensional-exponential-families.pdf}
}

@InProceedings{Wang2015Distributed,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srerbo},
  title     = {Distributed Multi-Task Learning},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  series    = {Proceedings of Machine Learning Research},
  pages     = {751--760},
  address   = {Cadiz, Spain},
  month     = {09--11 May},
  publisher = {PMLR},
  abstract  = {We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space, where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.},
  file      = {:Wang2015Distributed.pdf:PDF;wang16d.pdf:http\://proceedings.mlr.press/v51/wang16d.pdf:PDF},
  url       = {http://proceedings.mlr.press/v51/wang16d.html},
}

@Article{Gaynanova2014Optimal,
  author =     {Irina Gaynanova and } # mkolar,
  title =      {Optimal variable selection in multi-group sparse discriminant analysis},
  journal =    {Electron. J. Stat.},
  year =       {2015},
  volume =     {9},
  number =     {2},
  pages =      {2007--2034},
  doi =        {10.1214/15-EJS1064},
  fjournal =   {Electronic Journal of Statistics},
  issn =       {1935-7524},
  mrclass =    {62H30 (62J07 68Q32)},
  mrnumber =   {3393602},
  mrreviewer = {Changyi Park},
  url =        {http://dx.doi.org/10.1214/15-EJS1064}
}

@Article{Kolar2013Optimal,
  Title                    = {Optimal Feature Selection in High-Dimensional Discriminant Analysis},
  Author                   = mkolar # { and } # hliu,
  Journal                  = IEEEit_s,
  Year                     = {2014},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {1063--1083},
  Volume                   = {61}
}

@Article{Wasserman2014Berry,
  author =   lwasser #{ and } # mkolar #{ and Alessandro Rinaldo},
  title =    {Berry-{E}sseen bounds for estimating undirected graphs},
  journal =  ejs_s,
  year =     {2014},
  volume =   {8},
  pages =    {1188--1224},
  doi =      {10.1214/14-EJS928},
  fjournal = {Electronic Journal of Statistics},
  issn =     {1935-7524},
  mrclass =  {Preliminary Data},
  mrnumber = {3263117},
  url =      {http://dx.doi.org/10.1214/14-EJS928}
}

@Article{Kolar2014Graph,
  Title                    = {Graph Estimation From Multi-attribute Data},
  Author                   = mkolar # { and } # hliu # { and } # epxing,
  Journal                  = jmlr_s,
  Year                     = {2014},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1713-1750},
  Volume                   = {15}
}

@InProceedings{kolar13multiatticml,
  Title                    = {Markov Network Estimation From Multi-attribute Data},
  Author                   = mkolar # { and } # hliu # { and } # epxing,
  Booktitle                = {Proc. of ICML},
  Year                     = {2013},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar13road,
  Title                    = {Feature Selection in High-Dimensional Classification},
  Author                   = {Mladen Kolar and Han Liu},
  Booktitle                = {JMLR W$\&$CP: } # PROC_s # { } # ICML2013_s,
  Year                     = {2013},
  Pages                    = {100--108},
  Volume                   = {28},

  Editors                  = {Sanjoy Dasgupta and David McAllester},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{kolar10estimating,
  Title                    = {Estimating Networks With Jumps},
  Author                   = mkolar # { and } # epxing,
  Journal                  = ejs_s,
  Year                     = {2012},
  Pages                    = {2069--2106},
  Volume                   = {6},

  Newspaper                = {Electron. J. Stat.},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18}
}

@InProceedings{Kolar2012Variance,
  Title                    = {Variance Function Estimation in High-dimensions},
  Author                   = mkolar # { and James Sharpnack},
  Booktitle                = PROC_s # { } # ICML2012_s,
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Editor                   = {John Langford and Joelle Pineau},
  Month                    = {July},
  Pages                    = {1447--1454},
  Publisher                = {Omnipress},
  Series                   = {ICML '12},

  ISBN                     = {978-1-4503-1285-1},
  Location                 = {Edinburgh, Scotland, GB},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Kolar2012Consistent,
  Title                    = {Consistent Covariance Selection From Data With Missing Values},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of ICML},
  Year                     = {2012},

  Address                  = {Edinburgh, Scotland, GB},
  Editor                   = {John Langford and Joelle Pineau},
  Month                    = {July},
  Pages                    = {551--558},
  Publisher                = {Omnipress},

  ISBN                     = {978-1-4503-1285-1},
  Monthno                  = {7},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar2012marginal,
  Title                    = {Marginal Regression For Multitask Learning},
  Author                   = mkolar # { and } # hliu,
  Booktitle                = {Proc. of ICML},
  Year                     = {2012},

  Address                  = {Edinburgh, Scotland, GB},
  Editor                   = {John Langford and Joelle Pineau},
  Pages                    = {647--655},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{balakrishnan2011statistical,
  author =    {Balakrishnan, Sivaraman and Kolar, Mladen and Rinaldo, Alessandro and Singh, Aarti and Wasserman, Larry},
  title =     {Statistical and computational tradeoffs in biclustering},
  booktitle = {NIPS 2011 Workshop on Computational Trade-offs in Statistical Learning},
  year =      {2011},
  volume =    {4}
}

@InProceedings{Kolar2011Minimax,
  Title                    = {Minimax Localization of Structural Information in Large Noisy Matrices},
  Author                   = mkolar # { and Sivaraman Balakrishnan and Alessandro Rinaldo and Aarti Singh},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Year                     = {2011},
  Editor                   = {John Shawe-Taylor and Richard S. Zemel and Peter L. Bartlett and Fernando C. N. Pereira and Kilian Q. Weinberger},
  Pages                    = {909--917},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar2011time,
  Title                    = {On Time Varying Undirected Graphs},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2011},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{kolar11union,
  author =    mkolar #{ and } # jlafferty #{and } # lwasser,
  title =     {Union Support Recovery In Multi-task Learning},
  journal =   jmlr_s,
  year =      {2011},
  volume =    {12},
  pages =     {2415--2435},
  file =      {:kolar11union.pdf:PDF},
  issn =      {1532-4435},
  mrnumber =  {2825433 (2012f:62144)},
  newspaper = {J. Mach. Learn. Res.},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@InProceedings{kolar10nonparametric,
  Title                    = {On Sparse Nonparametric Conditional Covariance Selection},
  Author                   = mkolar # { and A.~P. Parikh and } # epxing,
  Booktitle                = PROC_s # { } # ICML2010_s,
  Year                     = {2010},

  Address                  = {Haifa, Israel},
  Editor                   = {Johannes F{\"u}rnkranz and Thorsten Joachims},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar10ultrahigh,
  Title                    = {Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2010},
  Pages                    = {413--420},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{song09time,
  Title                    = {Time-varying Dynamic Bayesian Networks},
  Author                   = lsong # {and } # mkolar # { and } # epxing,
  Booktitle                = {Proc. of NIPS},
  Year                     = {2009},
  Editor                   = {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  Pages                    = {1732--1740}
}

@InProceedings{kolar09nips_tv_paper,
  Title                    = {Sparsistent Learning Of Varying-coefficient Models With Structural Changes},
  Author                   = mkolar # { and } # lsong # {and } # epxing,
  Booktitle                = {Proc. of NIPS},
  Year                     = {2009},
  Editor                   = {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  Pages                    = {1006--1014},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Song2009KELLER,
  author    = {L. Song and M. Kolar and E. P. Xing},
  journal   = {Bioinformatics},
  title     = {{KELLER}: estimating time-varying interactions between genes},
  year      = {2009},
  month     = {may},
  number    = {12},
  pages     = {i128--i136},
  volume    = {25},
  doi       = {10.1093/bioinformatics/btp192},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Kolar2010Estimating,
  Title                    = {Estimating {Time-varying} Networks},
  Author                   = mkolar # { and } # lsong # {and } # aahmed # { and } # epxing,
  Journal                  = aoas_s,
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {94--123},
  Volume                   = {4},

  Newspaper                = {Ann. Appl. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Ray2008CSMET,
  author    = {Pradipta Ray and Suyash Shringarpure and Mladen Kolar and Eric P. Xing},
  journal   = {{PLoS} Computational Biology},
  title     = {{CSMET}: Comparative Genomic Motif Detection via Multi-Resolution Phylogenetic Shadowing},
  year      = {2008},
  month     = {jun},
  number    = {6},
  pages     = {e1000090},
  volume    = {4},
  doi       = {10.1371/journal.pcbi.1000090},
  editor    = {Uwe Ohler},
  publisher = {Public Library of Science ({PLoS})},
}

@InProceedings{Petrovic2006Comparison,
  author    = {S. Petrovic and J. Snajder and B. Dalbelo-Basic and M. Kolar},
  booktitle = {28th International Conference on Information Technology Interfaces, 2006.},
  title     = {Comparison of collocation extraction measures for document indexing},
  year      = {2006},
  publisher = {{IEEE}},
  doi       = {10.1109/iti.2006.1708523},
}

@Article{Kolar2005Computer,
  author  = {Mladen Kolar, Igor Vukmirovi\'c, Bojana Dalbelo Ba\"si\'c, Jan \"Snajder},
  journal = {Journal of Computing and Information Technology},
  title   = {Computer-Aided document Indexing Systems},
  year    = {2005},
  number  = {4},
  pages   = {299--305},
  volume  = {13},
}

@Comment{jabref-meta: databaseType:bibtex;}
