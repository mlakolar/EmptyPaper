@Article{Kim2019Two,
  author        = {Kim, Byol and Liu, Song and Kolar, Mladen},
  journal       = {Journal of the Royal Statistical Society.~Series B.},
  title         = {Two-sample inference for high-dimensional {M}arkov networks},
  year          = {2021},
  issn          = {1369-7412},
  number        = {5},
  pages         = {939--962},
  volume        = {83},
  archiveprefix = {arXiv},
  doi           = {10.1111/rssb.12446},
  eprint        = {1905.00466},
  mrclass       = {62F40},
  mrnumber      = {4349123},
  primaryclass  = {stat.ME},
  url           = {https://doi.org/10.1111/rssb.12446},
}

@Article{Na2019Estimating,
  author      = {Na, S. and Kolar, M. and Koyejo, O.},
  journal     = {Biometrika},
  title       = {Estimating differential latent variable graphical models with applications to brain connectivity},
  year        = {2021},
  issn        = {0006-3444},
  number      = {2},
  pages       = {425--442},
  volume      = {108},
  doi         = {10.1093/biomet/asaa066},
  eprint      = {1909.05892},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  fjournal    = {Biometrika},
  mrclass     = {62H22 (92B20)},
  mrnumber    = {4259141},
  url         = {https://doi.org/10.1093/biomet/asaa066},
}

@Article{Dai2021Inference,
  author        = {Ran Dai and Mladen Kolar},
  journal       = {Electronic Journal of Statistics},
  title         = {Inference for high-dimensional varying-coefficient quantile regression},
  year          = {2021},
  issn          = {1935-7524},
  month         = dec,
  number        = {2},
  pages         = {5696-5757},
  volume        = {15},
  archiveprefix = {arXiv},
  doi           = {10.1214/21-EJS1919},
  eprint        = {2002.07370v1},
  file          = {:http\://arxiv.org/pdf/2002.07370v1:PDF},
  groups        = {MyPapers},
  primaryclass  = {stat.ME},
  sici          = {1935-7524(2021)15:2<5696:IFHDVC>2.0.CO;2-N},
}

@Article{Hanzely2021Personalized,
  author        = {Filip Hanzely and Boxin Zhao and Mladen Kolar},
  journal       = {Technical report},
  title         = {Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques},
  year          = {2021},
  month         = feb,
  abstract      = {We study the optimization aspects of personalized Federated Learning (FL). We develop a universal optimization theory applicable to all convex personalized FL models in the literature. In particular, we propose a general personalized objective capable of recovering essentially any existing personalized FL objective as a special case. We design several optimization techniques to minimize the general objective, namely a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. We demonstrate the practicality and/or optimality of our methods both in terms of communication and local computation. Lastly, we argue about the implications of our general optimization theory when applied to solve specific personalized FL objectives.},
  archiveprefix = {arXiv},
  eprint        = {2102.09743},
  file          = {:http\://arxiv.org/pdf/2102.09743v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Na2021Adaptive,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  journal       = {Technical report},
  title         = {An Adaptive Stochastic Sequential Quadratic Programming with Differentiable Exact Augmented Lagrangians},
  year          = {2021},
  month         = feb,
  abstract      = {We consider the problem of solving nonlinear optimization programs with stochastic objective and deterministic equality constraints. We assume for the objective that the function evaluation, the gradient, and the Hessian are inaccessible, while one can compute their stochastic estimates by, for example, subsampling. We propose a stochastic algorithm based on sequential quadratic programming (SQP) that uses a differentiable exact augmented Lagrangian as the merit function. To motivate our algorithm, we revisit an old SQP method \citep{Lucidi1990Recursive} developed for deterministic programs. We simplify that method and derive an adaptive SQP, which serves as the skeleton of our stochastic algorithm. Based on the derived algorithm, we then propose a non-adaptive SQP for optimizing stochastic objectives, where the gradient and the Hessian are replaced by stochastic estimates but the stepsize is deterministic and prespecified. Finally, we incorporate a recent stochastic line search procedure \citep{Paquette2020Stochastic} into our non-adaptive stochastic SQP to arrive at an adaptive stochastic SQP. To our knowledge, the proposed algorithm is the first stochastic SQP that allows a line search procedure and the first stochastic line search procedure that allows the constraints. The global convergence for all proposed SQP methods is established, while numerical experiments on nonlinear problems in the CUTEst test set demonstrate the superiority of the proposed algorithm.},
  archiveprefix = {arXiv},
  eprint        = {2102.05320},
  file          = {:http\://arxiv.org/pdf/2102.05320v1:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC, cs.NA, math.NA, stat.CO, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Wang2021Robust,
  author        = {Y. Samuel Wang and Si Kai Lee and Panos Toulis and Mladen Kolar},
  journal       = {International Conference on Machine Learning (ICML)},
  title         = {Robust Inference for High-Dimensional Linear Models via Residual Randomization},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2106.07717},
  file          = {:http\://arxiv.org/pdf/2106.07717v2:PDF},
  keywords      = {stat.ME, stat.ML},
  primaryclass  = {stat.ME},
  timestamp     = {Wed, 25 Aug 2021 17:11:17 +0200},
  url           = {http://proceedings.mlr.press/v139/wang21m.html},
}

@Article{Zhao2021High,
  author        = {Boxin Zhao and Shengjun Zhai and Y. Samuel Wang and Mladen Kolar},
  journal       = {Technical report},
  title         = {High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach},
  year          = {2021},
  month         = may,
  abstract      = {Undirected graphical models have been widely used to model the conditional independence structure of high-dimensional random vector data for years. In many modern applications such as EEG and fMRI data, the observations are multivariate random functions rather than scalars. To model the conditional independence of this type of data, functional graphical models are proposed and have attracted an increasing attention in recent years. In this paper, we propose a neighborhood selection approach to estimate Gaussian functional graphical models. We first estimate the neighborhood of all nodes via function-on-function regression, and then we can recover the whole graph structure based on the neighborhood information. By estimating conditional structure directly, we can circumvent the need of a well-defined precision operator which generally does not exist. Besides, we can better explore the effect of the choice of function basis for dimension reduction. We give a criterion for choosing the best function basis and motivate two practically useful choices, which we justified by both theory and experiments and show that they are better than expanding each function onto its own FPCA basis as in previous literature. In addition, the neighborhood selection approach is computationally more efficient than fglasso as it is more easy to do parallel computing. The statistical consistency of our proposed methods in high-dimensional setting are supported by both theory and experiment.},
  archiveprefix = {arXiv},
  eprint        = {2105.02487},
  file          = {:http\://arxiv.org/pdf/2105.02487v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, stat.ME},
  primaryclass  = {stat.ML},
}

@Article{Na2021Inequality,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  journal       = {Technical report},
  title         = {Inequality Constrained Stochastic Nonlinear Optimization via Active-Set Sequential Quadratic Programming},
  year          = {2021},
  month         = sep,
  abstract      = {We study nonlinear optimization problems with stochastic objective and deterministic equality and inequality constraints, which emerge in numerous applications including finance, manufacturing, power systems and, recently, deep neural networks. We propose an active-set stochastic sequential quadratic programming algorithm, using a differentiable exact augmented Lagrangian as the merit function. The algorithm adaptively selects the penalty parameters of augmented Lagrangian and performs stochastic line search to decide the stepsize. The global convergence is established: for any initialization, the "liminf" of the KKT residuals converges to zero almost surely. Our algorithm and analysis further develop the prior work \cite{Na2021Adaptive} by allowing nonlinear inequality constraints. We demonstrate the performance of the algorithm on a subset of nonlinear problems collected in the CUTEst test set.},
  archiveprefix = {arXiv},
  eprint        = {2109.11502},
  file          = {:http\://arxiv.org/pdf/2109.11502v1:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC, cs.LG, cs.NA, math.NA, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Na2021Fast,
  author        = {Sen Na and Mihai Anitescu and Mladen Kolar},
  journal       = {Technical report},
  title         = {A Fast Temporal Decomposition Procedure for Long-horizon Nonlinear Dynamic Programming},
  year          = {2021},
  month         = jul,
  abstract      = {We propose a fast temporal decomposition procedure for solving long-horizon nonlinear dynamic programs. The core of the procedure is sequential quadratic programming (SQP), with a differentiable exact augmented Lagrangian being the merit function. Within each SQP iteration, we solve the Newton system approximately using an overlapping temporal decomposition. We show that the approximated search direction is still a descent direction of the augmented Lagrangian, provided the overlap size and penalty parameters are suitably chosen, which allows us to establish global convergence. Moreover, we show that a unit stepsize is accepted locally for the approximated search direction, and further establish a uniform, local linear convergence over stages. Our local convergence rate matches the rate of the recent Schwarz scheme \cite{Na2020Overlapping}. However, the Schwarz scheme has to solve nonlinear subproblems to optimality in each iteration, while we only perform one Newton step instead. Numerical experiments validate our theories and demonstrate the superiority of our method.},
  archiveprefix = {arXiv},
  eprint        = {2107.11560},
  file          = {:http\://arxiv.org/pdf/2107.11560v2:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC, math.DS},
  primaryclass  = {math.OC},
}

@Article{Liao2021Local,
  author        = {Luofeng Liao and Li Shen and Jia Duan and Mladen Kolar and Dacheng Tao},
  journal       = {Technical report},
  title         = {Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems},
  year          = {2021},
  month         = jun,
  abstract      = {Large scale convex-concave minimax problems arise in numerous applications, including game theory, robust training, and training of generative adversarial networks. Despite their wide applicability, solving such problems efficiently and effectively is challenging in the presence of large amounts of data using existing stochastic minimax methods. We study a class of stochastic minimax methods and develop a communication-efficient distributed stochastic extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable for solving convex-concave minimax problem in the Parameter-Server model. LocalAdaSEG has three main features: (i) periodic communication strategy reduces the communication cost between workers and the server; (ii) an adaptive learning rate that is computed locally and allows for tuning-free implementation; and (iii) theoretically, a nearly linear speed-up with respect to the dominant variance term, arising from estimation of the stochastic gradient, is proven in both the smooth and nonsmooth convex-concave settings. LocalAdaSEG is used to solve a stochastic bilinear game, and train generative adversarial network. We compare LocalAdaSEG against several existing optimizers for minimax problems and demonstrate its efficacy through several experiments in both the homogeneous and heterogeneous settings.},
  archiveprefix = {arXiv},
  eprint        = {2106.10022},
  file          = {:http\://arxiv.org/pdf/2106.10022v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG, cs.DC, math.OC},
  primaryclass  = {cs.LG},
}

@Article{Tsai2021Joint,
  author        = {Katherine Tsai and Oluwasanmi Koyejo and Mladen Kolar},
  journal       = {Technical report},
  title         = {Joint Gaussian Graphical Model Estimation: A Survey},
  year          = {2021},
  month         = oct,
  abstract      = {Graphs from complex systems often share a partial underlying structure across domains while retaining individual features. Thus, identifying common structures can shed light on the underlying signal, for instance, when applied to scientific discoveries or clinical diagnoses. Furthermore, growing evidence shows that the shared structure across domains boosts the estimation power of graphs, particularly for high-dimensional data. However, building a joint estimator to extract the common structure may be more complicated than it seems, most often due to data heterogeneity across sources. This manuscript surveys recent work on statistical inference of joint Gaussian graphical models, identifying model structures that fit various data generation processes. Simulations under different data generation processes are implemented with detailed discussions on the choice of models.},
  archiveprefix = {arXiv},
  eprint        = {2110.10281},
  file          = {:http\://arxiv.org/pdf/2110.10281v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ME, cs.LG, stat.ML},
  primaryclass  = {stat.ME},
}

@Article{Zhao2021Adaptive,
  author        = {Boxin Zhao and Ziqi Liu and Chaochao Chen and Mladen Kolar and Zhiqiang Zhang and Jun Zhou},
  journal       = {Technical report},
  title         = {Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback},
  year          = {2021},
  month         = dec,
  abstract      = {In federated learning (FL) problems, client sampling plays a key role in the convergence speed of training algorithm. However, while being an important problem in FL, client sampling is lack of study. In this paper, we propose an online learning with bandit feedback framework to understand the client sampling problem in FL. By adapting an Online Stochastic Mirror Descent algorithm to minimize the variance of gradient estimation, we propose a new adaptive client sampling algorithm. Besides, we use online ensemble method and doubling trick to automatically choose the tuning parameters in the algorithm. Theoretically, we show dynamic regret bound with comparator as the theoretically optimal sampling sequence; we also include the total variation of this sequence in our upper bound, which is a natural measure of the intrinsic difficulty of the problem. To the best of our knowledge, these theoretical contributions are novel to existing literature. Moreover, by implementing both synthetic and real data experiments, we show empirical evidence of the advantages of our proposed algorithms over widely-used uniform sampling and also other online learning based sampling strategies in previous studies. We also examine its robustness to the choice of tuning parameters. Finally, we discuss its possible extension to sampling without replacement and personalized FL objective. While the original goal is to solve client sampling problem, this work has more general applications on stochastic gradient descent and stochastic coordinate descent methods.},
  archiveprefix = {arXiv},
  eprint        = {2112.14332},
  file          = {:http\://arxiv.org/pdf/2112.14332v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Luo2021Dynamic,
  author        = {Yuwei Luo and Varun Gupta and Mladen Kolar},
  journal       = {Accepted to SIGMETRICS 2022},
  title         = {Dynamic Regret Minimization for Control of Non-stationary Linear Dynamical Systems},
  year          = {2021},
  month         = nov,
  abstract      = {We consider the problem of controlling a Linear Quadratic Regulator (LQR) system over a finite horizon $T$ with fixed and known cost matrices $Q,R$, but unknown and non-stationary dynamics $\{A_t, B_t\}$. The sequence of dynamics matrices can be arbitrary, but with a total variation, $V_T$, assumed to be $o(T)$ and unknown to the controller. Under the assumption that a sequence of stabilizing, but potentially sub-optimal controllers is available for all $t$, we present an algorithm that achieves the optimal dynamic regret of $\tilde{\mathcal{O}}\left(V_T^{2/5}T^{3/5}\right)$. With piece-wise constant dynamics, our algorithm achieves the optimal regret of $\tilde{\mathcal{O}}(\sqrt{ST})$ where $S$ is the number of switches. The crux of our algorithm is an adaptive non-stationarity detection strategy, which builds on an approach recently developed for contextual Multi-armed Bandit problems. We also argue that non-adaptive forgetting (e.g., restarting or using sliding window learning with a static window size) may not be regret optimal for the LQR problem, even when the window size is optimally tuned with the knowledge of $V_T$. The main technical challenge in the analysis of our algorithm is to prove that the ordinary least squares (OLS) estimator has a small bias when the parameter to be estimated is non-stationary. Our analysis also highlights that the key motif driving the regret is that the LQR problem is in spirit a bandit problem with linear feedback and locally quadratic cost. This motif is more universal than the LQR problem itself, and therefore we believe our results should find wider application.},
  archiveprefix = {arXiv},
  eprint        = {2111.03772},
  file          = {:http\://arxiv.org/pdf/2111.03772v1:PDF},
  groups        = {MyPapers},
  keywords      = {cs.LG, math.OC, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Yu2019Simultaneous,
  author   = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal  = {Journal of Machine Learning Research},
  title    = {Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching},
  year     = {2020},
  number   = {91},
  pages    = {1-51},
  volume   = {21},
  abstract = {Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is $\sqrt{n}$-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.},
  groups   = {MyPapers},
  url      = {http://jmlr.org/papers/v21/19-383.html},
}

@Article{Yu2018Recovery,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal   = {Electron. J. Statist.},
  title     = {Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach},
  year      = {2020},
  issn      = {1935-7524},
  number    = {1},
  pages     = {413-457},
  volume    = {14},
  doi       = {10.1214/19-EJS1658},
  fjournal  = {Electronic Journal of Statistics},
  groups    = {MyPapers},
  sici      = {1935-7524(2020)14:1<413:ROSLRA>2.0.CO;2-B},
  timestamp = {2020.01.22},
}

@Article{Yu2017Estimation,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {Journal of Machine Learning Research},
  title         = {Estimation of a Low-rank Topic-Based Model for Information Cascades},
  year          = {2020},
  number        = {71},
  pages         = {1-47},
  volume        = {21},
  abstract      = {We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or {\it cascades}. Here for a given cascade, we only observe the times of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades and prove an analytical upper bound on the estimation error. The estimated model can be used to build recommendation systems based on the receptivity vectors, as well as for marketing based on the influence vectors. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.},
  archiveprefix = {arXiv},
  eprint        = {1709.01919v2},
  file          = {:http\://arxiv.org/pdf/1709.01919v2:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, cs.SI},
  primaryclass  = {stat.ML},
  url           = {http://jmlr.org/papers/v21/19-496.html},
}

@Article{Zhao2020FuDGE,
  author        = {Boxin Zhao and Y. Samuel Wang and Mladen Kolar},
  journal       = {arXiv:2003.05402},
  title         = {FuDGE: Functional Differential Graph Estimation with fully and discretely observed curves},
  year          = {2020},
  month         = mar,
  abstract      = {We consider the problem of estimating the difference between two functional undirected graphical models with shared structures. In many applications, data are naturally regarded as high-dimensional random function vectors rather than multivariate scalars. For example, electroencephalography (EEG) data are more appropriately treated as functions of time. In these problems, not only can the number of functions measured per sample be large, but each function is itself an infinite dimensional object, making estimation of model parameters challenging. In practice, curves are usually discretely observed, which makes graph structure recovery even more challenging. We formally characterize when two functional graphical models are comparable and propose a method that directly estimates the functional differential graph, which we term FuDGE. FuDGE avoids separate estimation of each graph, which allows for estimation in problems where individual graphs are dense, but their difference is sparse. We show that FuDGE consistently estimates the functional differential graph in a high-dimensional setting for both discretely observed and fully observed function paths. We illustrate finite sample properties of our method through simulation studies. In order to demonstrate the benefits of our method, we propose Joint Functional Graphical Lasso as a competitor, which is a generalization of the Joint Graphical Lasso. Finally, we apply our method to EEG data to uncover differences in functional brain connectivity between alcoholics and control subjects.},
  archiveprefix = {arXiv},
  eprint        = {2003.05402v1},
  file          = {:http\://arxiv.org/pdf/2003.05402v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  timestamp     = {2020.03.13},
}

@Article{Zhang2020Posterior,
  author        = {Yulong Zhang and Mingxuan Yi and Song Liu and Mladen Kolar},
  journal       = {arXiv:2002.06410},
  title         = {Posterior Ratio Estimation for Latent Variables},
  year          = {2020},
  month         = feb,
  abstract      = {Density Ratio Estimation has attracted attention from machine learning community due to its ability of comparing the underlying distributions of two datasets. However, in some applications, we want to compare distributions of \emph{latent} random variables that can be only inferred from observations. In this paper, we study the problem of estimating the ratio between two posterior probability density functions of a latent variable. Particularly, we assume the posterior ratio function can be well-approximated by a parametric model, which is then estimated using observed datasets and synthetic prior samples. We prove consistency of our estimator and the asymptotic normality of the estimated parameters as the number of prior samples tending to infinity. Finally, we validate our theories using numerical experiments and demonstrate the usefulness of the proposed method through some real-world applications.},
  archiveprefix = {arXiv},
  eprint        = {2002.06410v1},
  file          = {:http\://arxiv.org/pdf/2002.06410v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  timestamp     = {2020.03.13},
}

@Article{Yu2019Constrained,
  author        = {Ming Yu and Varun Gupta and Mladen Kolar},
  journal       = {arXiv:1911.07319},
  title         = {Constrained High Dimensional Statistical Inference},
  year          = {2020},
  month         = nov,
  abstract      = {In typical high dimensional statistical inference problems, confidence intervals and hypothesis tests are performed for a low dimensional subset of model parameters under the assumption that the parameters of interest are unconstrained. However, in many problems, there are natural constraints on model parameters and one is interested in whether the parameters are on the boundary of the constraint or not. e.g. non-negativity constraints for transmission rates in network diffusion. In this paper, we provide algorithms to solve this problem of hypothesis testing in high-dimensional statistical models under constrained parameter space. We show that following our testing procedure we are able to get asymptotic designed Type I error under the null. Numerical experiments demonstrate that our algorithm has greater power than the standard algorithms where the constraints are ignored. We demonstrate the effectiveness of our algorithms on two real datasets where we have {\emph{intrinsic}} constraint on the parameters.},
  archiveprefix = {arXiv},
  eprint        = {1911.07319v1},
  file          = {:http\://arxiv.org/pdf/1911.07319v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ME},
  primaryclass  = {stat.ME},
  timestamp     = {2020.03.13},
}

@Article{Na2020Semiparametric,
  author    = {Na, Sen and Luo, Yuwei and Yang, Zhuoran and Wang, Zhaoran and Kolar, Mladen},
  title     = {Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees},
  year      = {2020},
  month     = {13--18 Jul},
  pages     = {7141--7152},
  volume    = {119},
  abstract  = {Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution: the bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks that take high-dimensional features as inputs, while nonparametric (nuisance) component is the base measure. In this setting, the representation learning problem is equivalent to recovering the weight matrices, and the main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor    = {Hal DaumÃ© III and Aarti Singh},
  groups    = {MyPapers},
  pdf       = {http://proceedings.mlr.press/v119/na20a/na20a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  timestamp = {2021.03.01},
  url       = {http://proceedings.mlr.press/v119/na20a.html},
}

@Article{Zhang2020Posterior,
  author        = {Yulong Zhang and Mingxuan Yi and Song Liu and Mladen Kolar},
  journal       = {arXiv:2002.06410},
  title         = {Posterior Ratio Estimation for Latent Variables},
  year          = {2020},
  month         = feb,
  abstract      = {Density Ratio Estimation has attracted attention from machine learning community due to its ability of comparing the underlying distributions of two datasets. However, in some applications, we want to compare distributions of \emph{latent} random variables that can be only inferred from observations. In this paper, we study the problem of estimating the ratio between two posterior probability density functions of a latent variable. Particularly, we assume the posterior ratio function can be well-approximated by a parametric model, which is then estimated using observed datasets and synthetic prior samples. We prove consistency of our estimator and the asymptotic normality of the estimated parameters as the number of prior samples tending to infinity. Finally, we validate our theories using numerical experiments and demonstrate the usefulness of the proposed method through some real-world applications.},
  archiveprefix = {arXiv},
  eprint        = {2002.06410v1},
  file          = {:http\://arxiv.org/pdf/2002.06410v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  timestamp     = {2020.03.13},
}

@Article{Wang2020Statistical,
  author        = {Xu Wang and Mladen Kolar and Ali Shojaie},
  journal       = {arXiv:2007.07448},
  title         = {Statistical Inference for Networks of High-Dimensional Point Processes},
  year          = {2020},
  month         = jul,
  abstract      = {Fueled in part by recent applications in neuroscience, the multivariate Hawkes process has become a popular tool for modeling the network of interactions among high-dimensional point process data. While evaluating the uncertainty of the network estimates is critical in scientific applications, existing methodological and theoretical work has primarily addressed estimation. To bridge this gap, this paper develops a new statistical inference procedure for high-dimensional Hawkes processes. The key ingredient for this inference procedure is a new concentration inequality on the first- and second-order statistics for integrated stochastic processes, which summarize the entire history of the process. Combining recent results on martingale central limit theory with the new concentration inequality, we then characterize the convergence rate of the test statistics. We illustrate finite sample validity of our inferential tools via extensive simulations and demonstrate their utility by applying them to a neuron spike train data set.},
  archiveprefix = {arXiv},
  eprint        = {2007.07448v1},
  file          = {:http\://arxiv.org/pdf/2007.07448v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Liao2020Provably,
  author        = {Luofeng Liao and You{-}Lin Chen and Zhuoran Yang and Bo Dai and Mladen Kolar and Zhaoran Wang},
  journal       = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {Provably Efficient Neural Estimation of Structural Equation Models: An Adversarial Approach},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2007.01290},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
  url           = {https://proceedings.neurips.cc/paper/2020/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html},
}

@Article{Chen2020Understanding,
  author        = {You-Lin Chen and Mladen Kolar},
  journal       = {arXiv: 2006.06782},
  title         = {Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  year          = {2020},
  month         = jun,
  abstract      = {We study accelerated stochastic gradient descent through the lens of the growth condition. Stochastic gradient methods (SGD) with momentum, such as heavy ball (HB) and Nesterov's accelerated methods (NAM), are widely used in practice, especially for training neural networks. However, numerical experiments and theoretical results have shown that there are simple problem instances where SGD with momentum cannot outperform vanilla SGD. Furthermore, most of the research on accelerated SGD consider restricted settings with quadratic functions or assumption of additive noise with bounded variance. In this work, we assume a growth condition that states the stochastic gradients can be dominated by a multiplicative part and an additive part, where the multiplicative part shrinks relative to the full gradient, while the additive part is bounded. We provide insight into the behavior of acceleration in stochastic settings under the growth condition by showing robustness, with respect to a perturbation on gradients, of accelerated methods. Several accelerated methods, including accelerated dual averaging methods and robust momentum methods, are examined for the robust property. We illustrate how the multiplicative noise affects the convergence rate of accelerated methods and may cause convergence to fail. We establish a trade-off between robustness and convergence rate, which shows that even though simple accelerated methods like HB and NAM are optimal in the deterministic case, more sophisticated design of algorithms leads to robustness in stochastic settings and achieves a better convergence rate than vanilla SGD. Moreover, we provide unified optimal schemes of averaging and diminishing momentum and stepsize, which achieves the theoretical lower bound up to some constants.},
  archiveprefix = {arXiv},
  eprint        = {2006.06782},
  file          = {:http\://arxiv.org/pdf/2006.06782v2:PDF},
  groups        = {MyPapers},
  keywords      = {math.OC},
  primaryclass  = {math.OC},
  timestamp     = {2021.03.01},
}

@Article{Tsai2020Nonconvex,
  author        = {Katherine Tsai and Mladen Kolar and Oluwasanmi Koyejo},
  journal       = {arXiv: 2011.05601},
  title         = {A Nonconvex Framework for Structured Dynamic Covariance Recovery},
  year          = {2020},
  month         = nov,
  abstract      = {We propose a flexible yet interpretable model for high-dimensional data with time-varying second order statistics, motivated and applied to functional neuroimaging data. Motivated by the neuroscience literature, we factorize the covariances into sparse spatial and smooth temporal components. While this factorization results in both parsimony and domain interpretability, the resulting estimation problem is nonconvex. To this end, we design a two-stage optimization scheme with a carefully tailored spectral initialization, combined with iteratively refined alternating projected gradient descent. We prove a linear convergence rate up to a nontrivial statistical error for the proposed descent scheme and establish sample complexity guarantees for the estimator. We further quantify the statistical error for the multivariate Gaussian case. Empirical results using simulated and real brain imaging data illustrate that our approach outperforms existing baselines.},
  archiveprefix = {arXiv},
  eprint        = {2011.05601},
  file          = {:http\://arxiv.org/pdf/2011.05601v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG, stat.AP, stat.ME},
  primaryclass  = {stat.ML},
  timestamp     = {2021.03.01},
}

@Article{Geng2018Joint,
  author    = {Sinong Geng and Mladen Kolar and Oluwasanmi Koyejo},
  title     = {Joint Nonparametric Precision Matrix Estimation with Confounding},
  year      = {2019},
  pages     = {378--388},
  volume    = {115},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/uai/GengKK19.bib},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  editor    = {Amir Globerson and Ricardo Silva},
  groups    = {MyPapers},
  publisher = {{AUAI} Press},
  series    = {Proceedings of Machine Learning Research},
  timestamp = {2021.03.01},
  url       = {http://proceedings.mlr.press/v115/geng20a.html},
}

@Article{Na2018High,
  author        = {Sen Na and Mladen Kolar},
  journal       = {Bernoulli},
  title         = {High-dimensional index volatility models via {S}tein's identity},
  year          = {2021},
  issn          = {1350-7265},
  number        = {2},
  pages         = {794-817},
  volume        = {27},
  archiveprefix = {arXiv},
  doi           = {10.3150/20-BEJ1238},
  eprint        = {1811.10790},
  file          = {:http\://arxiv.org/pdf/1811.10790v2:PDF},
  fjournal      = {Bernoulli},
  groups        = {MyPapers},
  primaryclass  = {math.ST},
  sici          = {1350-7265(2021)27:2<794:HDIVMV>2.0.CO;2-X},
}

@Article{Liao2021Instrumental,
  author        = {Luofeng Liao and Zuyue Fu and Zhuoran Yang and Mladen Kolar and Zhaoran Wang},
  journal       = {Technical report},
  title         = {Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning},
  year          = {2021},
  month         = feb,
  abstract      = {In offline reinforcement learning (RL) an optimal policy is learnt solely from a priori collected observational data. However, in observational data, actions are often confounded by unobserved variables. Instrumental variables (IVs), in the context of RL, are the variables whose influence on the state variables are all mediated through the action. When a valid instrument is present, we can recover the confounded transition dynamics through observational data. We study a confounded Markov decision process where the transition dynamics admit an additive nonlinear functional form. Using IVs, we derive a conditional moment restriction (CMR) through which we can identify transition dynamics based on observational data. We propose a provably efficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual reformulation of CMR. To the best of our knowledge, this is the first provably efficient algorithm for instrument-aided offline RL.},
  archiveprefix = {arXiv},
  eprint        = {2102.09907},
  file          = {:http\://arxiv.org/pdf/2102.09907v1:PDF},
  groups        = {MyPapers},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Chen2019Tensor,
  author        = {Chen, You-Lin and Kolar, Mladen and Tsay, Ruey S.},
  journal       = {Journal of Computational and Graphical Statistics},
  title         = {Tensor {C}anonical {C}orrelation {A}nalysis {W}ith {C}onvergence and {S}tatistical {G}uarantees},
  year          = {2021},
  issn          = {1061-8600},
  number        = {3},
  pages         = {728--744},
  volume        = {30},
  archiveprefix = {arXiv},
  doi           = {10.1080/10618600.2020.1856118},
  eprint        = {1906.05358},
  mrclass       = {62H20 (90C26)},
  mrnumber      = {4313472},
  primaryclass  = {stat.ML},
  url           = {https://doi.org/10.1080/10618600.2020.1856118},
}

@Comment{jabref-meta: databaseType:bibtex;}
