% Encoding: UTF-8


@String { aahmed       = {Amr Ahmed} }
@String { abuja        = {Andreas Buja} }
@String { acha         = {Applied and Computational Harmonic Analysis} }
@String { acha_s       = {Appl. Comput. Harmon. Anal.} }
@String { adalalyan    = {Arnak S. Dalalyan} }
@String { adawid       = {Alexander Philip Dawid} }
@String { aism_s       = {Ann. Inst. Statist. Math.} }
@String { alozano      = {Aur\'elie C. Lozano} }
@String { anobel       = {Andrew B. Nobel} }
@String { aoas         = { Annals of Applied Statistics } }
@String { aoas_s       = { Ann. Appl. Stat. } }
@String { aop          = { Annals of Probability } }
@String { aop_s        = { Ann. Probab. } }
@String { aos          = { Annals of Statistics } }
@String { aos_s        = { Ann. Stat. } }
@String { arothman     = { Adam J. Rothman } }
@String { atsybakov    = { Alexandre B. Tsybakov } }
@String { avdvaart     = {Aad W. {van der Vaart}} }
@String { awillsky     = {Alan S. Willsky} }

@String { bcaffo       = {Brian S. Caffo} }
@String { befron       = { Bradley Efron } }
@String { bpotscher    = {Benedikt M. P\"otscher} }
@String { byu          = { Bin Yu } }
@String { chansen      = {Christian B. Hansen} }
@String{chzhang      = {Cun-Hui Zhang}}
@String { csda         = {Computational Statistics \& Data Analysis} }
@String { csda_s       = {Comput. Stat. Data Anal.} }
@String { ddonoho      = { David L. Donoho } }
@String { dhusmeier    = { Dirk Husmeier } }
@String { dmason       = {David M. Mason} }
@String { druppert     = { David Ruppert } }
@String { dwitten      = { Daniela M. Witten } }
@String { eacastro     = {Ery Arias-Castro} }
@String { ecandes      = {Emmanuel J. Cand\`{e}s} }

@String { ecp          = {Electronic Communications in Probability} }
@String { ecp_s        = {Electron. Commun. Probab.} }
@String { egine        = {Evarist Gin\'{e} } }
@String { ejs          = { Electronic Journal of Statistics } }
@String { ejs_s        = { Electron. J. Stat. } }
@String { elevina      = { Elizaveta Levina } }
@String { epxing       = { Eric P. Xing } }
@String { fbach        = {Francis R. Bach} }
@String { fdondelinger = { Frank Dondelinger } }
@String { gallen       = {Genevera I. Allen} }
@String { graskutti    = { Garvesh Raskutti } }
@String { hhzhang      = {Hao Helen Zhang} }
@String { hleeb        = {Hannes Leeb} }
@String { hliu         = { Han Liu } }
@String { hwang        = { Hansheng Wang } }
@String { hzhou        = { Harrison H. Zhou } }
@String { hzou         = { Hui Zou } }
@String { ICML2010_s   = {27th Int. Conf. Mach. Learn.} }
@String { ICML2012_s   = {29th Int. Conf. Mach. Learn.} }
@String { ICML2013_s   = {30th Int. Conf. Mach. Learn.} }
@String { ICML2014_s   = {31st Int. Conf. Mach. Learn.} }
@String { IEEEit       = { IEEE Transactions on Information Theory } }
@String { IEEEit_s     = { IEEE Trans. Inf. Theory } }
@String { IEEEspm_s    = {IEEE Signal Process. Mag.} }
@String { ijohnstone   = { Iain M. Johnstone } }
@String { jasa         = {Journal of the American Statistical Association } }
@String { jasa_s       = { J. Am. Stat. Assoc. } }
@String { jbes_s       = {J. Bus. Econom. Statist.} }
@String { jcgs         = {Journal of Computational and Graphical Statistics} }
@String { jcgs_s       = {J. Comp. Graph. Stat.} }
@String { jdlee        = {Jason D. Lee} }


@String { jduchi       = { John C. Duchi } }
@String { jfan         = { Jianqing Fan } }
@String { jfried       = { Jerome H. Friedman } }
@String { jhaupt       = {Jarvis D. Haupt} }
@String { jhorowitz    = { Joel L. Horowitz } }
@String { jhuang       = {Jian Huang} }
@String{jjankova     = {Jana Jankov\'a}}
@String { jlafferty    = { John D. Lafferty } }
@String { jlv          = { Jinchi Lv } }
@String { jma          = {Journal of Multivariate Analysis} }
@String { jma_s        = {J. Multivar. Anal.} }
@String { jmlr         = { Journal Of Machine Learning Research } }
@String { jmlr_s       = { J. Mach. Learn. Res. } }
@String { jrssb        = { Journal of the Royal Statistical Society: Series B } }
@String { jrssb_s      = { J. R. Stat. Soc. B } }
@String { jspi_s       = {J. Statist. Plann. Inference} }
@String { jtaylor      = {Jonathan E. Taylor} }
@String { jtropp       = { Joel A. Tropp } }
@String { jwellner     = {Jon A. Wellner} }
@String { jzhu         = { Ji Zhu } }
@String { klange       = {Kenneth L. Lange} }

@String { klounici     = {Karim Lounici } }
@String { lbrown       = {Lawrence D. Brown} }
@String { lsong        = { Le Song } }
@String { lwasser      = { Larry~A. Wasserman } }
@String { machlearn    = { Machine Learning } }
@String { machlearn_s  = { Mach. Learn. } }@String { mbalcan      = {Maria-Florina Balcan} }

@String { mdavenport   = {Mark A. Davenport} }
@String { mdrton       = { Mathias Drton } }
@String { mfarrell     = {Max H. Farrell} }
@String { mgrzegorczyk = { Marco Grzegorczyk } }
@String { mjordan      = { Michael I. Jordan } }
@String { mkolar       = { Mladen Kolar } }
@String { mmalloy      = {Matthew L. Malloy } }
@String { mperlman     = { Michael D. Perlman } }
@String { mwainw       = { Martin J. Wainwright } }
@String { mwakin       = {Michael B. Wakin} }
@String { myuan        = { Ming Yuan } }
@String { NIPS_s       = {Adv. Neural Inf. Process. Syst.} }
@String { nmeins       = { Nicolas Meinshausen } }
@String { nsimon       = {Noah Simon} }@String { nsrebro      = {Nathan Srebro} }
@String { pbartlett    = {Peter L. Bartlett} }

@String { pbickel      = { Peter J. Bickel } }
@String { pbuhl        = { Peter B\"{u}hlmann} }
@String { phall        = {Peter Hall} }
@String { phoff        = {Peter Hoff} }
@String { ploh         = {Po-Ling Loh} }
@String { pnas         = { Proceedings of the National Academy of Sciences of the United States of America } }
@String { pnas_s       = { Proc. Natl. Acad. Sci. U.S.A. } }
@String { pravik       = { Pradeep Ravikumar } }
@String { PROC_s       = {Proc.} }
@String { ptrf         = {Probability Theory and Related Fields} }
@String { ptrf_s       = {Probab. Theory Related Fields} }
@String { rbaraniuk    = {Richard G. Baraniuk} }
@String { rcastro      = {Rui M. Castro} }@String { rdevore      = {Ronald DeVore} }
@String { rfoygel      = {Rina {Foygel Barber}} }

@String { rli          = { Runze Li } }
@String { rmazumder    = { Rahul Mazumder } }
@String { rnowak       = {Robert D. Nowak} }
@String { rsamworth    = {Richard J. Samworth} }
@String { rtibs        = {Robert J. Tibshirani } }
@String { sboyd        = { Stephen P. Boyd } }
@String { sigproc      = { IEEE Transactions on Signal Processing } }
@String { sigproc_s    = { IEEE Trans. Signal Proces. } }
@String { sjs          = {Scandinavian Journal of Statistics} }
@String { sjs_s        = {Scand. J. Stat.} }
@String { slauritzen   = {Steffen L. Lauritzen} }
@String { slebre       = { Sophie L\'{e}bre } }
@String { snegahban    = {Sahand Negahban} }
@String { sportnoy     = {Stephen L. Portnoy} }
@String { statcomp     = { Statistics and Computing } }
@String { statcomp_s   = { Stat. Comput. } }
@String { statprobl    = { Statistics $\&$ Probability Letters } }
@String { statprobl_s  = { Statist. Probab. Lett. } }
@String { statsci      = {Statistical Science} }
@String { statsci_s    = {Stat. Sci.} }
@String { statsin      = { Statistica Sinica } }
@String { statsin_s    = { Stat. Sinica } }
@String { svdgeer      = { Sara A. van de Geer } }
@String { tams         = { The Annals of Mathematical Statistics } }
@String { tams_s       = { Ann. Math. Stat. } }
@String { tcai         = { T. Tony Cai } }
@String { thastie      = { Trevor J. Hastie } }
@String { tzhang       = {Tong Zhang} }
@String { vdlpena      = {Victor {de la Pena}} }
@String { vkoltch      = {Vladimir Koltchinskii} }
@String { wbwu         = { Wei Biao Wu } }
@String { whardle      = {Wolfgang H{\"a}rdle} }
@String { wliu         = {Weidong Liu} }
@String { yingster     = {Yuri I. Ingster} }

@InProceedings{Lazaric2010Bayesian,
  author    = {Alessandro Lazaric and Mohammad Ghavamzadeh},
  title     = {Bayesian Multi-Task Reinforcement Learning},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel},
  year      = {2010},
  pages     = {599--606},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/icml/LazaricG10},
  crossref  = {DBLP:conf/icml/2010},
  timestamp = {Fri, 12 Jun 2015 19:15:11 +0200},
  url       = {http://www.icml2010.org/papers/269.pdf},
}

@InProceedings{Jain2015Fast,
  author    = {Prateek Jain and Praneeth Netrapalli},
  title     = {Fast Exact Matrix Completion with Finite Samples},
  booktitle = {Proceedings of The 28th Conference on Learning Theory, {COLT} 2015, Paris, France, July 3-6, 2015},
  year      = {2015},
  pages     = {1007--1034},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/colt/0002N15},
  crossref  = {DBLP:conf/colt/2015},
  timestamp = {Tue, 12 Jul 2016 21:51:13 +0200},
  url       = {http://jmlr.org/proceedings/papers/v40/Jain15.html},
}

@InProceedings{Hardt2014Fast,
  author    = {Moritz Hardt and Mary Wootters},
  title     = {Fast matrix completion without the condition number},
  booktitle = {Proceedings of The 27th Conference on Learning Theory, {COLT} 2014, Barcelona, Spain, June 13-15, 2014},
  year      = {2014},
  pages     = {638--678},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/colt/HardtW14},
  crossref  = {DBLP:conf/colt/2014},
  timestamp = {Tue, 12 Jul 2016 21:51:13 +0200},
  url       = {http://jmlr.org/proceedings/papers/v35/hardt14a.html},
}

@InProceedings{Hardt2014Computational,
  author    = {Moritz Hardt and Raghu Meka and Prasad Raghavendra and Benjamin Weitz},
  title     = {Computational Limits for Matrix Completion},
  booktitle = {Proceedings of The 27th Conference on Learning Theory, {COLT} 2014, Barcelona, Spain, June 13-15, 2014},
  year      = {2014},
  pages     = {703--725},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/colt/HardtMRW14},
  crossref  = {DBLP:conf/colt/2014},
  file      = {:Hardt2014Computational.pdf:PDF},
  timestamp = {Tue, 12 Jul 2016 21:51:13 +0200},
  url       = {http://jmlr.org/proceedings/papers/v35/hardt14b.html},
}

@InProceedings{Wytock2013Sparse,
  author    = {Matt Wytock and J. Zico Kolter},
  title     = {Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013},
  year      = {2013},
  volume    = {28},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {1265--1273},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/WytockK13},
  file      = {:Wytock2013Sparse.pdf:PDF},
  timestamp = {Wed, 03 Apr 2019 18:02:19 +0200},
  url       = {http://jmlr.org/proceedings/papers/v28/wytock13.html},
}

@Article{dAspremont2008First,
  Title                    = {First-order methods for sparse covariance selection},
  Author                   = {Alexandre d'Aspremont and Onureena Banerjee and Laurent {El Ghaoui}},
  Journal                  = {SIAM J. Matrix Anal. Appl.},
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {56--66},
  Volume                   = {30},

  Doi                      = {10.1137/060670985},
  Fjournal                 = {SIAM Journal on Matrix Analysis and Applications},
  ISSN                     = {0895-4798},
  Mrclass                  = {62H20 (65F50)},
  Mrnumber                 = {2399568 (2009e:62263)},
  Mrreviewer               = {Reza Modarres},
  Url                      = {http://dx.doi.org/10.1137/060670985}
}

@Article{amr09tesla,
  Title                    = {Recovering Time-varying Networks Of Dependencies In Social And Biological Studies},
  Author                   = aahmed # { and } # epxing,
  Journal                  = pnas_s,
  Year                     = {2009},
  Number                   = {29},
  Pages                    = {11878--11883},
  Volume                   = {106},

  Newspaper                = {Proc. Natl. Acad. Sci. U.S.A.}
}

@Article{Buja1989Linear,
  Title                    = {Linear Smoothers and Additive Models},
  Author                   = abuja # { and } # thastie # { and } # rtibs,
  Journal                  = aos_s,
  Year                     = {1989},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {453--510},
  Volume                   = {17},

  Doi                      = {10.1214/aos/1176347115},
  ISSN                     = {0090-5364},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1214/aos/1176347115}
}

@Article{Dalalyan2014Statistical,
  Title                    = {Statistical inference in compound functional models},
  Author                   = adalalyan # { and Yuri Ingster and } # atsybakov,
  Journal                  = {Probab. Theory Related Fields},
  Year                     = {2014},
  Number                   = {3-4},
  Pages                    = {513--532},
  Volume                   = {158},

  Doi                      = {10.1007/s00440-013-0487-y},
  Fjournal                 = {Probability Theory and Related Fields},
  ISSN                     = {0178-8051},
  Mrclass                  = {62G05 (60G15 62G08 62H12)},
  Mrnumber                 = {3176357},
  Url                      = {http://dx.doi.org/10.1007/s00440-013-0487-y}
}

@Article{Dawid2007geometry,
  Title                    = {The geometry of proper scoring rules},
  Author                   = adawid,
  Journal                  = aism_s,
  Year                     = {2007},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {77--93},
  Volume                   = {59},

  Doi                      = {10.1007/s10463-006-0099-8},
  ISSN                     = {1572-9052},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10463-006-0099-8}
}

@Article{Dawid2012Proper_discrete,
  Title                    = {Proper local scoring rules on discrete sample spaces},
  Author                   = adawid # { and } # slauritzen # { and Matthew F. Parry},
  Journal                  = aos_s,
  Year                     = {2012},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {593--608},
  Volume                   = {40},

  Doi                      = {10.1214/12-aos972},
  ISSN                     = {0090-5364},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1214/12-AOS972}
}

@Article{Dawid2014Theory,
  Title                    = {Theory and Applications of Proper Scoring Rules},
  Author                   = adawid # { and Monica Musio},
  Journal                  = {Metron},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {169--183},
  Volume                   = {72},

  Doi                      = {10.1007/s40300-014-0039-y},
  Fjournal                 = {Metron},
  ISSN                     = {0026-1424},
  Mrclass                  = {62C05 (62C10)},
  Mrnumber                 = {3233147},
  Url                      = {http://dx.doi.org/10.1007/s40300-014-0039-y}
}

@InProceedings{lozano09grouped,
  author =    alozano #{ and G. Swirszcz and N. Abe},
  title =     {Grouped Orthogonal Matching Pursuit For Variable Selection And Prediction},
  booktitle = {Proc. of NIPS},
  year =      {2009},
  editor =    {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  pages =     {1150--1158},
  file =      {:lozano09grouped.pdf:PDF},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Article{rothman08spice,
  Title                    = {Sparse Permutation Invariant Covariance Estimation},
  Author                   = arothman # { and } # pbickel # { and } # elevina # { and J. Zhu},
  Journal                  = ejs_s,
  Year                     = {2008},
  Pages                    = {494--515},
  Volume                   = {2},

  Doi                      = {10.1214/08-EJS176},
  ISSN                     = {1935-7524},
  Mrnumber                 = {2417391 (2010a:62186)},
  Newspaper                = {Electron. J. Stat.},
  Url                      = {http://dx.doi.org/10.1214/08-EJS176}
}

@Book{tsybakov09introduction,
  Title                    = {Introduction To Nonparametric Estimation},
  Author                   = atsybakov,
  Publisher                = {Springer},
  Year                     = {2009},

  Address                  = {New York},
  Note                     = {Revised and extended from the 2004 French original, Translated by Vladimir Zaiats},
  Series                   = {Springer Series in Statistics},

  Doi                      = {10.1007/b13794},
  ISBN                     = {978-0-387-79051-0},
  Mrnumber                 = {2724359 (2011g:62006)},
  Pages                    = {xii+214},
  Url                      = {http://dx.doi.org/10.1007/b13794}
}

@Book{vanderVaart1996Weak,
  Title                    = {Weak Convergence and Empirical Processes: With Applications to Statistics},
  Author                   = avdvaart # { and } # jwellner,
  Publisher                = {Springer},
  Year                     = {1996},

  ISBN                     = {1475725477},
  Url                      = {http://www.amazon.com/Weak-Convergence-Empirical-Processes-Applications/dp/0387946403%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387946403}
}

@Article{Efron1969Students,
  Title                    = {Student's {$t$}-test under symmetry conditions},
  Author                   = befron,
  Journal                  = jasa_s,
  Year                     = {1969},
  Pages                    = {1278--1302},
  Volume                   = {64},

  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62.10},
  Mrnumber                 = {0251826 (40 \#5053)}
}

@Article{Poetscher2009Confidence,
  author =    bpotscher,
  title =     {Confidence sets based on sparse estimators are necessarily large},
  journal =   {Sankhy{\=a}},
  year =      {2009},
  volume =    {71},
  number =    {1, Ser. A},
  pages =     {1--18},
  file =      {:Poetscher2009Confidence.pdf:PDF},
  publisher = {JSTOR}
}

@Article{Poetscher2009distribution,
  author =    bpotscher #{ and } # hleeb,
  title =     {On the distribution of penalized maximum likelihood estimators: The {LASSO}, {SCAD}, and thresholding},
  journal =   jma_s,
  year =      {2009},
  volume =    {100},
  number =    {9},
  pages =     {2065--2082},
  month =     {oct},
  doi =       {10.1016/j.jmva.2009.06.010},
  file =      {:Poetscher2009distribution.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier {BV}},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1016/j.jmva.2009.06.010}
}

@Article{zhang2010nearly,
  Title                    = {Nearly Unbiased Variable Selection Under Minimax Concave Penalty},
  Author                   = chzhang,
  Journal                  = aos_s,
  Year                     = {2010},
  Number                   = {2},
  Pages                    = {894--942},
  Volume                   = {38},

  Doi                      = {10.1214/09-AOS729},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2604701 (2011d:62211)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/09-AOS729}
}

@Article{Zhang2012General,
  Title                    = {A General Theory of Concave Regularization for High-Dimensional Sparse Estimation Problems},
  Author                   = chzhang # { and } # tzhang,
  Journal                  = statsci_s,
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {576--593},
  Volume                   = {27},

  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2013.10.15},
  Url                      = {http://projecteuclid.org/euclid.ss/1356098557}
}

@Article{zhang08sparsity,
  Title                    = {The Sparsity And Bias Of The {lasso} Selection In High-dimensional Linear Regression},
  Author                   = chzhang # { and J. Huang},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {1567--1594},
  Volume                   = {36},

  Doi                      = {10.1214/07-AOS520},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2435448 (2010h:62204)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/07-AOS520}
}

@Article{Zhang2011Confidence,
  Title                    = {Confidence intervals for low dimensional parameters in high dimensional linear models},
  Author                   = chzhang # { and Stephanie S. Zhang},
  Journal                  = JRSSB_s,
  Year                     = {2013},

  Month                    = {Jul},
  Number                   = {1},
  Pages                    = {217--242},
  Volume                   = {76},

  Doi                      = {10.1111/rssb.12026},
  ISSN                     = {1369-7412},
  Owner                    = {mkolar},
  Publisher                = {Wiley-Blackwell},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1111/rssb.12026}
}

@Article{Donoho2006Compressed,
  Title                    = {Compressed sensing},
  Author                   = ddonoho,
  Journal                  = IEEEit_s,
  Year                     = {2006},
  Number                   = {4},
  Pages                    = {1289--1306},
  Volume                   = {52},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066}
}

@Article{donoho2006most,
  Title                    = {For Most Large Underdetermined Systems Of Linear Equations The Minimal {$l\sb 1$}-norm Solution Is Also The Sparsest Solution},
  Author                   = ddonoho,
  Journal                  = {Comm. Pure Appl. Math.},
  Year                     = {2006},
  Number                   = {6},
  Pages                    = {797--829},
  Volume                   = {59},

  Doi                      = {10.1002/cpa.20132},
  ISSN                     = {0010-3640},
  Mrnumber                 = {2217606 (2007a:15004)},
  Newspaper                = {Comm. Pure Appl. Math.},
  Url                      = {http://dx.doi.org/10.1002/cpa.20132}
}

@Article{donoho2003optimally,
  Title                    = {Optimally Sparse Representation In General (nonorthogonal) Dictionaries Via {$l\sp 1$} Minimization},
  Author                   = ddonoho # { and M. Elad},
  Journal                  = pnas_s,
  Year                     = {2003},
  Number                   = {5},
  Pages                    = {2197--2202 (electronic)},
  Volume                   = {100},

  Doi                      = {10.1073/pnas.0437847100},
  ISSN                     = {1091-6490},
  Mrnumber                 = {1963681 (2004c:94068)},
  Newspaper                = {Proc. Natl. Acad. Sci. U.S.A.}
}

@Article{Donoho2001Uncertainty,
  Title                    = {Uncertainty principles and ideal atomic decomposition},
  Author                   = ddonoho # { and Xiaoming Huo},
  Journal                  = IEEEit_s,
  Year                     = {2001},
  Number                   = {7},
  Pages                    = {2845--2862},
  Volume                   = {47},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=959265}
}

@Article{donoho2004higher,
  Title                    = {Higher Criticism For Detecting Sparse Heterogeneous Mixtures},
  Author                   = ddonoho # { and J. Jin},
  Journal                  = aos_s,
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {962--994},
  Volume                   = {32},

  Doi                      = {10.1214/009053604000000265},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2065195 (2005e:62066)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/009053604000000265}
}

@Article{Mason2011general,
  Title                    = {A general result on the uniform in bandwidth consistency of kernel-type function estimators},
  Author                   = dmason # { and Jan W. H. Swanepoel},
  Journal                  = {TEST},
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {72--94},
  Volume                   = {20},

  Doi                      = {10.1007/s11749-010-0188-0},
  Fjournal                 = {TEST},
  ISSN                     = {1133-0686},
  Mrclass                  = {62G08 (60F15 62G07)},
  Mrnumber                 = {2806311 (2012g:62170)},
  Mrreviewer               = {Dimitris A. Ioannides},
  Url                      = {http://dx.doi.org/10.1007/s11749-010-0188-0}
}

@Article{Witten2011New,
  Title                    = {New Insights And Faster Computations For The Graphical Lasso},
  Author                   = dwitten # { and } # jfried # { and N. Simon},
  Journal                  = {J. Comput. Graph. Stat.},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {892--900},
  Volume                   = {20},

  Newspaper                = {J. Comput. Graph. Stat.},
  Publisher                = {ASA}
}

@Article{Witten2009Covariance,
  Title                    = {Covariance-regularized regression and classification for high dimensional problems},
  Author                   = dwitten # { and } # rtibs,
  Journal                  = jrssb_s,
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {615--636},
  Volume                   = {71},

  Publisher                = {Wiley Online Library},
  Timestamp                = {2013.10.29},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00699.x/full}
}

@Article{witten2011lda,
  Title                    = {Penalized classification using {F}isher's linear discriminant},
  Author                   = dwitten # { and } # rtibs,
  Journal                  = jrssb_s,
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {753--772},
  Volume                   = {73},

  Doi                      = {10.1111/j.1467-9868.2011.00783.x},
  Fjournal                 = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  ISSN                     = {1369-7412},
  Mrclass                  = {62H30 (62J07)},
  Mrnumber                 = {2867457 (2012k:62185)},
  Mrreviewer               = {David Benner Hitchcock},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2011.00783.x}
}

@Article{Arias-Castro2012Detecting,
  author =    eacastro,
  title =     {Detecting a vector based on linear measurements},
  journal =   ejs_s,
  year =      {2012},
  volume =    {6},
  pages =     {547--558},
  file =      {Arias-Castro2012Detecting.pdf:Arias-Castro2012Detecting.pdf:PDF},
  owner =     {mkolar},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.02.18},
  url =       {http://projecteuclid.org/euclid.ejs/1334065321}
}

@Article{Arias-Castro2013Fundamental,
  Title                    = {On the Fundamental Limits of Adaptive Sensing},
  Author                   = eacastro # { and } # ecandes # { and } # mdavenport,
  Journal                  = IEEEit_s,
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {472-481},
  Volume                   = {59},

  Doi                      = {10.1109/TIT.2012.2215837},
  Owner                    = {mkolar},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1109/TIT.2012.2215837}
}

@Article{Arias-Castro2011Detection,
  Title                    = {Detection of an anomalous cluster in a network},
  Author                   = eacastro # { and } # ecandes # { and Arnaud Durand},
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {278--304},
  Volume                   = {39},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.aos/1291388376}
}

@Article{Arias-Castro2011Global,
  Title                    = {Global testing under sparse alternatives: ANOVA, multiple comparisons and the higher criticism},
  Author                   = eacastro # { and } # ecandes # { and Yaniv Plan},
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {2533--2556},
  Volume                   = {39},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.aos/1322663467}
}

@Article{Candes2013How,
  Title                    = {How well can we estimate a sparse vector?},
  Author                   = ecandes # { and } # mdavenport,
  Journal                  = acha_s,
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {317--323},
  Volume                   = {34},

  Owner                    = {mkolar},
  Publisher                = {Elsevier},
  Timestamp                = {2014.02.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1063520312001388}
}

@Article{Candes2008introduction,
  Title                    = {An introduction to compressive sampling},
  Author                   = ecandes # { and } # mwakin,
  Journal                  = IEEEspm_s,
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {21--30},
  Volume                   = {25},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4472240}
}

@Article{Candes2005Decoding,
  Title                    = {Decoding by linear programming},
  Author                   = ecandes # { and Terence Tao},
  Journal                  = IEEEit_s,
  Year                     = {2005},
  Number                   = {12},
  Pages                    = {4203--4215},
  Volume                   = {51},

  Coden                    = {IETTAW},
  Doi                      = {10.1109/TIT.2005.858979},
  Fjournal                 = {Institute of Electrical and Electronics Engineers.
 Transactions on Information Theory},
  ISSN                     = {0018-9448},
  Mrclass                  = {94B35 (90C08)},
  Mrnumber                 = {2243152 (2007b:94313)},
  Mrreviewer               = {Josep Rif{\`a}},
  Url                      = {http://dx.doi.org/10.1109/TIT.2005.858979}
}

@Article{Candes2006Near,
  Title                    = {Near-optimal signal recovery from random projections: Universal encoding strategies?},
  Author                   = ecandes # { and Terence Tao},
  Journal                  = IEEEit_s,
  Year                     = {2006},
  Number                   = {12},
  Pages                    = {5406--5425},
  Volume                   = {52},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4016283}
}

@Article{Candes2007dantzig,
  Title                    = {The {d}antzig Selector: Statistical Estimation When {$p$} Is Much Larger Than {$n$}},
  Author                   = ecandes # { and T. Tao},
  Journal                  = aos_s,
  Year                     = {2007},
  Number                   = {6},
  Pages                    = {2313--2351},
  Volume                   = {35},

  Doi                      = {10.1214/009053606000001523},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2382644 (2009b:62016)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/009053606000001523}
}

@Article{Gine2007Laws,
  Title                    = {Laws of the iterated logarithm for the local U-statistic process},
  Author                   = egine # { and } # dmason,
  Journal                  = {J. Theoret. Probab.},
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {457--485},
  Volume                   = {20},

  Owner                    = {mkolar},
  Publisher                = {Springer},
  Timestamp                = {2014.03.20},
  Url                      = {http://link.springer.com/article/10.1007/s10959-007-0067-0}
}

@Article{Bach2008Consistency,
  author   = fbach,
  title    = {Consistency of the group lasso and multiple kernel learning},
  journal  = jmlr_s,
  year     = {2008},
  volume   = {9},
  pages    = {1179--1225},
  issn     = {1532-4435},
  file     = {:Bach2008Consistency.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {68T05 (60B11 62G05)},
  mrnumber = {2417268 (2010a:68132)},
}

@Article{Bach2015equivalence,
  author =    fbach,
  title =     {On the equivalence between quadrature rules and random features},
  journal =   {ArXiv e-prints, arXiv:1502.06800},
  year =      {2015},
  file =      {:Bach2015equivalence.pdf:PDF},
  owner =     {mkolar},
  timestamp = {2016.02.23}
}

@Article{Bach2011Optimization,
  author =    fbach #{ and R. Jenatton and J. Mairal and G. Obozinski},
  title =     {Optimization with Sparsity-Inducing Penalties},
  journal =   {Found. Trends Mach. Learn.},
  year =      {2011},
  volume =    {4},
  number =    {1},
  pages =     {1--106},
  doi =       {10.1561/2200000015},
  file =      {:Bach2011Optimization.pdf:PDF},
  issn =      {1935-8245},
  publisher = {Now Publishers},
  url =       {http://dx.doi.org/10.1561/2200000015}
}

@Article{Allen2012Log,
  Title                    = {A Log-Linear Graphical Model for Inferring Genetic Networks from High-Throughput Sequencing Data},
  Author                   = gallen # { and Zhandong Liu},
  Journal                  = {ArXiv e-prints, arXiv:1204.3941},
  Year                     = {2012},

  Month                    = apr,

  Abstract                 = {Gaussian graphical models are often used to infer gene networks based on microarray expression data. Many scientists, however, have begun using high-throughput sequencing technologies to measure gene expression. As the resulting high-dimensional count data consists of counts of sequencing reads for each gene, Gaussian graphical models are not optimal for modeling gene networks based on this discrete data. We develop a novel method for estimating high-dimensional Poisson graphical models, the Log-Linear Graphical Model, allowing us to infer networks based on high-throughput sequencing data. Our model assumes a pair-wise Markov property: conditional on all other variables, each variable is Poisson. We estimate our model locally via neighborhood selection by fitting 1-norm penalized log-linear models. Additionally, we develop a fast parallel algorithm, an approach we call the Poisson Graphical Lasso, permitting us to fit our graphical model to high-dimensional genomic data sets. In simulations, we illustrate the effectiveness of our methods for recovering network structure from count data. A case study on breast cancer microRNAs, a novel application of graphical models, finds known regulators of breast cancer genes and discovers novel microRNA clusters and hubs that are targets for future research.},
  Eprint                   = {1204.3941},
  Oai2identifier           = {1204.3941}
}

@Article{Zhang2011Linear,
  author =     hhzhang #{ and Guang Cheng and Yufeng Liu},
  title =      {Linear or nonlinear? {A}utomatic structure discovery for partially linear models},
  journal =    jasa_s,
  year =       {2011},
  volume =     {106},
  number =     {495},
  pages =      {1099--1112},
  coden =      {JSTNAL},
  doi =        {10.1198/jasa.2011.tm10281},
  file =       {Zhang2011Linear.pdf:Zhang2011Linear.pdf:PDF},
  fjournal =   {Journal of the American Statistical Association},
  issn =       {0162-1459},
  mrclass =    {62G08 (62G10 62P25)},
  mrnumber =   {2894767},
  mrreviewer = {Feng Yao},
  url =        {http://dx.doi.org/10.1198/jasa.2011.tm10281}
}

@Article{Zhang2006Component,
  Title                    = {Component selection and smoothing for nonparametric regression in exponential families},
  Author                   = hhzhang # { and Yi Lin},
  Journal                  = statsin_s,
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {1021--1041},
  Volume                   = {16},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62G05 (62G08 62J10)},
  Mrnumber                 = {2281313}
}

@Article{Leeb2005Model,
  author =    hleeb #{ and } # bpotscher,
  title =     {Model Selection And Inference: Facts And Fiction},
  journal =   {Econ. Theory},
  year =      {2005},
  volume =    {21},
  number =    {01},
  month =     {feb},
  doi =       {10.1017/s0266466605050036},
  file =      {Leeb2005Model.pdf:Leeb2005Model.pdf:PDF},
  owner =     {mkolar},
  publisher = {Cambridge University Press ({CUP})},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1017/S0266466605050036}
}

@Article{Leeb2007Can,
  author =    hleeb #{ and } # bpotscher,
  title =     {Can One Estimate The Unconditional Distribution Of Post-model-selection Estimators?},
  journal =   {Econ. Theory},
  year =      {2007},
  volume =    {24},
  number =    {02},
  pages =     {338--376},
  month =     {Nov},
  doi =       {10.1017/s0266466608080158},
  file =      {Leeb2007Can.pdf:Leeb2007Can.pdf:PDF},
  issn =      {1469-4360},
  publisher = {Cambridge University Press (CUP)},
  url =       {http://dx.doi.org/10.1017/S0266466608080158}
}

@Article{Leeb2008Sparse,
  author =    hleeb #{ and } # bpotscher,
  title =     {Sparse estimators and the oracle property, or the return of Hodges' estimator},
  journal =   {Journal of Econometrics},
  year =      {2008},
  volume =    {142},
  number =    {1},
  pages =     {201--211},
  month =     {jan},
  doi =       {10.1016/j.jeconom.2007.05.017},
  file =      {:Leeb2008Sparse.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier {BV}},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1016/j.jeconom.2007.05.017}
}

@Article{Liu2009Nonparanormal:,
  author =    hliu #{ and } # jlafferty #{and } # lwasser,
  title =     {The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs},
  journal =   jmlr_s,
  year =      {2009},
  volume =    {10},
  pages =     {2295--2328},
  file =      {:Liu2009Nonparanormal\:.pdf:PDF},
  publisher = {JMLR. org}
}

@InProceedings{Liu2012Transelliptical,
  author    = {Han Liu and Fang Han and Cun{-}Hui Zhang},
  title     = {Transelliptical Graphical Models},
  booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States.},
  year      = {2012},
  editor    = {Peter L. Bartlett and Fernando C. N. Pereira and Christopher J. C. Burges and L{\'{e}}on Bottou and Kilian Q. Weinberger},
  pages     = {809--817},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/LiuHZ12},
  timestamp = {2019.05.02},
  url       = {http://papers.nips.cc/paper/4822-transelliptical-graphical-models},
}

@Article{Liu2012High,
  author =   hliu #{ and Fang Han and } # myuan #{ and } # jlafferty #{ and } # lwasser,
  title =    {High-dimensional semiparametric {G}aussian copula graphical models},
  journal =  aos_s,
  year =     {2012},
  volume =   {40},
  number =   {4},
  pages =    {2293--2326},
  doi =      {10.1214/12-AOS1037},
  file =     {Liu2012High.pdf:Liu2012High.pdf:PDF},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62G05 (62F12 62G20 62H20 62H99)},
  mrnumber = {3059084},
  url =      {http://dx.doi.org/10.1214/12-AOS1037}
}

@InProceedings{han09blockwise,
  Title                    = {Blockwise Coordinate Descent Procedures For The Multi-task Lasso, With Applications To Neural Semantic Basis Discovery},
  Author                   = hliu # { and M. Palatucci and J. Zhang},
  Booktitle                = {Proc. of ICML},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {649--656}
}

@Article{Liu2012TIGER,
  author    = {Liu, Han and Wang, Lie},
  title     = {T{IGER}: a tuning-insensitive approach for optimally estimating {G}aussian graphical models},
  journal   = {Electron. J. Stat.},
  year      = {2017},
  volume    = {11},
  number    = {1},
  pages     = {241--294},
  issn      = {1935-7524},
  doi       = {10.1214/16-EJS1195},
  fjournal  = {Electronic Journal of Statistics},
  mrclass   = {62H99 (05C80 05C90 62H12)},
  mrnumber  = {3606771},
  timestamp = {2019.05.02},
  url       = {https://doi.org/10.1214/16-EJS1195},
}

@InProceedings{Liu2009Estimation,
  author =    hliu #{ and Jian Zhang},
  title =     {Estimation consistency of the group lasso and its applications},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year =      {2009},
  pages =     {376--383}
}

@Article{Wang2009Forward,
  Title                    = {Forward Regression For Ultra-high Dimensional Variable Screening},
  Author                   = hwang,
  Journal                  = jasa_s,
  Year                     = {2009},
  Number                   = {488},
  Pages                    = {1512--1524},
  Volume                   = {104},

  Doi                      = {10.1198/jasa.2008.tm08516},
  ISSN                     = {0162-1459},
  Mrnumber                 = {2750576},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1198/jasa.2008.tm08516}
}

@Article{Wang2007Robust,
  Title                    = {Robust regression shrinkage and consistent variable selection
 through the {LAD}-{L}asso},
  Author                   = hwang # { and Guodong Li and Guohua Jiang},
  Journal                  = jbes_s,
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {347--355},
  Volume                   = {25},

  Doi                      = {10.1198/073500106000000251},
  Fjournal                 = {Journal of Business \& Economic Statistics},
  ISSN                     = {0735-0015},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2380753},
  Url                      = {http://dx.doi.org/10.1198/073500106000000251}
}

@Article{wang08shrinkage,
  author =    hwang #{ and Yingcun Xia},
  title =     {Shrinkage Estimation Of The Varying Coefficient Model},
  journal =   jasa_s,
  year =      {2009},
  volume =    {104},
  number =    {486},
  pages =     {747--757},
  doi =       {10.1198/jasa.2009.0138},
  file =      {wang08shrinkage.pdf:wang08shrinkage.pdf:PDF},
  issn =      {0162-1459},
  mrnumber =  {2541592 (2010j:62211)},
  newspaper = {J. Am. Stat. Assoc.},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.1198/jasa.2009.0138}
}

@Article{zou06adaptive,
  Title                    = {The Adaptive Lasso And Its Oracle Properties},
  Author                   = hzou,
  Journal                  = jasa_s,
  Year                     = {2006},
  Number                   = {476},
  Pages                    = {1418--1429},
  Volume                   = {101},

  Doi                      = {10.1198/016214506000000735},
  ISSN                     = {0162-1459},
  Mrnumber                 = {2279469 (2008d:62024)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Url                      = {http://dx.doi.org/10.1198/016214506000000735}
}

@Article{Zou2008Composite,
  Title                    = {Composite quantile regression and the oracle model selection
 theory},
  Author                   = hzou # { and } # myuan,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {1108--1126},
  Volume                   = {36},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/07-AOS507},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J05 (62J07)},
  Mrnumber                 = {2418651 (2009i:62146)},
  Mrreviewer               = {Lutz Edler},
  Url                      = {http://dx.doi.org/10.1214/07-AOS507}
}

@Article{zou2005regularization,
  Title                    = {Regularization And Variable Selection Via The Elastic Net},
  Author                   = hzou # { and } # thastie,
  Journal                  = jrssb_s,
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {301--320},
  Volume                   = {67},

  Doi                      = {10.1111/j.1467-9868.2005.00503.x},
  ISSN                     = {1369-7412},
  Mrnumber                 = {2137327},
  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2005.00503.x}
}

@Article{zou08onestep,
  Title                    = {One-step Sparse Estimates In Nonconcave Penalized Likelihood Models},
  Author                   = hzou # { and R. Li},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {1509--1533},
  Volume                   = {36},

  Doi                      = {10.1214/009053607000000802},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2435443 (2010a:62222)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053607000000802}
}

@Article{zou08finfinity,
  Title                    = {The {$F\sb \infty$}-norm Support Vector Machine},
  Author                   = hzou # { and M. Yuan},
  Journal                  = statsin,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {379--398},
  Volume                   = {18},

  ISSN                     = {1017-0405},
  Mrnumber                 = {2416909},
  Newspaper                = {Stat. Sinica},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InCollection{johnstone2001chi,
  Title                    = {Chi-square Oracle Inequalities},
  Author                   = ijohnstone,
  Booktitle                = {State of the art in probability and statistics ({L}eiden, 1999)},
  Publisher                = {Inst. Math. Statist.},
  Year                     = {2001},

  Address                  = {Beachwood, OH},
  Pages                    = {399--418},
  Series                   = {IMS Lecture Notes Monogr. Ser.},
  Volume                   = {36},

  Doi                      = {10.1214/lnms/1215090080},
  Mrnumber                 = {1836572 (2002f:62012)},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/lnms/1215090080}
}

@Article{johnstone2009consistency,
  Title                    = {On consistency and sparsity for principal components analysis in high dimensions},
  Author                   = ijohnstone # { and Arthur Yu Lu},
  Journal                  = jasa_s,
  Year                     = {2009},
  Number                   = {486},
  Pages                    = {682--693},
  Volume                   = {104},

  Coden                    = {JSTNAL},
  Doi                      = {10.1198/jasa.2009.0121},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2751448},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1198/jasa.2009.0121}
}

@Article{Lee2014Exact,
  author =         jdlee #{ and } # jtaylor,
  title =          {Exact Post Model Selection Inference for Marginal Screening},
  journal =        {ArXiv e-prints, arXiv:1402.5596},
  year =           {2014},
  month =          feb,
  abstract =       {We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.},
  eprint =         {1402.5596},
  oai2identifier = {1402.5596},
  owner =          {mkolar},
  timestamp =      {2015.10.07}
}

@Article{Lee2012Learning,
  author =   jdlee #{ and } # thastie,
  title =    {Learning the structure of mixed graphical models},
  journal =  {J. Comput. Graph. Statist.},
  year =     {2015},
  volume =   {24},
  number =   {1},
  pages =    {230--253},
  doi =      {10.1080/10618600.2014.900500},
  file =     {Lee2012Learning.pdf:Lee2012Learning.pdf:PDF},
  fjournal = {Journal of Computational and Graphical Statistics},
  issn =     {1061-8600},
  mrclass =  {62-09},
  mrnumber = {3328255},
  url =      {http://dx.doi.org/10.1080/10618600.2014.900500}
}

@Article{Lee2013Exact,
  author =         jdlee #{ and Dennis L. Sun and Yuekai Sun and } # jtaylor,
  title =          {Exact post-selection inference with the lasso},
  journal =        {ArXiv e-prints, arXiv:1311.6238},
  year =           {2013},
  month =          nov,
  abstract =       {We develop a framework for post-selection inference with the lasso. At the core of our framework is a result that characterizes the exact (non-asymptotic) distribution of linear combinations/contrasts of truncated normal random variables. This result allows us to (i) obtain honest confidence intervals for the selected coefficients that account for the selection procedure, and (ii) devise a test statistic that has an exact (non-asymptotic) Unif(0,1) distribution when all relevant variables have been included in the model.},
  eprint =         {1311.6238},
  oai2identifier = {1311.6238}
}

@Article{Lee2015Communication,
  author =  jdlee #{ and Yuekai Sun and Qiang Liu and } # jtaylor,
  title =   {Communication-efficient sparse regression: a one-shot approach},
  journal = {ArXiv e-prints, arXiv:1503.04337},
  year =    {2015},
  file =    {:Lee2015Communication.pdf:PDF}
}

@InProceedings{Duchi08projected,
  author =    jduchi #{ and S. Gould and D. Koller},
  title =     {Projected Subgradient Methods For Learning Sparse Gaussians},
  booktitle = {Proc. of UAI},
  year =      {2008},
  pages =     {145--152},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Article{fan93local,
  Title                    = {Local Linear Regression Smoothers And Their Minimax Efficiencies},
  Author                   = jfan,
  Journal                  = aos_s,
  Year                     = {1993},
  Number                   = {1},
  Pages                    = {196--216},
  Volume                   = {21},

  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Fan2014High,
  author    = {Fan, Jianqing and Liu, Han and Ning, Yang and Zou, Hui},
  title     = {High dimensional semiparametric latent graphical model for mixed data},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2017},
  volume    = {79},
  number    = {2},
  pages     = {405--421},
  issn      = {1369-7412},
  doi       = {10.1111/rssb.12168},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62H12 (60E15 62G20 62H20 62J07)},
  mrnumber  = {3611752},
  timestamp = {2019.05.02},
  url       = {https://doi.org/10.1111/rssb.12168},
}

@Article{Fan2015Large,
  author =         jfan #{ and } # hliu #{ and Weichen Wang},
  title =          {Large Covariance Estimation through Elliptical Factor Models},
  journal =        {ArXiv e-prints, arXiv:1507.08377},
  year =           {2015},
  month =          jul,
  abstract =       {We proposed a general Principal Orthogonal complEment Thresholding (POET) framework for large-scale covariance matrix estimation based on an approximate factor model. A set of high level sufficient conditions for the procedure to achieve optimal rates of convergence under different matrix norms were brought up to better understand how POET works. Such a framework allows us to recover the results for sub-Gaussian in a more transparent way that only depends on the concentration properties of the sample covariance matrix. As a new theoretical contribution, for the first time, such a framework allows us to exploit conditional sparsity covariance structure for the heavy-tailed data. In particular, for the elliptical data, we proposed a robust estimator based on marginal and multivariate Kendall's tau to satisfy these conditions. In addition, conditional graphical model was also studied under the same framework. The technical tools developed in this paper are of general interest to high dimensional principal component analysis. Thorough numerical results were also provided to back up the developed theory.},
  comments =       {48 pages},
  eprint =         {1507.08377},
  file =           {Fan2015Large.pdf:Fan2015Large.pdf:PDF},
  oai2identifier = {1507.08377},
  owner =          {mkolar},
  timestamp =      {2016.03.02}
}

@Article{fan08sis,
  Title                    = {Sure Independence Screening For Ultrahigh Dimensional Feature Space},
  Author                   = jfan # { and } # jlv,
  Journal                  = jrssb_s,
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {849--911},
  Volume                   = {70},

  Doi                      = {10.1111/j.1467-9868.2008.00674.x},
  ISSN                     = {1369-7412},
  Mrnumber                 = {2530322},
  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2008.00674.x}
}

@Article{Fan2010selective,
  Title                    = {A selective overview of variable selection in high dimensional feature space},
  Author                   = jfan # { and } # jlv,
  Journal                  = statsin_s,
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {101},
  Volume                   = {20},

  Owner                    = {mkolar},
  Publisher                = {NIH Public Access},
  Timestamp                = {2013.10.15}
}

@Article{Fan2011Nonconcave,
  Title                    = {Nonconcave penalized likelihood with NP-dimensionality},
  Author                   = jfan # { and } # jlv,
  Journal                  = IEEEit_s,
  Year                     = {2011},
  Number                   = {8},
  Pages                    = {5467--5484},
  Volume                   = {57},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2013.10.15},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5961830}
}

@Article{fan01variable,
  Title                    = {Variable Selection Via Nonconcave Penalized Likelihood And Its Oracle Properties},
  Author                   = jfan # { and } # rli,
  Journal                  = jasa_s,
  Year                     = {2001},
  Number                   = {456},
  Pages                    = {1348--1360},
  Volume                   = {96},

  Doi                      = {10.1198/016214501753382273},
  ISSN                     = {0162-1459},
  Mrnumber                 = {1946581 (2003k:62160)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1198/016214501753382273}
}

@Article{fan2009ultrahigh,
  Title                    = {Ultrahigh Dimensional Feature Selection: Beyond The Linear Model},
  Author                   = jfan # { and } # rsamworth # { and Y. Wu},
  Journal                  = jmlr_s,
  Year                     = {2009},
  Pages                    = {2013--2038},
  Volume                   = {10},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2550099 (2010j:62174)},
  Newspaper                = {J. Mach. Learn. Res.}
}

@Article{fan2008high,
  Title                    = {High-dimensional classification using features annealed independence rules},
  Author                   = jfan # { and Yingying Fan},
  Journal                  = aos,
  Year                     = {2008},
  Number                   = {6},
  Pages                    = {2605--2637},
  Volume                   = {36},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/07-AOS504},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G08 (62H30 62J12)},
  Mrnumber                 = {2485009 (2010f:62109)},
  Url                      = {http://dx.doi.org/10.1214/07-AOS504}
}

@Article{Fan2014Adaptive,
  author =     jfan #{ and Yingying Fan and Emre Barut},
  title =      {Adaptive robust variable selection},
  journal =    aos_s,
  year =       {2014},
  volume =     {42},
  number =     {1},
  pages =      {324--351},
  doi =        {10.1214/13-AOS1191},
  file =       {:Fan2014Adaptive_supp.pdf:PDF;:Fan2014Adaptive.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62J07 (62H12)},
  mrnumber =   {3189488},
  mrreviewer = {Marvin H. J. Gruber},
  url =        {http://dx.doi.org/10.1214/13-AOS1191}
}

@Article{fan2009nonparametric,
  Title                    = {Nonparametric Independence Screening In Sparse Ultra-high-dimensional Additive Models},
  Author                   = jfan # { and Y. Feng and R. Song},
  Journal                  = jasa_s,
  Year                     = {2011},
  Number                   = {494},
  Pages                    = {544--557},
  Volume                   = {106},

  Doi                      = {10.1198/jasa.2011.tm09779},
  ISSN                     = {0162-1459},
  Mrnumber                 = {2847969 (2012i:62117)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Url                      = {http://dx.doi.org/10.1198/jasa.2011.tm09779}
}

@Article{Fan2012road,
  Title                    = {A road to classification in high dimensional space: the regularized optimal affine discriminant},
  Author                   = jfan # { and Yang Feng and Xin Tong},
  Journal                  = jrssb_s,
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {745--771},
  Volume                   = {74},

  Doi                      = {10.1111/j.1467-9868.2012.01029.x},
  Fjournal                 = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  ISSN                     = {1369-7412},
  Mrclass                  = {62H30 (62F15)},
  Mrnumber                 = {2965958},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2012.01029.x}
}

@Article{fan09network,
  author    = jfan #{ and Y. Feng and Y. Wu},
  title     = {Network Exploration Via The Adaptive Lasso And {scad} Penalties},
  journal   = aoas_s,
  year      = {2009},
  volume    = {3},
  number    = {2},
  pages     = {521--541},
  issn      = {1932-6157},
  doi       = {10.1214/08-AOAS215},
  file      = {:fan09network.pdf:PDF},
  mrnumber  = {2750671},
  newspaper = {Ann. Appl. Stat.},
  timestamp = {2018.04.30},
  url       = {http://dx.doi.org/10.1214/08-AOAS215},
}

@TechReport{Fan2014PAGE,
  Title                    = {PAGE: Robust pattern guided estimation of large covariance matrix},
  Author                   = jfan # { and Fang Han and } # hliu,
  Institution              = {Technical report, Princeton University},
  Year                     = {2014}
}

@Article{fan05profile,
  Title                    = {Profile Likelihood Inferences On Semiparametric Varying-coefficient Partially Linear Models},
  Author                   = jfan # { and T. Huang},
  Journal                  = {Bernoulli},
  Year                     = {2005},
  Number                   = {6},
  Pages                    = {1031--1057},
  Volume                   = {11},

  Doi                      = {10.3150/bj/1137421639},
  ISSN                     = {1350-7265},
  Mrnumber                 = {2189080 (2006i:62062)},
  Newspaper                = {Bernoulli},
  Url                      = {http://dx.doi.org/10.3150/bj/1137421639}
}

@Article{Fan2005Nonparametric,
  Title                    = {Nonparametric inferences for additive models},
  Author                   = jfan # { and Jiancheng Jiang},
  Journal                  = jasa_s,
  Year                     = {2005},
  Number                   = {471},
  Pages                    = {890--907},
  Volume                   = {100},

  Coden                    = {JSTNAL},
  Doi                      = {10.1198/016214504000001439},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62G10 (62J02)},
  Mrnumber                 = {2201017 (2007d:62026)},
  Mrreviewer               = {Per Kragh Andersen},
  Url                      = {http://dx.doi.org/10.1198/016214504000001439}
}

@Article{Fan2004Nonconcave,
  Title                    = {Nonconcave penalized likelihood with a diverging number of parameters},
  Author                   = jfan # { and Heng Peng},
  Journal                  = aos_s,
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {928--961},
  Volume                   = {32},

  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2013.10.25},
  Url                      = {http://projecteuclid.org/euclid.aos/1085408491}
}

@Article{Fan2012Strong,
  Title                    = {Strong oracle optimality of folded concave penalized estimation},
  Author                   = jfan # { and Lingzhou Xue and } # hzou,
  Journal                  = aos_s,
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {819--849},
  Volume                   = {42},

  Doi                      = {10.1214/13-AOS1198},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J07},
  Mrnumber                 = {3210988},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1198}
}

@Article{fan98efficient,
  Title                    = {Efficient Estimation Of Conditional Variance Functions In Stochastic Regression},
  Author                   = jfan # { and Q. Yao},
  Journal                  = {Biometrika},
  Year                     = {1998},
  Number                   = {3},
  Pages                    = {645--660},
  Volume                   = {85},

  Doi                      = {10.1093/biomet/85.3.645},
  ISSN                     = {0006-3444},
  Mrnumber                 = {1665822 (2000a:62216)},
  Newspaper                = {Biometrika},
  Url                      = {http://dx.doi.org/10.1093/biomet/85.3.645}
}

@Article{Fan2000Simultaneous,
  author =    jfan #{ and Wenyang Zhang},
  title =     {Simultaneous Confidence Bands and Hypothesis Testing in Varying-coefficient Models},
  journal =   sjs_s,
  year =      {2000},
  volume =    {27},
  number =    {4},
  pages =     {715--731},
  month =     {Dec},
  doi =       {10.1111/1467-9469.00218},
  file =      {Fan2000Simultaneous.pdf:Fan2000Simultaneous.pdf:PDF},
  issn =      {1467-9469},
  owner =     {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2014.08.27},
  url =       {http://dx.doi.org/10.1111/1467-9469.00218}
}

@Article{Fan2008Statistical,
  author    = jfan #{ and Wenyang Zhang},
  title     = {Statistical methods with varying coefficient models},
  journal   = {Statistics and its Interface},
  year      = {2008},
  volume    = {1},
  number    = {1},
  pages     = {179--195},
  file      = {:Fan2008Statistical.pdf:PDF},
  publisher = {NIH Public Access},
}

@Article{friedman08regularization,
  Title                    = {Regularization Paths For Generalized Linear Models Via Coordinate Descent},
  Author                   = jfried # { and } # thastie # { and } # rtibs,
  Journal                  = {Department of Statistics, Stanford University, Tech. Rep},
  Year                     = {2008},

  Newspaper                = {Department of Statistics, Stanford University, Tech. Rep}
}

@Article{Friedman2008Sparse,
  author =    jfried #{ and } # thastie #{ and } # rtibs,
  title =     {Sparse Inverse Covariance Estimation With The Graphical Lasso},
  journal =   {Biostatistics},
  year =      {2008},
  volume =    {9},
  number =    {3},
  pages =     {432-441},
  file =      {Friedman2008Sparse.pdf:Friedman2008Sparse.pdf:PDF},
  newspaper = {Biostatistics}
}

@Article{friedman10note,
  Title                    = {A Note On The Group Lasso And A Sparse Group Lasso},
  Author                   = jfried # { and } # thastie # { and } # rtibs,
  Journal                  = {ArXiv e-prints, arXiv:1001.0736},
  Year                     = {2010},

  Newspaper                = {ArXiv e-prints, arXiv:1001.0736}
}

@Article{friedman07pathwise,
  Title                    = {Pathwise Coordinate Optimization},
  Author                   = jfried # { and } # thastie # { and H. H{\"o}fling and } # rtibs,
  Journal                  = aoas_s,
  Year                     = {2007},
  Number                   = {2},
  Pages                    = {302--332},
  Volume                   = {1},

  Doi                      = {10.1214/07-AOAS131},
  ISSN                     = {1932-6157},
  Mrnumber                 = {2415737},
  Newspaper                = {Ann. Appl. Stat.},
  Url                      = {http://dx.doi.org/10.1214/07-AOAS131}
}

@Article{Friedman1981Projection,
  Title                    = {Projection pursuit regression},
  Author                   = jfried # { and Werner Stuetzle},
  Journal                  = jasa_s,
  Year                     = {1981},
  Number                   = {376},
  Pages                    = {817--823},
  Volume                   = {76},

  Coden                    = {JSTNAL},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62J02 (62G05)},
  Mrnumber                 = {650892 (83j:62097)},
  Mrreviewer               = {Hira L. Koul},
  Url                      = {http://links.jstor.org/sici?sici=0162-1459(198112)76:376<817:PPR>2.0.CO;2-1&origin=MSN}
}

@InProceedings{Haupt2009Compressive,
  Title                    = {Compressive distilled sensing: Sparse recovery using adaptivity in compressive measurements},
  Author                   = jhaupt # { and } # rbaraniuk # { and } # rcastro # { and } # rnowak,
  Booktitle                = {Proc. 43rd Asilomar Conf. Signals, Systems and Computers},
  Year                     = {2009},
  Organization             = {IEEE},
  Pages                    = {1551--1555},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5470138}
}

@InProceedings{Haupt2007Compressive,
  Title                    = {Compressive sampling for signal detection},
  Author                   = jhaupt # { and } # rnowak,
  Booktitle                = {Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {III-1509--III-1512},
  Volume                   = {3},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4218008}
}

@Article{Huang2010Variable,
  author =     jhuang #{ and } # jhorowitz #{ and Fengrong Wei},
  title =      {Variable selection in nonparametric additive models},
  journal =    aos_s,
  year =       {2010},
  volume =     {38},
  number =     {4},
  pages =      {2282--2313},
  coden =      {ASTSC7},
  doi =        {10.1214/09-AOS781},
  file =       {Huang2010Variable.pdf:Huang2010Variable.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G08 (62G20)},
  mrnumber =   {2676890 (2011m:62142)},
  mrreviewer = {Arnab Maity},
  url =        {http://dx.doi.org/10.1214/09-AOS781}
}

@Article{Lafferty2012Sparse,
  author =   jlafferty #{ and } # hliu #{ and } # lwasser,
  title =    {Sparse nonparametric graphical models},
  journal =  statsci_s,
  year =     {2012},
  volume =   {27},
  number =   {4},
  pages =    {519--537},
  doi =      {10.1214/12-STS391},
  file =     {Lafferty2012Sparse.pdf:Lafferty2012Sparse.pdf:PDF},
  fjournal = {Statistical Science. A Review Journal of the Institute of Mathematical Statistics},
  issn =     {0883-4237},
  mrclass =  {62H05 (62G07)},
  mrnumber = {3025132},
  url =      {http://dx.doi.org/10.1214/12-STS391}
}

@Article{Lafferty2008Rodeo,
  Title                    = {Rodeo: sparse, greedy nonparametric regression},
  Author                   = jlafferty # { and } # lwasser,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {28--63},
  Volume                   = {36},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/009053607000000811},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G08 (62G20)},
  Mrnumber                 = {2387963 (2009j:62111)},
  Url                      = {http://dx.doi.org/10.1214/009053607000000811}
}

@Article{Taylor2014Exact,
  Title                    = {Exact post-selection inference for forward stepwise and least angle regression},
  Author                   = jtaylor # { and Richard Lockhart and } # rtibs # { and } # rtibs,
  Journal                  = {ArXiv e-prints, arXiv:1401.3889},
  Year                     = {2014},

  Owner                    = {mkolar},
  Timestamp                = {2015.10.07}
}

@Article{Taylor2014Post,
  Title                    = {Post-selection adaptive inference for Least Angle Regression and the Lasso},
  Author                   = jtaylor # { and Richard Lockhart and } # rtibs # { and } # rtibs,
  Journal                  = {arXiv preprint arXiv:1401.3889},
  Year                     = {2014},

  Month                    = jan,

  Abstract                 = {We propose inference tools for least angle regression and the lasso, from the joint distribution of suitably normalized spacings of the LARS algorithm. From this we extend the results of the asymptotic null distribution of the "covariance test" of Lockhart et al (2013). But we go much further, deriving exact finite sample results for a new asymptotically equivalent procedure called the "spacing test". This provides exact conditional tests at any step of the LAR algorithm as well as "selection intervals" for the appropriate true underlying regression parameter. Remarkably, these tests and intervals account correctly for the adaptive selection done by LARS.},
  Comments                 = {22 pages, 6 figures},
  Eprint                   = {1401.3889},
  Oai2identifier           = {1401.3889}
}

@Article{Taylor2013Tests,
  Title                    = {Tests in adaptive regression via the Kac-Rice formula},
  Author                   = jtaylor # { and Joshua Loftus and } # rtibs,
  Journal                  = {ArXiv e-prints, arXiv:1308.3020},
  Year                     = {2013},

  Month                    = aug,

  Abstract                 = {We derive an exact p-value for testing a global null hypothesis in a general adaptive regression problem. The general approach uses the Kac-Rice formula, as described in (Adler & Taylor 2007). The resulting formula is exact in finite samples, requiring only Gaussianity of the errors. We apply the formula to the lasso, group lasso, and principal components and matrix completion problems. In the case of the lasso, the new test relates closely to the recently proposed covariance test of Lockhart et al. (2013).},
  Eprint                   = {1308.3020},
  Oai2identifier           = {1308.3020}
}

@Article{brownlow96asymptotic,
  Title                    = {Asymptotic Equivalence Of Nonparametric Regression And White Noise},
  Author                   = lbrown # { and M.~G. Low},
  Journal                  = aos_s,
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {2384--2398},
  Volume                   = {24},

  Doi                      = {10.1214/aos/1032181159},
  ISSN                     = {0090-5364},
  Mrnumber                 = {1425958 (98a:62042)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/aos/1032181159}
}

@InProceedings{song09time,
  Title                    = {Time-varying Dynamic Bayesian Networks},
  Author                   = lsong # {and } # mkolar # { and } # epxing,
  Booktitle                = {Proc. of NIPS},
  Year                     = {2009},
  Editor                   = {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  Pages                    = {1732--1740}
}

@Article{le09keller,
  Title                    = {Keller: Estimating Time-varying Interactions Between Genes},
  Author                   = lsong # {and } # mkolar # { and } # epxing,
  Journal                  = {Bioinformatics},
  Year                     = {2009},
  Number                   = {12},
  Pages                    = {i128--i136},
  Volume                   = {25},

  Newspaper                = {Bioinformatics},
  Publisher                = {Oxford Univ Press}
}

@Misc{Wasserman2014Steins,
  author = lwasser,
  title =  {Stein's Method and The Bootstrap in Low and High Dimensions: A Tutorial},
  year =   {2014},
  file =   {:Wasserman2014Steins.pdf:PDF}
}

@Article{Wasserman2014Berry,
  author =   lwasser #{ and } # mkolar #{ and Alessandro Rinaldo},
  title =    {Berry-{E}sseen bounds for estimating undirected graphs},
  journal =  ejs_s,
  year =     {2014},
  volume =   {8},
  pages =    {1188--1224},
  doi =      {10.1214/14-EJS928},
  fjournal = {Electronic Journal of Statistics},
  issn =     {1935-7524},
  mrclass =  {Preliminary Data},
  mrnumber = {3263117},
  url =      {http://dx.doi.org/10.1214/14-EJS928}
}

@Article{wasserman2009high,
  Title                    = {High-dimensional Variable Selection},
  Author                   = lwasser # { and K. Roeder},
  Journal                  = aos_s,
  Year                     = {2009},
  Number                   = {5A},
  Pages                    = {2178--2201},
  Volume                   = {37},

  Doi                      = {10.1214/08-AOS646},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2543689 (2010k:62287)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/08-AOS646}
}

@InProceedings{Balcan2012Distributed,
  author =    mbalcan #{ and Avrim Blum and Shai Fine and Yishay Mansour},
  title =     {Distributed learning, communication complexity and privacy},
  booktitle = {JMLR W\&CP 23: COLT 2012},
  year =      {2012},
  editor =    {Shie Mannor and } # nsrebro #{ and {Robert C.} Williamson},
  volume =    {23},
  pages =     {26.1-26.22},
  file =      {:Balcan2012Distributed.pdf:PDF},
  journal =   {arXiv preprint arXiv:1204.3514}
}

@Article{Davenport2012Compressive,
  author =         mdavenport #{ and } # eacastro,
  title =          {Compressive binary search},
  journal =        {arXiv preprint arXiv:1202.0937},
  year =           {2012},
  month =          feb,
  abstract =       {In this paper we consider the problem of locating a nonzero entry in a high-dimensional vector from possibly adaptive linear measurements. We consider a recursive bisection method which we dub the compressive binary search and show that it improves on what any nonadaptive method can achieve. We also establish a non-asymptotic lower bound that applies to all methods, regardless of their computational complexity. Combined, these results show that the compressive binary search is within a double logarithmic factor of the optimal performance.},
  eprint =         {1202.0937},
  file =           {:Davenport2012Compressive.pdf:PDF},
  oai2identifier = {1202.0937},
  owner =          {mkolar},
  timestamp =      {2014.02.18}
}

@Article{Drton04model,
  Title                    = {Model Selection For {g}aussian Concentration Graphs},
  Author                   = mdrton # { and M.~D. Perlman},
  Journal                  = {Biometrika},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {591--602},
  Volume                   = {91},

  Doi                      = {10.1093/biomet/91.3.591},
  ISSN                     = {0006-3444},
  Mrnumber                 = {2090624},
  Newspaper                = {Biometrika},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1093/biomet/91.3.591}
}

@Article{Drton2007Multiple,
  Title                    = {Multiple testing and error control in {G}aussian graphical
 model selection},
  Author                   = mdrton # { and Perlman, Michael D.},
  Journal                  = statsci_s,
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {430--449},
  Volume                   = {22},

  Doi                      = {10.1214/088342307000000113},
  Fjournal                 = {Statistical Science. A Review Journal of the Institute of
 Mathematical Statistics},
  ISSN                     = {0883-4237},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2416818},
  Url                      = {http://dx.doi.org/10.1214/088342307000000113}
}

@Article{Farrell2013Robust,
  author =    mfarrell,
  title =     {Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations},
  journal =   {Journal of Econometrics},
  year =      {2015},
  volume =    {189},
  number =    {1},
  pages =     {1--23},
  month =     {nov},
  doi =       {10.1016/j.jeconom.2015.06.017},
  file =      {:Farrell2013Robust.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier {BV}},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1016/j.jeconom.2015.06.017}
}

@Article{Jordan2016Communication,
  author        = mjordan #{ and } # jdlee #{ and Yun Yang},
  title         = {Communication-Efficient Distributed Statistical Inference},
  journal       = {ArXiv e-prints},
  year          = {2016},
  month         = may,
  adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160507689J},
  archiveprefix = {arXiv},
  eprint        = {1605.07689},
  file          = {:lee2016communication.pdf:PDF},
  keywords      = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Methodology},
  primaryclass  = {stat.ML},
}

@InProceedings{Kolar2012Consistent,
  Title                    = {Consistent Covariance Selection From Data With Missing Values},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of ICML},
  Year                     = {2012},

  Address                  = {Edinburgh, Scotland, GB},
  Editor                   = {John Langford and Joelle Pineau},
  Month                    = {July},
  Pages                    = {551--558},
  Publisher                = {Omnipress},

  ISBN                     = {978-1-4503-1285-1},
  Monthno                  = {7},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{kolar09sparsistent,
  Title                    = {Sparsistent Estimation Of Time-varying Discrete Markov Random Fields},
  Author                   = mkolar # { and } # epxing,
  Journal                  = {ArXiv e-prints, arXiv:0907.2337},
  Year                     = {2009},

  Month                    = {July},

  Keywords                 = {Statistics - Machine Learning},
  Monthno                  = {7},
  Newspaper                = {ArXiv e-prints, arXiv:0907.2337},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar10ultrahigh,
  Title                    = {Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2010},
  Pages                    = {413--420},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar2011time,
  Title                    = {On Time Varying Undirected Graphs},
  Author                   = mkolar # { and } # epxing,
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2011},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{kolar10estimating,
  Title                    = {Estimating Networks With Jumps},
  Author                   = mkolar # { and } # epxing,
  Journal                  = ejs_s,
  Year                     = {2012},
  Pages                    = {2069--2106},
  Volume                   = {6},

  Newspaper                = {Electron. J. Stat.},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar2012marginal,
  Title                    = {Marginal Regression For Multitask Learning},
  Author                   = mkolar # { and } # hliu,
  Booktitle                = {Proc. of ICML},
  Year                     = {2012},

  Address                  = {Edinburgh, Scotland, GB},
  Editor                   = {John Langford and Joelle Pineau},
  Pages                    = {647--655},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Kolar2013Optimal,
  Title                    = {Optimal Feature Selection in High-Dimensional Discriminant Analysis},
  Author                   = mkolar # { and } # hliu,
  Journal                  = IEEEit_s,
  Year                     = {2014},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {1063--1083},
  Volume                   = {61}
}

@InProceedings{kolar13multiatticml,
  Title                    = {Markov Network Estimation From Multi-attribute Data},
  Author                   = mkolar # { and } # hliu # { and } # epxing,
  Booktitle                = {Proc. of ICML},
  Year                     = {2013},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Kolar2014Graph,
  Title                    = {Graph Estimation From Multi-attribute Data},
  Author                   = mkolar # { and } # hliu # { and } # epxing,
  Journal                  = jmlr_s,
  Year                     = {2014},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1713-1750},
  Volume                   = {15}
}

@Article{kolar11union,
  author =    mkolar #{ and } # jlafferty #{and } # lwasser,
  title =     {Union Support Recovery In Multi-task Learning},
  journal =   jmlr_s,
  year =      {2011},
  volume =    {12},
  pages =     {2415--2435},
  file =      {:kolar11union.pdf:PDF},
  issn =      {1532-4435},
  mrnumber =  {2825433 (2012f:62144)},
  newspaper = {J. Mach. Learn. Res.},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Article{Kolar2010Estimating,
  Title                    = {Estimating {Time-varying} Networks},
  Author                   = mkolar # { and } # lsong # {and } # aahmed # { and } # epxing,
  Journal                  = aoas_s,
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {94--123},
  Volume                   = {4},

  Newspaper                = {Ann. Appl. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar09nips_tv_paper,
  Title                    = {Sparsistent Learning Of Varying-coefficient Models With Structural Changes},
  Author                   = mkolar # { and } # lsong # {and } # epxing,
  Booktitle                = {Proc. of NIPS},
  Year                     = {2009},
  Editor                   = {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  Pages                    = {1006--1014},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Kolar2011Minimax,
  Title                    = {Minimax Localization of Structural Information in Large Noisy Matrices},
  Author                   = mkolar # { and Sivaraman Balakrishnan and Alessandro Rinaldo and Aarti Singh},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Year                     = {2011},
  Editor                   = {John Shawe-Taylor and Richard S. Zemel and Peter L. Bartlett and Fernando C. N. Pereira and Kilian Q. Weinberger},
  Pages                    = {909--917},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{kolar10nonparametric,
  Title                    = {On Sparse Nonparametric Conditional Covariance Selection},
  Author                   = mkolar # { and A.~P. Parikh and } # epxing,
  Booktitle                = PROC_s # { } # ICML2010_s,
  Year                     = {2010},

  Address                  = {Haifa, Israel},
  Editor                   = {Johannes F{\"u}rnkranz and Thorsten Joachims},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Kolar2012Variance,
  Title                    = {Variance Function Estimation in High-dimensions},
  Author                   = mkolar # { and James Sharpnack},
  Booktitle                = PROC_s # { } # ICML2012_s,
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Editor                   = {John Langford and Joelle Pineau},
  Month                    = {July},
  Pages                    = {1447--1454},
  Publisher                = {Omnipress},
  Series                   = {ICML '12},

  ISBN                     = {978-1-4503-1285-1},
  Location                 = {Edinburgh, Scotland, GB},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Malloy2012Neara,
  Title                    = {Near-Optimal Compressive Binary Search},
  Author                   = mmalloy # { and } # rnowak,
  Journal                  = {arXiv preprint arXiv:1203.1804},
  Year                     = {2012},

  Month                    = mar,

  Abstract                 = {We propose a simple modification to the recently proposed compressive binary search. The modification removes an unnecessary and suboptimal factor of log log n from the SNR requirement, making the procedure optimal (up to a small constant). Simulations show that the new procedure performs significantly better in practice as well. We also contrast this problem with the more well known problem of noisy binary search.},
  Eprint                   = {1203.1804},
  Oai2identifier           = {1203.1804},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{wainwright06sharp,
  author =    mwainw,
  title =     {Sharp Thresholds For High-dimensional And Noisy Sparsity Recovery Using {$\ell\sb 1$}-constrained Quadratic Programming ({l}asso)},
  journal =   IEEEit_s,
  year =      {2009},
  volume =    {55},
  number =    {5},
  pages =     {2183--2202},
  doi =       {10.1109/TIT.2009.2016018},
  file =      {wainwright06sharp.pdf:wainwright06sharp.pdf:PDF},
  issn =      {0018-9448},
  mrnumber =  {2729873 (2011f:62084)},
  newspaper = {IEEE Trans. Inf. Theory},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.1109/TIT.2009.2016018}
}

@Article{Wainwright2009Information,
  author =    mwainw,
  title =     {Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting},
  journal =   IEEEit_s,
  year =      {2009},
  volume =    {55},
  number =    {12},
  pages =     {5728--5741},
  coden =     {IETTAW},
  doi =       {10.1109/TIT.2009.2032816},
  file =      {Wainwright2009Information.pdf:Wainwright2009Information.pdf:PDF},
  fjournal =  {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  issn =      {0018-9448},
  mrclass =   {94A12 (94A15)},
  mrnumber =  {2597190 (2010i:94069)},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.1109/TIT.2009.2032816}
}

@Article{wainwright08graphical,
  Title                    = {Graphical Models, Exponential Families, And Variational Inference},
  Author                   = mwainw # { and } # mjordan,
  Journal                  = {Found. and Trends Mach. Learn.},
  Year                     = {2008},
  Number                   = {1-2},
  Pages                    = {1--305},
  Volume                   = {1},

  Address                  = {Hanover, MA, USA},
  Newspaper                = {Found. and Trends Mach. Learn.},
  Owner                    = {mkolar},
  Publisher                = {Now Publishers Inc.},
  Timestamp                = {2014.02.18}
}

@InProceedings{Wakin2006architecture,
  Title                    = {An architecture for compressive imaging},
  Author                   = mwakin # { and Jason N. Laska and Marco F. Duarte and Dror Baron and Shriram Sarvotham and Dharmpal Takhar and Kevin F. Kelly and } # rbaraniuk,
  Booktitle                = {Proc. IEEE Int. Conf. Image Processing,},
  Year                     = {2006},
  Organization             = {IEEE},
  Pages                    = {1273--1276},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4106769}
}

@Article{meinshausen08note,
  Title                    = {A Note On The Lasso For Graphical Gaussian Model Selection},
  Author                   = nmeins,
  Journal                  = {Statist. Probab. Lett.},
  Year                     = {2008},
  Number                   = {7},
  Pages                    = {880--884},
  Volume                   = {78},

  Newspaper                = {Statist. Probab. Lett.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Meinshausen2013Assumption,
  author     = {Meinshausen, Nicolai},
  title      = {Group bound: confidence intervals for groups of variables in sparse high dimensional regression without assumptions on the design},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2015},
  volume     = {77},
  number     = {5},
  pages      = {923--945},
  issn       = {1369-7412},
  doi        = {10.1111/rssb.12094},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62J05 (62F03 62F25 62J07)},
  mrnumber   = {3414134},
  mrreviewer = {B. M. Golam Kibria},
  url        = {https://doi.org/10.1111/rssb.12094},
}

@Article{Meinshausen2013Sign,
  author =     nmeins,
  title =      {Sign-constrained least squares estimation for high-dimensional regression},
  journal =    ejs_s,
  year =       {2013},
  volume =     {7},
  pages =      {1607--1631},
  doi =        {10.1214/13-EJS818},
  file =       {Meinshausen2013Sign.pdf:Meinshausen2013Sign.pdf:PDF},
  fjournal =   {Electronic Journal of Statistics},
  issn =       {1935-7524},
  mrclass =    {62J07 (62F30 62J05)},
  mrnumber =   {3066380},
  mrreviewer = {Shalabh},
  url =        {http://dx.doi.org/10.1214/13-EJS818}
}

@Article{Meinshausen2006High,
  Title                    = {High Dimensional Graphs And Variable Selection With The Lasso},
  Author                   = nmeins # { and } # pbuhl,
  Journal                  = aos_s,
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {1436--1462},
  Volume                   = {34},

  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Meinshausen2010Stability,
  Title                    = {Stability Selection},
  Author                   = nmeins # { and } # pbuhl,
  Journal                  = jrssb_s,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {417--473},
  Volume                   = {72},

  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Meinshausen2009Pvalue,
  Title                    = {P-values for high-dimensional regression},
  Author                   = nmeins # { and Lukas Meier and } # pbuhl,
  Journal                  = jasa_s,
  Year                     = {2009},
  Number                   = {488},
  Volume                   = {104},

  Owner                    = {mkolar},
  Timestamp                = {2013.10.25},
  Url                      = {http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08647}
}

@Article{meinshausen08lasso,
  Title                    = {Lasso-type Recovery Of Sparse Representations For High-dimensional Data},
  Author                   = nmeins # { and B. Yu},
  Journal                  = aos_s,
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {246--270},
  Volume                   = {37},

  Doi                      = {10.1214/07-AOS582},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2488351 (2010e:62176)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/07-AOS582}
}

@InCollection{Srebro2005Rank,
  Title                    = {Rank, trace-norm and max-norm},
  Author                   = nsrebro # { and Adi Shraibman},
  Booktitle                = {Learning theory},
  Publisher                = {Springer, Berlin},
  Year                     = {2005},
  Pages                    = {545--560},
  Series                   = {Lecture Notes in Comput. Sci.},
  Volume                   = {3559},

  Doi                      = {10.1007/11503415_37},
  Mrclass                  = {15A03 (15A60 62H25 68Q32 68T05)},
  Mrnumber                 = {2203286 (2006i:15002)},
  Url                      = {http://dx.doi.org/10.1007/11503415_37}
}

@Article{Bartlett2003Rademacher,
  author =    pbartlett #{ and Shahar Mendelson},
  title =     {Rademacher and Gaussian complexities: Risk bounds and structural results},
  journal =   jmlr_s,
  year =      {2002},
  volume =    {3},
  pages =     {463--482},
  file =      {:Bartlett2003Rademacher.pdf:PDF},
  publisher = {JMLR. org}
}

@Article{Bickel1973somea,
  Title                    = {On some analogues to linear combinations of order statistics
 in the linear model},
  Author                   = pbickel,
  Journal                  = aos_s,
  Year                     = {1973},
  Pages                    = {597--616},
  Volume                   = {1},

  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J05},
  Mrnumber                 = {0314206 (47 \#2758)},
  Mrreviewer               = {Lionel Weiss}
}

@Article{bickel04some,
  Title                    = {Some Theory Of {f}isher's Linear Discriminant Function, `naive {b}ayes', And Some Alternatives When There Are Many More Variables Than Observations},
  Author                   = pbickel # { and } # elevina,
  Journal                  = {Bernoulli},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {989--1010},
  Volume                   = {10},

  Doi                      = {10.3150/bj/1106314847},
  ISSN                     = {1350-7265},
  Mrnumber                 = {2108040 (2006a:62081)},
  Newspaper                = {Bernoulli},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.3150/bj/1106314847}
}

@Article{bickel08covariance,
  Title                    = {Covariance Regularization By Thresholding},
  Author                   = pbickel # { and } # elevina,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {6},
  Pages                    = {2577--2604},
  Volume                   = {36},

  Doi                      = {10.1214/08-AOS600},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2485008 (2010b:62197)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/08-AOS600}
}

@Article{bickel08regularized,
  Title                    = {Regularized Estimation Of Large Covariance Matrices},
  Author                   = pbickel # { and } # elevina,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {199--227},
  Volume                   = {36},

  Doi                      = {10.1214/009053607000000758},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2387969 (2009a:62255)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053607000000758}
}

@Article{Bickel2009Simultaneous,
  Title                    = {Simultaneous analysis of lasso and {D}antzig selector},
  Author                   = pbickel # { and Ya'acov Ritov and } # atsybakov,
  Journal                  = aos_s,
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {1705--1732},
  Volume                   = {37},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/08-AOS620},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G08 (62C20 62G20)},
  Mrnumber                 = {2533469 (2010j:62118)},
  Mrreviewer               = {Ursula U. M{\"u}ller},
  Url                      = {http://dx.doi.org/10.1214/08-AOS620}
}

@Article{Bickel1973some,
  Title                    = {On some global measures of the deviations of density function
 estimates},
  Author                   = pbickel # { and M. Rosenblatt},
  Journal                  = aos_s,
  Year                     = {1973},
  Pages                    = {1071--1095},
  Volume                   = {1},

  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62G15)},
  Mrnumber                 = {0348906 (50 \#1400)},
  Mrreviewer               = {I. R. Savage}
}

@Article{Buehlmann2012Statistical,
  Title                    = {Statistical significance in high-dimensional linear models},
  Author                   = pbuhl,
  Journal                  = {Bernoulli},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {1212-1242},
  Volume                   = {19},

  Owner                    = {mkolar},
  Timestamp                = {2013.10.25},
  Url                      = {http://arxiv.org/abs/1202.1377}
}

@Book{Buehlmann2011book,
  Title                    = {Statistics for high-dimensional data},
  Author                   = pbuhl # { and } # svdgeer,
  Publisher                = {Springer},
  Year                     = {2011},

  Address                  = {Heidelberg},
  Note                     = {Methods, theory and applications},
  Series                   = {Springer Series in Statistics},

  Doi                      = {10.1007/978-3-642-20192-9},
  ISBN                     = {978-3-642-20191-2},
  Mrclass                  = {62-07 (60E15 62G05 62G20 62G30 62H30 62J07)},
  Mrnumber                 = {2807761 (2012e:62006)},
  Mrreviewer               = {Pierre Alquier},
  Owner                    = {mkolar},
  Pages                    = {xviii+556},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-20192-9}
}

@Article{Buehlmann2014High,
  Title                    = {High-Dimensional Statistics with a View Toward Applications in Biology},
  Author                   = pbuhl # { and Markus Kalisch and Lukas Meier},
  Journal                  = {Ann. Rev. Stat. \& Appl.},
  Year                     = {2014},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {255--278},
  Volume                   = {1},

  Doi                      = {10.1146/annurev-statistics-022513-115545},
  ISSN                     = {2326-831X},
  Owner                    = {mkolar},
  Publisher                = {Annual Reviews},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1146/annurev-statistics-022513-115545}
}

@Article{Hall1991convergence,
  Title                    = {On convergence rates of suprema},
  Author                   = phall,
  Journal                  = {Probab. Theory Related Fields},
  Year                     = {1991},
  Number                   = {4},
  Pages                    = {447--455},
  Volume                   = {89},

  Coden                    = {PTRFEU},
  Doi                      = {10.1007/BF01199788},
  Fjournal                 = {Probability Theory and Related Fields},
  ISSN                     = {0178-8051},
  Mrclass                  = {60G70},
  Mrnumber                 = {1118558 (92i:60101)},
  Mrreviewer               = {Georg Lindgren},
  Url                      = {http://dx.doi.org/10.1007/BF01199788}
}

@Book{Hall1992bootstrap,
  Title                    = {The bootstrap and {E}dgeworth expansion},
  Author                   = phall,
  Publisher                = {Springer-Verlag, New York},
  Year                     = {1992},
  Series                   = {Springer Series in Statistics},

  Doi                      = {10.1007/978-1-4612-4384-7},
  ISBN                     = {0-387-97720-1},
  Mrclass                  = {62E20 (62E25 62G05 62G09)},
  Mrnumber                 = {1145237 (93h:62029)},
  Mrreviewer               = {Gutti J. Babu},
  Pages                    = {xiv+352},
  Url                      = {http://dx.doi.org/10.1007/978-1-4612-4384-7}
}

@Article{hoff2012covariance,
  Title                    = {A Covariance Regression Model},
  Author                   = phoff # { and Xiaoyue Niu},
  Journal                  = statsin_s,
  Year                     = {2012},
  Pages                    = {729--753},
  Volume                   = {22},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{loh11high,
  Title                    = {High-dimensional Regression With Noisy And Missing Data: Provable Guarantees With Nonconvexity},
  Author                   = ploh # { and } # mwainw,
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {1637--1664},
  Volume                   = {40},

  Doi                      = {10.1214/12-AOS1018},
  ISSN                     = {0090-5364},
  Mrnumber                 = {3015038},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/12-AOS1018}
}

@Article{Loh2013Structure,
  Title                    = {Structure estimation for discrete graphical models:
 generalized covariance matrices and their inverses},
  Author                   = ploh # { and } # mwainw,
  Journal                  = aos_s,
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {3022--3049},
  Volume                   = {41},

  Doi                      = {10.1214/13-AOS1162},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62H12 (62H99)},
  Mrnumber                 = {3161456},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1162}
}

@Article{Loh2013Regularized,
  author =  ploh #{ and } # mwainw,
  title =   {Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima},
  journal = jmlr_s,
  year =    {2015},
  volume =  {16},
  pages =   {559-616},
  file =    {:Loh2013Regularized.pdf:PDF},
  url =     {http://jmlr.org/papers/v16/loh15a.html}
}

@Article{Ravikumar2009Sparse,
  Title                    = {Sparse additive models},
  Author                   = pravik # { and } # jlafferty # { and } # hliu # { and } # lwasser,
  Journal                  = JRSSB_s,
  Year                     = {2009},

  Month                    = {Nov},
  Number                   = {5},
  Pages                    = {1009--1030},
  Volume                   = {71},

  Doi                      = {10.1111/j.1467-9868.2009.00718.x},
  ISSN                     = {1467-9868},
  Publisher                = {Wiley-Blackwell},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2009.00718.x}
}

@Article{ravikumar09high,
  author    = pravik #{ and } # mwainw #{ and J.~D. Lafferty},
  title     = {High-dimensional {i}sing Model Selection Using {$\ell\sb 1$}-regularized Logistic Regression},
  journal   = aos_s,
  year      = {2010},
  volume    = {38},
  number    = {3},
  pages     = {1287--1319},
  issn      = {0090-5364},
  doi       = {10.1214/09-AOS691},
  file      = {:ravikumar09high.pdf:PDF},
  mrnumber  = {2662343 (2011d:62066)},
  newspaper = {Ann. Stat.},
  timestamp = {2019.04.23},
  url       = {http://dx.doi.org/10.1214/09-AOS691},
}

@Article{Baraniuk2008Simple,
  Title                    = {A Simple Proof of the Restricted Isometry Property for Random Matrices},
  Author                   = rbaraniuk # { and } # mdavenport # { and } # rdevore # { and } # mwakin,
  Journal                  = {Constructive Approximation},
  Year                     = {2008},

  Month                    = {Jan},
  Number                   = {3},
  Pages                    = {253--263},
  Volume                   = {28},

  Doi                      = {10.1007/s00365-007-9003-x},
  ISSN                     = {1432-0940},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s00365-007-9003-x}
}

@Article{Baraniuk2010Model,
  Title                    = {Model-based compressive sensing},
  Author                   = rbaraniuk # { and Volkan Cevher and Marco F. Duarte and Chinmay Hegde},
  Journal                  = IEEEit_s,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1982--2001},
  Volume                   = {56},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5437428}
}

@Article{Castro2012Adaptive,
  Title                    = {Adaptive Sensing Performance Lower Bounds for Sparse Signal Detection and Support Estimation},
  Author                   = rcastro,
  Journal                  = {arXiv preprint arXiv:1206.0648},
  Year                     = {2012},

  Month                    = jun,

  Abstract                 = {This paper gives a precise characterization of the fundamental limits of adaptive sensing for diverse estimation and testing problems concerning sparse signals. We consider in particular the setting introduced in Haupt, Castro and Nowak (2011) and show necessary conditions on the minimum signal magnitude for both detection and estimation: if $x\in\R^n$ is a sparse vector with $s$ non-zero components then it can be reliably detected in noise provided the magnitude of the non-zero components exceeds $\sqrt{2/s}$. Furthermore, the signal support can be exactly identified provided the minimum magnitude exceeds $\sqrt{2\log s}$. Notably there is no dependence on $n$, the extrinsic signal dimension. These results show that the adaptive sensing methodologies proposed previously in the literature are essentially optimal, and cannot be substantially improved. In addition these results provide further insights on the limits of adaptive compressive sensing.},
  Comments                 = {submitted to the Bernoulli journal},
  Eprint                   = {1206.0648},
  Oai2identifier           = {1206.0648},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Foygel2010Extended,
  author =    rfoygel #{ and } # mdrton,
  title =     {Extended bayesian information criteria for gaussian graphical models},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2010},
  pages =     {604--612}
}

@Article{tibshirani96regression,
  Title                    = {Regression Shrinkage And Selection Via The Lasso},
  Author                   = rtibs,
  Journal                  = jrssb_s,
  Year                     = {1996},
  Number                   = {1},
  Pages                    = {267--288},
  Volume                   = {58},

  ISSN                     = {0035-9246},
  Mrnumber                 = {1379242 (96j:62134)},
  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://links.jstor.org/sici?sici=0035-9246(1996)58:1<267:RSASVT>2.0.CO;2-G&origin=MSN}
}

@Article{Tibshirani2003Class,
  Title                    = {Class prediction by nearest shrunken centroids, with applications to {DNA} microarrays},
  Author                   = rtibs # { and } # thastie # { and Balasubramanian Narasimhan and Gilbert Chu},
  Journal                  = statsci_s,
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {104--117},
  Volume                   = {18},

  Doi                      = {10.1214/ss/1056397488},
  Fjournal                 = {Statistical Science. A Review Journal of the Institute of Mathematical Statistics},
  ISSN                     = {0883-4237},
  Mrclass                  = {62M20 (62P10)},
  Mrnumber                 = {1997067},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/ss/1056397488}
}

@Article{tibshirani2010strong,
  Title                    = {Strong Rules For Discarding Predictors In Lasso-type Problems},
  Author                   = rtibs # { and J. Bien and } # jfried # { and } # thastie # { and N. Simon and } # jtaylor # { and } # rtibs,
  Journal                  = jrssb_s,
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {245--266},
  Volume                   = {74},

  Doi                      = {10.1111/j.1467-9868.2011.01004.x},
  ISSN                     = {1369-7412},
  Mrnumber                 = {2899862},
  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2011.01004.x}
}

@Article{tibshirani05sparsity,
  Title                    = {Sparsity And Smoothness Via The Fused Lasso},
  Author                   = rtibs # { and M. Saunders and S. Rosset and J. Zhu and K. Knight},
  Journal                  = jrssb_s,
  Year                     = {2005},
  Number                   = {1},
  Pages                    = {91--108},
  Volume                   = {67},

  Doi                      = {10.1111/j.1467-9868.2005.00490.x},
  ISSN                     = {1369-7412},
  Mrnumber                 = {2136641},
  Newspaper                = {J. R. Stat. Soc. B},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2005.00490.x}
}

@Article{Boyd2011Distributed,
  author =     sboyd #{ and Neal Parikh and Eric Chu and Borja Peleato and Jonathan Eckstein},
  title =      {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
  journal =    {Found. Trends Mach. Learn.},
  year =       {2011},
  volume =     {3},
  number =     {1},
  pages =      {1--122},
  month =      jan,
  acmid =      {2185816},
  address =    {Hanover, MA, USA},
  doi =        {10.1561/2200000016},
  file =       {:Boyd2011Distributed.pdf:PDF},
  issn =       {1935-8237},
  issue_date = {January 2011},
  numpages =   {122},
  owner =      {mkolar},
  publisher =  {Now Publishers Inc.},
  timestamp =  {2014.02.18},
  url =        {http://dx.doi.org/10.1561/2200000016}
}

@Book{Lauritzen1996Graphical,
  Title                    = {Graphical Models},
  Author                   = slauritzen,
  Publisher                = {The Clarendon Press Oxford University Press},
  Year                     = {1996},

  Address                  = {New York},
  Note                     = {Oxford Science Publications},
  Series                   = {Oxford Statistical Science Series},
  Volume                   = {17},

  ISBN                     = {0-19-852219-3},
  Mrnumber                 = {1419991 (98g:62001)},
  Owner                    = {mkolar},
  Pages                    = {x+298},
  Timestamp                = {2014.02.18}
}

@Article{Lauritzen1989Graphical,
  Title                    = {Graphical Models for Associations between Variables, some of which are Qualitative and some Quantitative},
  Author                   = slauritzen # { and N. Wermuth},
  Journal                  = aos_s,
  Year                     = {1989},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {31--57},
  Volume                   = {17},

  Doi                      = {10.1214/aos/1176347003},
  ISSN                     = {0090-5364},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1214/aos/1176347003}
}

@Article{Negahban2011Estimation,
  Title                    = {Estimation of (near) low-rank matrices with noise and high-dimensional scaling},
  Author                   = snegahban # { and } # mwainw,
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {1069--1097},
  Volume                   = {39},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.aos/1304947044}
}

@Article{Portnoy1988Asymptotic,
  author =     sportnoy,
  title =      {Asymptotic behavior of likelihood methods for exponential families when the number of parameters tends to infinity},
  journal =    aos_s,
  year =       {1988},
  volume =     {16},
  number =     {1},
  pages =      {356--366},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1176350710},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62F05 (62F12)},
  mrnumber =   {924876},
  mrreviewer = {Zhi-Dong Bai},
  url =        {http://dx.doi.org/10.1214/aos/1176350710}
}

@Article{Portnoy2012Nearly,
  author =     sportnoy,
  title =      {Nearly root-{$n$} approximation for regression quantile processes},
  journal =    aos_s,
  year =       {2012},
  volume =     {40},
  number =     {3},
  pages =      {1714--1736},
  doi =        {10.1214/12-AOS1021},
  file =       {Portnoy2012Nearly.pdf:Portnoy2012Nearly.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G20 (60F17 62G30 62J99)},
  mrnumber =   {3015041},
  mrreviewer = {Ou Zhao},
  url =        {http://dx.doi.org/10.1214/12-AOS1021}
}

@Book{Geer2000Applications,
  Title                    = {Applications of empirical process theory},
  Author                   = svdgeer,
  Publisher                = {Cambridge University Press, Cambridge},
  Year                     = {2000},
  Series                   = {Cambridge Series in Statistical and Probabilistic Mathematics},
  Volume                   = {6},

  ISBN                     = {0-521-65002-X},
  Mrclass                  = {62-01 (62G05 62G20)},
  Mrnumber                 = {1739079 (2001h:62002)},
  Mrreviewer               = {Miguel A. Arcones},
  Pages                    = {xii+286}
}

@Article{Geer2008High,
  Title                    = {High-dimensional generalized linear models and the lasso},
  Author                   = svdgeer,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {614--645},
  Volume                   = {36},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/009053607000000929},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G08},
  Mrnumber                 = {2396809 (2009h:62048)},
  Mrreviewer               = {Songqiao Wen},
  Url                      = {http://dx.doi.org/10.1214/009053607000000929}
}

@Article{Geer2015concentration,
  author =         svdgeer #{ and } # mwainw,
  title =          {On concentration for (regularized) empirical risk minimization},
  journal =        {ArXiv e-prints, arXiv:1512.00677},
  year =           {2015},
  month =          dec,
  abstract =       {Rates of convergence for empirical risk minimizers have been well studied in the literature. In this paper, we aim to provide a complementary set of results, in particular by showing that after normalization, the risk of the empirical minimizer concentrates on a single point. Such results have been established by~\cite{chatterjee2014new} for constrained estimators in the normal sequence model. We first generalize and sharpen this result to regularized least squares with convex penalties, making use of a "direct" argument based on Borell's theorem. We then study generalizations to other loss functions, including the negative log-likelihood for exponential families combined with a strictly convex regularization penalty. The results in this general setting are based on more "indirect" arguments as well as on concentration inequalities for maxima of empirical processes.},
  comments =       {27 pages},
  eprint =         {1512.00677},
  file =           {:Geer2015concentration.pdf:PDF},
  oai2identifier = {1512.00677},
  owner =          {mkolar},
  timestamp =      {2016.04.21}
}

@Article{geer09conditions,
  Title                    = {On The Conditions Used To Prove Oracle Results For The {l}asso},
  Author                   = svdgeer # { and } # pbuhl,
  Journal                  = ejs_s,
  Year                     = {2009},
  Pages                    = {1360--1392},
  Volume                   = {3},

  Doi                      = {10.1214/09-EJS506},
  ISSN                     = {1935-7524},
  Mrnumber                 = {2576316 (2011c:62231)},
  Newspaper                = {Electron. J. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/09-EJS506}
}

@Article{Geer2013penalized,
  author =     svdgeer #{ and } # pbuhl,
  title =      {{$\ell\sb 0$}-penalized maximum likelihood for sparse directed acyclic graphs},
  journal =    aos_s,
  year =       {2013},
  volume =     {41},
  number =     {2},
  pages =      {536--567},
  doi =        {10.1214/13-AOS1085},
  file =       {:Geer2013penalized.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62F10 (62F30 62H99)},
  mrnumber =   {3099113},
  mrreviewer = {Kuang-Yao Lee},
  url =        {http://dx.doi.org/10.1214/13-AOS1085}
}

@Article{Geer2013asymptotically,
  author =    svdgeer #{ and } # pbuhl #{ and Ya'acov Ritov and Ruben Dezeure},
  title =     {On asymptotically optimal confidence regions and tests for high-dimensional models},
  journal =   aos_s,
  year =      {2014},
  volume =    {42},
  number =    {3},
  pages =     {1166--1202},
  month =     {Jun},
  doi =       {10.1214/14-aos1221},
  file =      {Geer2013asymptotically.pdf:Geer2013asymptotically.pdf:PDF},
  issn =      {0090-5364},
  owner =     {mkolar},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.11.25},
  url =       {http://dx.doi.org/10.1214/14-AOS1221}
}

@Article{Cai2010Optimal,
  Title                    = {Optimal rates of convergence for covariance matrix estimation},
  Author                   = tcai # { and } # chzhang # { and } # hzhou,
  Journal                  = aos_s,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {2118--2144},
  Volume                   = {38},

  Publisher                = {Institute of Mathematical Statistics},
  Url                      = {http://projecteuclid.org/euclid.aos/1278861244}
}

@Article{Cai2012Minimax,
  Title                    = {Minimax estimation of large covariance matrices under L1-norm},
  Author                   = tcai # { and } # hzhou,
  Journal                  = statsin_s,
  Year                     = {2012},
  Pages                    = {1319--1378},
  Volume                   = {22},

  Doi                      = {10.5705/ss.2010.253},
  Publisher                = {Institute of Statistical Science, Academia Sinica},
  Url                      = {http://dx.doi.org/10.5705/ss.2010.253}
}

@Article{Cai2012Optimal,
  Title                    = {Optimal rates of convergence for sparse covariance matrix estimation},
  Author                   = tcai # { and } # hzhou,
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {2389--2420},
  Volume                   = {40},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.13},
  Url                      = {http://projecteuclid.org/euclid.aos/1359987525}
}

@Article{Cai2012Adaptive,
  Title                    = {Adaptive covariance matrix estimation through block thresholding},
  Author                   = tcai # { and } # myuan,
  Journal                  = aos_s,
  Year                     = {2012},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {2014-2042},
  Volume                   = {40},

  Doi                      = {10.1214/12-AOS999},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Url                      = {http://dx.doi.org/10.1214/12-AOS999}
}

@Article{Cai2011Adaptive,
  Title                    = {Adaptive Thresholding for Sparse Covariance Matrix Estimation},
  Author                   = tcai # { and } # wliu,
  Journal                  = jasa_s,
  Year                     = {2011},

  Month                    = {Jun},
  Number                   = {494},
  Pages                    = {672-684},
  Volume                   = {106},

  Doi                      = {10.1198/jasa.2011.tm10560},
  Owner                    = {mkolar},
  Publisher                = {Informa UK Limited},
  Timestamp                = {2014.02.13},
  Url                      = {http://dx.doi.org/10.1198/jasa.2011.tm10560}
}

@Article{Cai2012Estimating,
  author   = {Cai, T. Tony and Liu, Weidong and Zhou, Harrison H.},
  title    = {Estimating sparse precision matrix: optimal rates of convergence and adaptive estimation},
  journal  = {Ann. Statist.},
  year     = {2016},
  volume   = {44},
  number   = {2},
  pages    = {455--488},
  issn     = {0090-5364},
  doi      = {10.1214/13-AOS1171},
  file     = {:Cai2012Estimating.pdf:PDF},
  fjournal = {The Annals of Statistics},
  mrclass  = {62H12 (62F12 62H99)},
  mrnumber = {3476606},
  url      = {https://doi.org/10.1214/13-AOS1171},
}

@Article{Cai2013Two,
  author =    tcai #{ and } # wliu #{ and Yin Xia},
  title =     {Two-Sample Covariance Matrix Testing and Support Recovery in High-Dimensional and Sparse Settings},
  journal =   jasa_s,
  year =      {2013},
  volume =    {108},
  number =    {501},
  pages =     {265-277},
  month =     {Mar},
  doi =       {10.1080/01621459.2012.758041},
  file =      {:Cai2013Two.pdf:PDF},
  publisher = {Informa UK Limited},
  url =       {http://dx.doi.org/10.1080/01621459.2012.758041}
}

@Article{Cai2015Confidence,
  author =         tcai #{ and Zijian Guo},
  title =          {Confidence Intervals for High-Dimensional Linear Regression: Minimax Rates and Adaptivity},
  journal =        {ArXiv e-prints, arXiv:1506.05539},
  year =           {2015},
  month =          jun,
  abstract =       {Confidence sets play a fundamental role in statistical inference. In this paper, we consider confidence intervals for high dimensional linear regression with random design. We first establish the convergence rates of the minimax expected length for confidence intervals in the oracle setting where the sparsity parameter is given. The focus is then on the problem of adaptation to sparsity for the construction of confidence intervals. Ideally, an adaptive confidence interval should have its length automatically adjusted to the sparsity of the unknown regression vector, while maintaining a prespecified coverage probability. It is shown that such a goal is in general not attainable, except when the sparsity parameter is restricted to a small region over which the confidence intervals have the optimal length of the usual parametric rate. It is further demonstrated that the lack of adaptivity is not due to the conservativeness of the minimax framework, but is fundamentally caused by the difficulty of learning the bias accurately.},
  comments =       {31 pages, 1 figure},
  eprint =         {1506.05539},
  file =           {:Cai2015Confidence.pdf:PDF},
  oai2identifier = {1506.05539},
  owner =          {mkolar},
  timestamp =      {2016.03.02}
}

@Article{cai2007estimation,
  Title                    = {Estimation And Confidence Sets For Sparse Normal Mixtures},
  Author                   = tcai # { and J. Jin and M.~G. Low},
  Journal                  = aos_s,
  Year                     = {2007},
  Number                   = {6},
  Pages                    = {2421--2449},
  Volume                   = {35},

  Doi                      = {10.1214/009053607000000334},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2382653 (2009c:62058)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053607000000334}
}

@Article{Cai2013Covariate,
  author =    tcai #{ and Hongzhe Li and } # wliu #{ and Jichun Xie},
  title =     {Covariate-adjusted precision matrix estimation with an application in genetical genomics},
  journal =   {Biometrika},
  year =      {2013},
  volume =    {100},
  number =    {1},
  pages =     {139-156},
  month =     {Feb},
  doi =       {10.1093/biomet/ass058},
  file =      {Cai2013Covariate.pdf:Cai2013Covariate.pdf:PDF},
  owner =     {mkolar},
  publisher = {Oxford University Press (OUP)},
  timestamp = {2014.02.13},
  url =       {http://dx.doi.org/10.1093/biomet/ass058}
}

@Article{Cai2011Constrained,
  Title                    = {A Constrained $\ell_1$ Minimization Approach To Sparse Precision Matrix Estimation},
  Author                   = tcai # { and W. Liu and X. Luo},
  Journal                  = jasa_s,
  Year                     = {2011},
  Number                   = {494},
  Pages                    = {594-607},
  Volume                   = {106},

  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Cai2014Two,
  author =     tcai #{ and Weidong Liu and Yin Xia},
  title =      {Two-sample test of high dimensional means under dependence},
  journal =    JRSSB_s,
  year =       {2014},
  volume =     {76},
  number =     {2},
  pages =      {349--372},
  doi =        {10.1111/rssb.12034},
  file =       {:Cai2014Two.pdf:PDF},
  fjournal =   {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  issn =       {1369-7412},
  mrclass =    {62H15 (62E20 62F05)},
  mrnumber =   {3164870},
  mrreviewer = {Konrad Furma{\'n}czyk},
  url =        {http://dx.doi.org/10.1111/rssb.12034}
}

@Article{Cai2013Optimal,
  Title                    = {Optimal hypothesis testing for high dimensional covariance matrices},
  Author                   = tcai # { and Zongming Ma},
  Journal                  = {Bernoulli},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {5B},
  Pages                    = {2359-2388},
  Volume                   = {19},

  Doi                      = {10.3150/12-BEJ455},
  Publisher                = {Project Euclid},
  Url                      = {http://dx.doi.org/10.3150/12-BEJ455}
}

@Article{Cai2013Optimala,
  Title                    = {Optimal rates of convergence for estimating {Toeplitz} covariance matrices},
  Author                   = tcai # { and Zhao Ren and } # hzhou,
  Journal                  = ptrf_s,
  Year                     = {2013},

  Month                    = {Jun},
  Number                   = {1-2},
  Pages                    = {101-143},
  Volume                   = {156},

  Doi                      = {10.1007/s00440-012-0422-7},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s00440-012-0422-7}
}

@Article{cai2008adaptive,
  Title                    = {Adaptive Variance Function Estimation In Heteroscedastic Nonparametric Regression},
  Author                   = tcai # { and L. Wang},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {2025--2054},
  Volume                   = {36},

  Doi                      = {10.1214/07-AOS509},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2458178 (2010d:62091)},
  Newspaper                = {Ann. Statist.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/07-AOS509}
}

@Article{cai2010shifting,
  Title                    = {Shifting Inequality And Recovery Of Sparse Signals},
  Author                   = tcai # { and L. Wang and G. Xu},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2010},
  Number                   = {3, part 1},
  Pages                    = {1300--1308},
  Volume                   = {58},

  Doi                      = {10.1109/TSP.2009.2034936},
  ISSN                     = {1053-587X},
  Mrnumber                 = {2730209 (2011f:94035)},
  Newspaper                = {IEEE Trans. Signal Proces.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1109/TSP.2009.2034936}
}

@Article{Cai2015High,
  author =         tcai #{ and Linjun Zhang},
  title =          {High-Dimensional Gaussian Copula Regression: Adaptive Estimation and Statistical Inference},
  journal =        {ArXiv e-prints, arXiv:1512.02487},
  year =           {2015},
  month =          dec,
  abstract =       {We develop adaptive estimation and inference methods for high-dimensional Gaussian copula regression that achieve the same performance without the knowledge of the marginal transformations as that for high-dimensional linear regression. Using a Kendall's tau based covariance matrix estimator, an $\ell_1$ regularized estimator is proposed and a corresponding de-biased estimator is developed for the construction of the confidence intervals and hypothesis tests. Theoretical properties of the procedures are studied and the proposed estimation and inference methods are shown to be adaptive to the unknown monotone marginal transformations. Prediction of the response for a given value of the covariates is also considered. The procedures are easy to implement and perform well numerically. The methods are also applied to analyze the Communities and Crime Unnormalized Data from the UCI Machine Learning Repository.},
  comments =       {41 pages, 1 figure},
  eprint =         {1512.02487},
  file =           {Cai2015High.pdf:Cai2015High.pdf:PDF},
  oai2identifier = {1512.02487},
  owner =          {mkolar},
  timestamp =      {2016.03.02}
}

@Book{Hastie1990Generalized,
  Title                    = {Generalized additive models},
  Author                   = thastie # { and } # rtibs,
  Publisher                = {Chapman and Hall, Ltd., London},
  Year                     = {1990},
  Series                   = {Monographs on Statistics and Applied Probability},
  Volume                   = {43},

  ISBN                     = {0-412-34390-8},
  Mrclass                  = {62J02 (62-07 62G05 62J20)},
  Mrnumber                 = {1082147 (92e:62117)},
  Mrreviewer               = {Bent J{\o}rgensen},
  Pages                    = {xvi+335}
}

@Article{Hastie1993Varying,
  author    = thastie #{ and } # rtibs,
  title     = {Varying-coefficient Models},
  journal   = jrssb_s,
  year      = {1993},
  volume    = {55},
  number    = {4},
  pages     = {757--796},
  newspaper = {J. R. Stat. Soc. B},
  owner     = {mkolar},
  timestamp = {2014.02.18},
}

@Article{zhang09consistency,
  Title                    = {On The Consistency Of Feature Selection Using Greedy Least Squares Regression},
  Author                   = tzhang,
  Journal                  = jmlr_s,
  Year                     = {2009},
  Pages                    = {555--568},
  Volume                   = {10},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2491749 (2010g:62221)},
  Newspaper                = {J. Mach. Learn. Res.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Zhang2010Analysis,
  Title                    = {Analysis of multi-stage convex relaxation for sparse regularization},
  Author                   = tzhang,
  Journal                  = jmlr_s,
  Year                     = {2010},
  Pages                    = {1081--1107},
  Volume                   = {11},

  Owner                    = {mkolar},
  Timestamp                = {2013.10.14},
  Url                      = {http://dl.acm.org/citation.cfm?id=1756041}
}

@Article{Zhang2011Sparse,
  author =    tzhang,
  title =     {Sparse recovery with orthogonal matching pursuit under RIP},
  journal =   IEEEit_s,
  year =      {2011},
  volume =    {57},
  number =    {9},
  pages =     {6215--6221},
  file =      {:Zhang2011Sparse.pdf:PDF},
  publisher = {IEEE}
}

@Article{Zhang2013Multi,
  Title                    = {Multi-stage Convex Relaxation for Feature Selection},
  Author                   = tzhang,
  Journal                  = {Bernoulli},
  Year                     = {2013},
  Pages                    = {to appear},

  Owner                    = {mkolar},
  Timestamp                = {2013.10.14}
}

@Book{DelaPena1999Decoupling,
  Title                    = {Decoupling: from dependence to independence},
  Author                   = vdlpena # { and } # egine,
  Publisher                = {Springer},
  Year                     = {1999}
}

@Book{Koltchinskii2011Oracle,
  Title                    = {Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems},
  Author                   = vkoltch,
  Publisher                = {Springer, Heidelberg},
  Year                     = {2011},
  Note                     = {Lectures from the 38th Probability Summer School held in
 Saint-Flour, 2008,
 {\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour.
 [Saint-Flour Probability Summer School]},
  Series                   = {Lecture Notes in Mathematics},
  Volume                   = {2033},

  Doi                      = {10.1007/978-3-642-22147-7},
  ISBN                     = {978-3-642-22146-0},
  Mrclass                  = {91B30 (60B11 62G08 62J07 68T05)},
  Mrnumber                 = {2829871 (2012i:91165)},
  Mrreviewer               = {Alexandre B. Tsybakov},
  Pages                    = {x+254},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-22147-7}
}

@Article{Koltchinskii2010Sparsity,
  author =     vkoltch #{ and } # myuan,
  title =      {Sparsity in multiple kernel learning},
  journal =    {Ann. Statist.},
  year =       {2010},
  volume =     {38},
  number =     {6},
  pages =      {3660--3695},
  coden =      {ASTSC7},
  doi =        {10.1214/10-AOS825},
  file =       {Koltchinskii2010Sparsity.pdf:Koltchinskii2010Sparsity.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G08 (62F12 62J07)},
  mrnumber =   {2766864 (2012a:62121)},
  mrreviewer = {Xiaogang Su},
  url =        {http://dx.doi.org/10.1214/10-AOS825}
}

@Article{Koltchinskii2011Nuclear,
  Title                    = {Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion},
  Author                   = vkoltch # { and Karim Lounici and } # atsybakov,
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {2302--2329},
  Volume                   = {39},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.aos/1322663459}
}

@Article{Haerdle1989Asymptotic,
  Title                    = {Asymptotic maximal deviation of {$M$}-smoothers},
  Author                   = whardle,
  Journal                  = jma_s,
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {163--179},
  Volume                   = {29},

  Doi                      = {10.1016/0047-259X(89)90022-5},
  Fjournal                 = {Journal of Multivariate Analysis},
  ISSN                     = {0047-259X},
  Mrclass                  = {62G05},
  Mrnumber                 = {1004333 (90h:62092)},
  Mrreviewer               = {Kumar Joag-Dev},
  Url                      = {http://dx.doi.org/10.1016/0047-259X(89)90022-5}
}

@Article{Liu2013Gaussian,
  author    = wliu,
  title     = {Gaussian Graphical Model Estimation with False Discovery Rate Control},
  journal   = aos_s,
  year      = {2013},
  volume    = {41},
  number    = {6},
  pages     = {2948--2978},
  issn      = {0090-5364},
  doi       = {10.1214/13-AOS1169},
  file      = {:Liu2013Gaussian.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  mrclass   = {62H12 (62H15 62J15)},
  mrnumber  = {3161453},
  timestamp = {2018.04.30},
  url       = {http://dx.doi.org/10.1214/13-AOS1169},
}

@Article{Ingster2010Detection,
  Title                    = {Detection boundary in sparse regression},
  Author                   = yingster # { and } # atsybakov # { and Nicolas Verzelen},
  Journal                  = ejs_s,
  Year                     = {2010},
  Pages                    = {1476--1526},
  Volume                   = {4},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.ejs/1293028087}
}

@Article{abbeel06learning,
  Title                    = {Learning Factor Graphs In Polynomial Time And Sample Complexity},
  Author                   = {P. Abbeel and D. Koller and A.~Y. Ng},
  Journal                  = jmlr_s,
  Year                     = {2006},
  Pages                    = {1743--1788},
  Volume                   = {7},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2274423},
  Newspaper                = {J. Mach. Learn. Res.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Aeron2010Information,
  Title                    = {Information theoretic bounds for compressed sensing},
  Author                   = {Shuchin Aeron and Venkatesh Saligrama and Manqi Zhao},
  Journal                  = IEEEit_s,
  Year                     = {2010},
  Number                   = {10},
  Pages                    = {5111--5130},
  Volume                   = {56},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5571873}
}

@Article{afifi66missing,
  Title                    = {Missing Observations In Multivariate Statistics. {i}. {r}eview Of The Literature},
  Author                   = {A.~A. Afifi and R.~M. Elashoff},
  Journal                  = jasa_s,
  Year                     = {1966},
  Pages                    = {595--604},
  Volume                   = {61},

  ISSN                     = {0162-1459},
  Mrnumber                 = {0203865 (34 \#3712)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Agarwal2012Fast,
  author =     {Alekh Agarwal and Sahand Negahban and } # mwainw,
  title =      {Fast global convergence of gradient methods for high-dimensional statistical recovery},
  journal =    aos_s,
  year =       {2012},
  volume =     {40},
  number =     {5},
  pages =      {2452--2482},
  doi =        {10.1214/12-AOS1032},
  file =       {:Agarwal2012Fast.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62F30 (62H12 62J07)},
  mrnumber =   {3097609},
  mrreviewer = {Marco Bee},
  url =        {http://dx.doi.org/10.1214/12-AOS1032}
}

@Article{Ahelegbey2015Bayesian,
  author =    {Daniel Felix Ahelegbey and Monica Billio and Roberto Casarin},
  title =     {{Bayes}ian Graphical Models for {STructural} Vector Autoregressive Processes},
  journal =   {J. Appl. Econ.},
  year =      {2015},
  pages =     {n/a--n/a},
  month =     {feb},
  doi =       {10.1002/jae.2443},
  file =      {Ahelegbey2015Bayesian.pdf:Ahelegbey2015Bayesian.pdf:PDF},
  owner =     {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1002/jae.2443}
}

@Article{Akaike1974new,
  Title                    = {A new look at the statistical model identification},
  Author                   = {Hirotugu Akaike},
  Journal                  = {IEEE Trans. Automat. Contr.},
  Year                     = {1974},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {716-723},
  Volume                   = {19},

  Doi                      = {10.1109/TAC.1974.1100705},
  Owner                    = {mkolar},
  Publisher                = {Institute of Electrical and Electronics Engineers},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1109/TAC.1974.1100705}
}

@InCollection{Alaoui2015Fast,
  author =    {Alaoui, Ahmed and Mahoney, Michael W},
  title =     {Fast Randomized Kernel Ridge Regression with Statistical Guarantees},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {775--783},
  file =      {:Alaoui2015Fast.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5716-fast-randomized-kernel-ridge-regression-with-statistical-guarantees.pdf}
}

@Article{Albert2002Statistical,
  Title                    = {Statistical mechanics of complex networks},
  Author                   = {R\'eka Albert and Albert-L\'aszl\'o Barab\'asi},
  Journal                  = {Rev. Mod. Phys.},
  Year                     = {2002},

  Month                    = {Jan},
  Pages                    = {47--97},
  Volume                   = {74},

  Doi                      = {10.1103/RevModPhys.74.47},
  Issue                    = {1},
  Numpages                 = {0},
  Owner                    = {mkolar},
  Publisher                = {American Physical Society},
  Timestamp                = {2014.02.18},
  Url                      = {http://link.aps.org/doi/10.1103/RevModPhys.74.47}
}

@InCollection{aldous85exchangeability,
  Title                    = {Exchangeability And Related Topics},
  Author                   = {D.~J. Aldous},
  Booktitle                = {\'{E}cole d'\'et\'e de probabilit\'es de {S}aint-{F}lour, {XIII}---1983},
  Publisher                = {Springer},
  Year                     = {1985},

  Address                  = {Berlin},
  Pages                    = {1--198},
  Series                   = {Lecture Notes in Math.},
  Volume                   = {1117},

  Doi                      = {10.1007/BFb0099421},
  Mrnumber                 = {883646 (88d:60107)},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/BFb0099421}
}

@Article{alquier2008lasso,
  Title                    = {L{asso}, Iterative Feature Selection And The Correlation Selector: Oracle Inequalities And Numerical Performances},
  Author                   = {P. Alquier},
  Journal                  = ejs_s,
  Year                     = {2008},
  Pages                    = {1129--1152},
  Volume                   = {2},

  Doi                      = {10.1214/08-EJS288},
  ISSN                     = {1935-7524},
  Mrnumber                 = {2460860 (2009m:62107)},
  Newspaper                = {Electron. J. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/08-EJS288}
}

@InProceedings{Amit2007Uncovering,
  author =       {Yonatan Amit and Michael Fink and } # nsrebro #{ and Shimon Ullman},
  title =        {Uncovering shared structures in multiclass classification},
  booktitle =    {Proceedings of the 24th international conference on Machine learning},
  year =         {2007},
  pages =        {17--24},
  organization = {ACM},
  file =         {:Amit2007Uncovering.pdf:PDF}
}

@Article{Anandkumar2012High,
  author =    {Animashree Anandkumar and Vincent Y.~F. Tan and Furong Huang and } # awillsky,
  title =     {High-dimensional Gaussian graphical model selection: Walk summability and local separation criterion},
  journal =   jmlr_s,
  year =      {2012},
  volume =    {13},
  number =    {1},
  pages =     {2293--2337},
  file =      {:Anandkumar2012High.pdf:PDF},
  publisher = {JMLR. org}
}

@Article{anderson57maximum,
  Title                    = {Maximum Likelihood Estimates For A Multivariate Normal Distribution When Some Observations Are Missing},
  Author                   = {T.~W. Anderson},
  Journal                  = jasa_s,
  Year                     = {1957},
  Pages                    = {200--203},
  Volume                   = {52},

  ISSN                     = {0162-1459},
  Mrnumber                 = {0087286 (19,332d)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{Anderson2003introduction,
  Title                    = {An introduction to multivariate statistical analysis},
  Author                   = {T. W. Anderson},
  Publisher                = {Wiley-Interscience [John Wiley \& Sons]},
  Year                     = {2003},

  Address                  = {Hoboken, NJ},
  Edition                  = {Third},
  Series                   = {Wiley Series in Probability and Statistics},

  ISBN                     = {0-471-36091-0},
  Mrclass                  = {62-01 (62Hxx)},
  Mrnumber                 = {1990662 (2004c:62001)},
  Owner                    = {mkolar},
  Pages                    = {xx+721},
  Timestamp                = {2014.02.18}
}

@Article{Ando2005framework,
  author =    {Rie Kubota Ando and } # tzhang,
  title =     {A framework for learning predictive structures from multiple tasks and unlabeled data},
  journal =   jmlr_s,
  year =      {2005},
  volume =    {6},
  pages =     {1817--1853},
  file =      {:Ando2005framework.pdf:PDF},
  publisher = {JMLR. org}
}

@Article{Andrews-Hanna2007Disruption,
  Title                    = {Disruption Of Large-scale Brain Systems In Advanced Aging},
  Author                   = {J.~R. Andrews-Hanna and A.~Z. Snyder and J.~L. Vincent and C. Lustig and D. Head and M.~E. Raichle and R.~L. Buckner},
  Journal                  = {Neuron},
  Year                     = {2007},
  Number                   = {5},
  Pages                    = {924-935},
  Volume                   = {56},

  Newspaper                = {Neuron},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{andrieu2003efficient,
  Title                    = {Efficient Particle Filtering For Jump {m}arkov Systems. {a}pplication To Time-varying Autoregressions},
  Author                   = {C. Andrieu and M. Davy and A. Doucet},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2003},
  Number                   = {7},
  Pages                    = {1762--1770},
  Volume                   = {51},

  Doi                      = {10.1109/TSP.2003.810284},
  ISSN                     = {1053-587X},
  Mrnumber                 = {1996962 (2005e:94017)},
  Newspaper                = {IEEE Trans. Signal Proces.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1109/TSP.2003.810284}
}

@Article{Angles2015NYTRO,
  author =         {Tomas Angles and Raffaello Camoriano and Alessandro Rudi and Lorenzo Rosasco},
  title =          {NYTRO: When Subsampling Meets Early Stopping},
  journal =        {ArXiv e-prints, arXiv:1510.05684},
  year =           {2015},
  month =          oct,
  abstract =       {Early stopping is a well known approach to reduce the time complexity for performing training and model selection of large scale learning machines. On the other hand, memory/space (rather than time) complexity is the main constraint in many applications, and randomized subsampling techniques have been proposed to tackle this issue. In this paper we ask whether early stopping and subsampling ideas can be combined in a fruitful way. We consider the question in a least squares regression setting and propose a form of randomized iterative regularization based on early stopping and subsampling. In this context, we analyze the statistical and computational properties of the proposed method. Theoretical results are complemented and validated by a thorough experimental analysis.},
  eprint =         {1510.05684},
  file =           {:Angles2015NYTRO.pdf:PDF},
  oai2identifier = {1510.05684},
  owner =          {mkolar},
  timestamp =      {2016.03.10}
}

@Article{Aragam2015Learning,
  author =         {Bryon Aragam and Arash A. Amini and Qing Zhou},
  title =          {Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression},
  journal =        {ArXiv e-prints, arXiv:1511.08963},
  year =           {2015},
  month =          nov,
  abstract =       {We consider the problem of estimating a directed acyclic graph (DAG) for a multivariate normal distribution from high-dimensional data with $p\gg n$. Our main results establish nonasymptotic deviation bounds on the estimation error, sparsity bounds, and model selection consistency for a penalized least squares estimator under concave regularization. The proofs rely on interpreting the graphical model as a recursive linear structural equation model, which reduces the estimation problem to a series of tractable neighbourhood regressions and allows us to avoid making any assumptions regarding faithfulness. In doing so, we provide some novel techniques for handling general nonidentifiable and nonconvex problems. These techniques are used to guarantee uniform control over a superexponential number of neighbourhood regression problems by exploiting various notions of monotonicity among them. Our results apply to a wide variety of practical situations that allow for arbitrary nondegenerate covariance structures as well as many popular regularizers including the MCP, SCAD, $\ell_{0}$ and $\ell_{1}$.},
  comments =       {46 pages, 2 figures},
  eprint =         {1511.08963},
  file =           {:Aragam2015Learning.pdf:PDF},
  oai2identifier = {1511.08963},
  owner =          {mkolar},
  timestamp =      {2016.03.11}
}

@Article{Aragam2015Concave,
  author =  {Bryon Aragam and Qing Zhou},
  title =   {Concave Penalized Estimation of Sparse Gaussian Bayesian Networks},
  journal = jmlr_s,
  year =    {2015},
  volume =  {16},
  pages =   {2273-2328},
  file =    {:Aragam2015Concave.pdf:PDF},
  url =     {http://jmlr.org/papers/v16/aragam15a.html}
}

@Article{Arbeitmanetal2002,
  Title                    = {Gene Expression During The Life Cycle Of Drosophila Melanogaster},
  Author                   = {M.~N. Arbeitman and E.~E.~M. Furlong and F. Imam and E. Johnson and B.~H. Null and B.~S. Baker and M.~A. Krasnow and M.~P. Scott and R.~W. Davis and K.~P. White},
  Journal                  = {Science},
  Year                     = {2002},
  Number                   = {5590},
  Pages                    = {2270--2275},
  Volume                   = {297},

  Newspaper                = {Science},
  Owner                    = {mkolar},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2014.02.18}
}

@Article{Arcones1992bootstrap,
  author =     {Arcones, Miguel A. and Gin{\'e}, Evarist},
  title =      {On the bootstrap of {$U$} and {$V$} statistics},
  journal =    aos_s,
  year =       {1992},
  volume =     {20},
  number =     {2},
  pages =      {655--674},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1176348650},
  file =       {Arcones1992bootstrap.pdf:Arcones1992bootstrap.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62E20 (60F05 62F05 62G09)},
  mrnumber =   {1165586},
  mrreviewer = {Michael R. Chernick},
  url =        {http://dx.doi.org/10.1214/aos/1176348650}
}

@Article{argyriou08convex,
  author =    {A. {Argyriou} and T. {Evgeniou} and M. {Pontil}},
  title =     {Convex Multi-task Feature Learning},
  journal =   {Mach. Learn.},
  year =      {2008},
  volume =    {73},
  number =    {3},
  pages =     {243--272},
  file =      {:argyriou08convex.pdf:PDF},
  newspaper = {Mach. Learn.},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Article{Arjevani2015Communication,
  author =         {Yossi Arjevani and Ohad Shamir},
  title =          {Communication Complexity of Distributed Convex Learning and Optimization},
  journal =        {ArXiv e-prints, arXiv:1506.01900},
  year =           {2015},
  month =          jun,
  abstract =       {We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.},
  eprint =         {1506.01900},
  file =           {:Arjevani2015Communication.pdf:PDF},
  oai2identifier = {1506.01900}
}

@InProceedings{arlot09datadriven,
  author =    {S. Arlot and } # fbach,
  title =     {Data-driven Calibration Of Linear Estimators With Minimal Penalties},
  booktitle = {Proc. of NIPS},
  year =      {2009},
  editor =    {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  pages =     {46--54},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Book{Arnold1999Conditional,
  title =      {Conditional specification of statistical models},
  publisher =  {Springer-Verlag, New York},
  year =       {1999},
  author =     {Arnold, Barry C. and Castillo, Enrique and Sarabia, Jos{\'e} Mar{\'{\i}}a},
  series =     {Springer Series in Statistics},
  isbn =       {0-387-98761-4},
  mrclass =    {60E05 (62E10)},
  mrnumber =   {1716531},
  mrreviewer = {J. A. Melamed},
  pages =      {xvi+411}
}

@Article{Arnold2001Conditionally,
  Title                    = {Conditionally specified distributions: an introduction},
  Author                   = {Barry C. Arnold and Enrique Castillo and Jos{\'e} Mar{\'{\i}}a Sarabia},
  Journal                  = statsci_s,
  Year                     = {2001},
  Note                     = {With comments and a rejoinder by the authors},
  Number                   = {3},
  Pages                    = {249--274},
  Volume                   = {16},

  Doi                      = {10.1214/ss/1009213728},
  Fjournal                 = {Statistical Science. A Review Journal of the Institute of
 Mathematical Statistics},
  ISSN                     = {0883-4237},
  Mrclass                  = {62G05 (62F15)},
  Mrnumber                 = {1874154},
  Url                      = {http://dx.doi.org/10.1214/ss/1009213728}
}

@Article{Avalos2007Parsimonious,
  Title                    = {Parsimonious additive models},
  Author                   = {Marta Avalos and Yves Grandvalet and Christophe Ambroise},
  Journal                  = csda_s,
  Year                     = {2007},
  Number                   = {6},
  Pages                    = {2851--2870},
  Volume                   = {51},

  Doi                      = {10.1016/j.csda.2006.10.007},
  Fjournal                 = {Computational Statistics \& Data Analysis},
  ISSN                     = {0167-9473},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2345610},
  Url                      = {http://dx.doi.org/10.1016/j.csda.2006.10.007}
}

@Article{Buehlmann2014CAM,
  author =     {B{\"u}hlmann, Peter and Peters, Jonas and Ernest, Jan},
  title =      {C{AM}: causal additive models, high-dimensional order search and penalized regression},
  journal =    aos_s,
  year =       {2014},
  volume =     {42},
  number =     {6},
  pages =      {2526--2556},
  doi =        {10.1214/14-AOS1260},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G99 (62H99 68T99)},
  mrnumber =   {3277670},
  mrreviewer = {Hiroto Hyakutake},
  url =        {http://dx.doi.org/10.1214/14-AOS1260}
}

@Article{Bahadur1966note,
  Title                    = {A note on quantiles in large samples},
  Author                   = {R.~R. Bahadur},
  Journal                  = {Ann. Math. Statist.},
  Year                     = {1966},
  Pages                    = {577--580},
  Volume                   = {37},

  Fjournal                 = {Annals of Mathematical Statistics},
  ISSN                     = {0003-4851},
  Mrclass                  = {60.30 (62.75)},
  Mrnumber                 = {0189095 (32 \#6522)},
  Mrreviewer               = {H. Kesten}
}

@Article{Bai98estimating,
  author =    {J. Bai and P. Perron},
  title =     {Estimating And Testing Linear Models With Multiple Structural Changes},
  journal =   {Econometrica},
  year =      {1998},
  volume =    {66},
  number =    {1},
  pages =     {47--78},
  doi =       {10.2307/2998540},
  file =      {:Bai98estimating.pdf:PDF},
  issn =      {0012-9682},
  mrnumber =  {1616121 (98m:62184)},
  newspaper = {Econometrica},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.2307/2998540}
}

@Article{Balakrishnan2012Recovering,
  author   = {Sivaraman Balakrishnan and Mladen Kolar and Alessandro Rinaldo and Aarti Singh},
  title    = {Recovering block-structured activations using compressive measurements},
  journal  = {Electron. J. Statist.},
  year     = {2017},
  volume   = {11},
  number   = {1},
  pages    = {2647-2678},
  issn     = {1935-7524},
  doi      = {10.1214/17-EJS1267},
  fjournal = {Electronic Journal of Statistics},
  sici     = {1935-7524(2017)11:1<2647:RBSAUC>2.0.CO;2-N},
}

@InProceedings{balakrishnan2011statistical,
  author =    {Balakrishnan, Sivaraman and Kolar, Mladen and Rinaldo, Alessandro and Singh, Aarti and Wasserman, Larry},
  title =     {Statistical and computational tradeoffs in biclustering},
  booktitle = {NIPS 2011 Workshop on Computational Trade-offs in Statistical Learning},
  year =      {2011},
  volume =    {4}
}

@InCollection{Banerjee2014Estimation,
  author =    {Banerjee, Arindam and Chen, Sheng and Fazayeli, Farideh and Sivakumar, Vidyashankar},
  title =     {Estimation with Norm Regularization},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year =      {2014},
  editor =    {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages =     {1556--1564},
  file =      {:Banerjee2014Estimation.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5465-estimation-with-norm-regularization.pdf}
}

@Article{Banerjee2008Model,
  Title                    = {Model Selection Through Sparse Maximum Likelihood Estimation},
  Author                   = {O. Banerjee and L. {El Ghaoui} and A. d'Aspremont},
  Journal                  = jmlr_s,
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {485-516},
  Volume                   = {9},

  Newspaper                = {J. Mach. Learn. Res.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Barabasi1999Emergence,
  Title                    = {Emergence of Scaling in Random Networks},
  Author                   = {Albert-L\'aszl\'o Barab\'asi and R\'eka Albert},
  Journal                  = {Science},
  Year                     = {1999},

  Month                    = {Oct},
  Number                   = {5439},
  Pages                    = {509-512},
  Volume                   = {286},

  Doi                      = {10.1126/science.286.5439.509},
  Owner                    = {mkolar},
  Publisher                = {American Association for the Advancement of Science (AAAS)},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1126/science.286.5439.509}
}

@Article{baraud02non-asymptotic,
  Title                    = {Non-asymptotic Minimax Rates Of Testing In Signal Detection},
  Author                   = {Y. Baraud},
  Journal                  = {Bernoulli},
  Year                     = {2002},
  Number                   = {5},
  Pages                    = {577--606},
  Volume                   = {8},

  Newspaper                = {Bernoulli},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Barber2015ROCKET,
  author    = {Rina Foygel Barber and Mladen Kolar},
  title     = {ROCKET: Robust confidence intervals via Kendall's tau for transelliptical graphical models},
  journal   = {Ann. Statist.},
  year      = {2018},
  volume    = {46},
  number    = {6B},
  pages     = {3422-3450},
  issn      = {0090-5364},
  doi       = {10.1214/17-AOS1663},
  fjournal  = {Annals of Statistics},
  sici      = {0090-5364(2018)46:6B<3422:RURBUI>2.0.CO;2-J},
  timestamp = {2018.09.11},
}

@Article{Barigozzi2014NETS,
  author =    {Barigozzi, Matteo and Brownlees, Christian T},
  title =     {NETS: network estimation for time series},
  journal =   {Available at SSRN 2249909},
  year =      {2014},
  file =      {:Barigozzi2014NETS.pdf:PDF},
  owner =     {mkolar},
  timestamp = {2016.02.09}
}

@Article{barron08approximation,
  Title                    = {Approximation And Learning By Greedy Algorithms},
  Author                   = {A.~R. Barron and A. Cohen and W. Dahmen and } # rdevore,
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {64--94},
  Volume                   = {36},

  Doi                      = {10.1214/009053607000000631},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2387964 (2009c:62055)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053607000000631}
}

@Article{Basu2015Regularized,
  author =     {Basu, Sumanta and Michailidis, George},
  title =      {Regularized estimation in sparse high-dimensional time series models},
  journal =    aos_s,
  year =       {2015},
  volume =     {43},
  number =     {4},
  pages =      {1535--1567},
  doi =        {10.1214/15-AOS1315},
  file =       {:Basu2015Regularized.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62M10 (62J07 62M15)},
  mrnumber =   {3357870},
  mrreviewer = {Yuzo Hosoya},
  url =        {http://dx.doi.org/10.1214/15-AOS1315}
}

@Article{Basu2012Network.pdf,
  author =  {Sumanta Basu and Ali Shojaie and George Michailidis},
  title =   {Network Granger Causality with Inherent Grouping Structure},
  journal = jmlr_s,
  year =    {2015},
  volume =  {16},
  pages =   {417-453},
  file =    {:Basu2012Network.pdf:PDF},
  url =     {http://jmlr.org/papers/v16/basu15a.html}
}

@Article{Battey2015Distributed,
  author =         {Heather Battey and Jianqing Fan and Han Liu and Junwei Lu and Ziwei Zhu},
  title =          {Distributed Estimation and Inference with Statistical Guarantees},
  journal =        {ArXiv e-prints, arXiv:1509.05457},
  year =           {2015},
  month =          sep,
  abstract =       {This paper studies hypothesis testing and parameter estimation in the context of the divide and conquer algorithm. In a unified likelihood based framework, we propose new test statistics and point estimators obtained by aggregating various statistics from $k$ subsamples of size $n/k$, where $n$ is the sample size. In both low dimensional and high dimensional settings, we address the important question of how to choose $k$ as $n$ grows large, providing a theoretical upper bound on $k$ such that the information loss due to the divide and conquer algorithm is negligible. In other words, the resulting estimators have the same inferential efficiencies and estimation rates as a practically infeasible oracle with access to the full sample. Thorough numerical results are provided to back up the theory.},
  eprint =         {1509.05457},
  file =           {:Battey2015Distributed.pdf:PDF},
  oai2identifier = {1509.05457}
}

@InProceedings{baxter1995learning,
  Title                    = {Learning Internal Representations},
  Author                   = {J. Baxter},
  Booktitle                = {Proc. of COLT},
  Year                     = {1995},
  Organization             = {ACM},
  Pages                    = {311--320},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Beck2009Fast,
  Title                    = {A Fast Iterative Shrinkage-thresholding Algorithm For Linear Inverse Problems},
  Author                   = {A. Beck and M. Teboulle},
  Journal                  = {SIAM J. Imag. Sci.},
  Year                     = {2009},
  Pages                    = {183-202},
  Volume                   = {2},

  Newspaper                = {SIAM J. Imag. Sci.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{Bekkerman2011Scaling,
  title =     {Scaling up machine learning: Parallel and distributed approaches},
  publisher = {Cambridge University Press},
  year =      {2011},
  author =    {Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
  owner =     {mkolar},
  timestamp = {2016.02.10}
}

@InCollection{Bellet2015Distributed,
  author =    {Aur{\'{e}}lien Bellet and Yingyu Liang and Alireza Bagheri Garakani and } # mbalcan #{ and Fei Sha},
  title =     {A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning},
  booktitle = {Proceedings of the 2015 {SIAM} International Conference on Data Mining},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year =      {2015},
  pages =     {478--486},
  month =     {jun},
  doi =       {10.1137/1.9781611974010.54},
  file =      {:Bellet2015Distributed.pdf:PDF},
  url =       {http://dx.doi.org/10.1137/1.9781611974010.54}
}

@Article{Belloni2012Sparse,
  Title                    = {Sparse models and methods for optimal instruments with an application to eminent domain},
  Author                   = {Alexandre Belloni and Daniel Chen and Victor Chernozhukov and } # chansen,
  Journal                  = {Econometrica},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {2369--2429},
  Volume                   = {80},

  Owner                    = {mkolar},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2013.10.15},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA9626/abstract}
}

@Article{Belloni2016Quantile,
  author =         {Alexandre Belloni and Mingli Chen and Victor Chernozhukov},
  title =          {Quantile Graphical Models: Prediction and Conditional Independence with Applications to Financial Risk Management},
  journal =        {ArXiv e-prints, arXiv:1607.00286},
  year =           {2016},
  month =          jul,
  abstract =       {We propose Quantile Graphical Models (QGMs) to characterize predictive and conditional independence relationships within a set of random variables of interest. This framework is intended to quantify the dependence in non-Gaussian settings which are ubiquitous in many econometric applications. We consider two distinct QGMs. First, Condition Independence QGMs characterize conditional independence at each quantile index revealing the distributional dependence structure. Second, Predictive QGMs characterize the best linear predictor under asymmetric loss functions. Under Gaussianity these notions essentially coincide but non-Gaussian settings lead us to different models as prediction and conditional independence are fundamentally different properties. Combined the models complement the methods based on normal and nonparanormal distributions that study mean predictability and use covariance and precision matrices for conditional independence. We also propose estimators for each QGMs. The estimators are based on high-dimension techniques including (a continuum of) $\ell_{1}$-penalized quantile regressions and low biased equations, which allows us to handle the potentially large number of variables. We build upon recent results to obtain valid choice of the penalty parameters and rates of convergence. These results are derived without any assumptions on the separation from zero and are uniformly valid across a wide-range of models. With the additional assumptions that the coefficients are well-separated from zero, we can consistently estimate the graph associated with the dependence structure by hard thresholding the proposed estimators. Further we show how QGM can be used in measuring systemic risk contributions and the impact of downside movement in the market on the dependence structure of assets' return.},
  eprint =         {1607.00286},
  oai2identifier = {1607.00286},
  owner =          {mkolar},
  timestamp =      {2016.09.23}
}

@Article{Belloni2011penalized,
  author =    {Alexandre Belloni and Victor Chernozhukov},
  title =     {$\ell_1$-penalized quantile regression in high-dimensional sparse models},
  journal =   aos_s,
  year =      {2011},
  volume =    {39},
  number =    {1},
  pages =     {82--130},
  doi =       {10.1214/10-aos827},
  file =      {Belloni2011penalized.pdf:Belloni2011penalized.pdf:PDF},
  issn =      {0090-5364},
  publisher = {Institute of Mathematical Statistics - care of Project Euclid},
  url =       {http://dx.doi.org/10.1214/10-AOS827}
}

@Article{Belloni2013Least,
  author    = {Alexandre Belloni and Victor Chernozhukov},
  title     = {Least squares after model selection in high-dimensional sparse models},
  journal   = {Bernoulli},
  year      = {2013},
  volume    = {19},
  number    = {2},
  pages     = {521--547},
  month     = {May},
  issn      = {1350-7265},
  doi       = {10.3150/11-bej410},
  file      = {Belloni2013Least.pdf:Belloni2013Least.pdf:PDF},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability},
  url       = {http://dx.doi.org/10.3150/11-BEJ410},
}

@Article{Belloni2012Inference,
  author =    {Alexandre Belloni and Victor Chernozhukov and } # chansen,
  title =     {Inference on Treatment Effects After Selection Amongst High-Dimensional Controls},
  journal =   {Rev. Econ. Stud.},
  year =      {2013},
  volume =    {81},
  number =    {2},
  pages =     {608--650},
  month =     {Nov},
  doi =       {10.1093/restud/rdt044},
  file =      {Belloni2012Inference.pdf:Belloni2012Inference.pdf:PDF},
  issn =      {1467-937X},
  owner =     {mkolar},
  publisher = {Oxford University Press (OUP)},
  timestamp = {2014.11.25},
  url =       {http://dx.doi.org/10.1093/restud/rdt044}
}

@Article{Belloni2012Asymptotic,
  Title                    = {On the Asymptotic Theory for Least Squares Series: Pointwise and Uniform Results},
  Author                   = {Alexandre Belloni and Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  Journal                  = {arXiv preprint arXiv:1212.0442},
  Year                     = {2012},

  Month                    = dec,

  Abstract                 = {In this work we consider series estimators for the conditional mean in light of three new ingredients: (i) sharp LLNs for matrices derived from the non-commutative Khinchin inequalities, (ii) bounds on the Lebesgue constant that controls the ratio between the $L^{\infty}$ and $L^{2}$-norms, and (iii) maximal inequalities with data-dependent bounds for processes whose entropy integrals diverge at some rate. These technical tools allow us to contribute to the series literature, specifically the seminal work of Newey (1995), as follows. First, we weaken considerably the condition on the number $k$ of approximating functions used in series estimation from the typical $k^2/n \to 0$ to $k/n \to 0$, up to log factors, which was available only for splines before. Second, under the same weak conditions we derive $L^{2}$ rates and pointwise central limit theorems results when the approximation error vanishes. Under a incorrectly specified model, i.e. when the approximation error does not vanish, analogous results are also shown. Third, under stronger conditions we derive uniform rates and functional central limit theorems that holds if the approximation error vanishes or not. That is, we derive the strong approximation for the entire estimate of the non-parametric function. Finally, we derive uniform rates and inference results for linear functionals of interest of the conditional expectation function such as its partial derivative or conditional average partial derivative.},
  Eprint                   = {1212.0442},
  Oai2identifier           = {1212.0442}
}

@Article{Belloni2015Uniformly,
  author =         {Alexandre Belloni and Victor Chernozhukov and Denis Chetverikov and Ying Wei},
  title =          {Uniformly Valid Post-Regularization Confidence Regions for Many Functional Parameters in Z-Estimation Framework},
  journal =        {ArXiv e-prints, arXiv:1512.07619},
  year =           {2015},
  month =          dec,
  abstract =       {In this paper we develop procedures to construct simultaneous confidence bands for $\tilde p$ potentially infinite-dimensional parameters after model selection for general moment condition models where $\tilde p$ is potentially much larger than the sample size of available data, $n$. This allows us to cover settings with functional response data where each of the $\tilde p$ parameters is a function. The procedure is based on the construction of score functions that satisfy certain orthogonality condition. The proposed simultaneous confidence bands rely on uniform central limit theorems for high-dimensional vectors (and not on Donsker arguments as we allow for $\tilde p \gg n$). To construct the bands, we employ a multiplier bootstrap procedure which is computationally efficient as it only involves resampling the estimated score functions (and does not require resolving the high-dimensional optimization problems). We formally apply the general theory to inference on regression coefficient process in the distribution regression model with a logistic link, where two implementations are analyzed in detail. Simulations and an application to real data are provided to help illustrate the applicability of the results.},
  comments =       {2 figures},
  eprint =         {1512.07619},
  file =           {:Belloni2015Uniformly.pdf:PDF;:Belloni2015Uniformly.pdf:PDF},
  oai2identifier = {1512.07619},
  owner =          {mkolar},
  timestamp =      {2016.02.08}
}

@Article{Belloni2013Robust,
  author =         {Alexandre Belloni and Victor Chernozhukov and Kengo Kato},
  title =          {Valid Post-Selection Inference in High-Dimensional Approximately Sparse Quantile Regression Models},
  journal =        {arXiv preprint arXiv:1312.7186},
  year =           {2013},
  month =          dec,
  abstract =       {This work proposes new inference methods for the estimation of a regression coefficient of interest in quantile regression models. We consider high-dimensional models where the number of regressors potentially exceeds the sample size but a subset of them suffice to construct a reasonable approximation of the unknown quantile regression function in the model. The proposed methods are protected against moderate model selection mistakes, which are often inevitable in the approximately sparse model considered here. The methods construct (implicitly or explicitly) an optimal instrument as a residual from a density-weighted projection of the regressor of interest on other regressors. Under regularity conditions, the proposed estimators of the quantile regression coefficient are asymptotically root-$n$ normal, with variance equal to the semi-parametric efficiency bound of the partially linear quantile regression model. In addition, the performance of the technique is illustrated through Monte-carlo experiments and an empirical example, dealing with risk factors in childhood malnutrition. The numerical results confirm the theoretical findings that the proposed methods should outperform the naive post-model selection methods in non-parametric settings. Moreover, the empirical results demonstrate soundness of the proposed methods.},
  eprint =         {1312.7186},
  file =           {Belloni2013Robust.pdf:Belloni2013Robust.pdf:PDF},
  oai2identifier = {1312.7186}
}

@Article{Belloni2013Uniform,
  author =   {Alexandre Belloni and Victor Chernozhukov and Kengo Kato},
  title =    {Uniform post-selection inference for least absolute deviation regression and other {Z}-estimation problems},
  journal =  {Biometrika},
  year =     {2015},
  volume =   {102},
  number =   {1},
  pages =    {77--94},
  doi =      {10.1093/biomet/asu056},
  file =     {Belloni2013Uniform.pdf:Belloni2013Uniform.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62F12 (62H05 62H12 62J05)},
  mrnumber = {3335097},
  url =      {http://dx.doi.org/10.1093/biomet/asu056}
}

@Article{Belloni2011Square,
  author =     {Belloni, A. and Chernozhukov, V. and Wang, L.},
  title =      {Square-root lasso: pivotal recovery of sparse signals via conic programming},
  journal =    {Biometrika},
  year =       {2011},
  volume =     {98},
  number =     {4},
  pages =      {791--806},
  coden =      {BIOKAX},
  doi =        {10.1093/biomet/asr043},
  file =       {:Belloni2011Square.pdf:PDF},
  fjournal =   {Biometrika},
  issn =       {0006-3444},
  mrclass =    {62J07 (90C90 94A12)},
  mrnumber =   {2860324},
  mrreviewer = {Su-Yun Chen Huang},
  url =        {http://dx.doi.org/10.1093/biomet/asr043}
}

@Article{Belloni2013Honest,
  author   = {Belloni, Alexandre and Chernozhukov, Victor and Wei, Ying},
  title    = {Post-selection inference for generalized linear models with many controls},
  journal  = {J. Bus. Econom. Statist.},
  year     = {2016},
  volume   = {34},
  number   = {4},
  pages    = {606--619},
  issn     = {0735-0015},
  doi      = {10.1080/07350015.2016.1166116},
  file     = {Belloni2013Honest.pdf:Belloni2013Honest.pdf:PDF},
  fjournal = {Journal of Business \& Economic Statistics},
  mrclass  = {62J12 (62F35)},
  mrnumber = {3547999},
  url      = {https://doi.org/10.1080/07350015.2016.1166116},
}

@Article{Bentkus1996Berry,
  author =     {Bentkus, V. and Bloznelis, M. and G{\"o}tze, F.},
  title =      {A {B}erry-{E}ss\'een bound for {S}tudent's statistic in the non-i.i.d.\ case},
  journal =    {J. Theoret. Probab.},
  year =       {1996},
  volume =     {9},
  number =     {3},
  pages =      {765--796},
  coden =      {JTPREO},
  doi =        {10.1007/BF02214086},
  file =       {Bentkus1996Berry.pdf:Bentkus1996Berry.pdf:PDF},
  fjournal =   {Journal of Theoretical Probability},
  issn =       {0894-9840},
  mrclass =    {60F05 (62E20)},
  mrnumber =   {1400598},
  mrreviewer = {Zuzana Pr{\'a}{\v{s}}kov{\'a}},
  url =        {http://dx.doi.org/10.1007/BF02214086}
}

@Article{Berk2013Valid,
  Title                    = {Valid post-selection inference},
  Author                   = {Richard Berk and } # lbrown # { and Andreas Buja and Kai Zhang and Linda Zhao},
  Journal                  = aos_s,
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {802--837},
  Volume                   = {41},

  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2013.10.25},
  Url                      = {http://projecteuclid.org/euclid.aos/1369836961}
}

@InCollection{Berthet2006Revisiting,
  author =     {Berthet, Philippe and Mason, David M.},
  title =      {Revisiting two strong approximation results of {D}udley and {P}hilipp},
  booktitle =  {High dimensional probability},
  publisher =  {Inst. Math. Statist., Beachwood, OH},
  year =       {2006},
  volume =     {51},
  series =     {IMS Lecture Notes Monogr. Ser.},
  pages =      {155--172},
  doi =        {10.1214/074921706000000824},
  file =       {Berthet2006Revisiting.pdf:Berthet2006Revisiting.pdf:PDF},
  mrclass =    {62E17 (60F15 62E20 62G30)},
  mrnumber =   {2387767},
  mrreviewer = {Qiying Wang},
  url =        {http://dx.doi.org/10.1214/074921706000000824}
}

@Article{Berti2014Compatibility,
  Title                    = {Compatibility results for conditional distributions},
  Author                   = {Patrizia Berti and Emanuela Dreassi and Pietro Rigo},
  Journal                  = jma_s,
  Year                     = {2014},
  Pages                    = {190--203},
  Volume                   = {125},

  Doi                      = {10.1016/j.jmva.2013.12.009},
  Fjournal                 = {Journal of Multivariate Analysis},
  ISSN                     = {0047-259X},
  Mrclass                  = {62F15 (62H05)},
  Mrnumber                 = {3163838},
  Url                      = {http://dx.doi.org/10.1016/j.jmva.2013.12.009}
}

@Article{Bertin2008Selection,
  Title                    = {Selection of variables and dimension reduction in
 high-dimensional non-parametric regression},
  Author                   = {Karine Bertin and Guillaume Lecu{\'e}},
  Journal                  = ejs_s,
  Year                     = {2008},
  Pages                    = {1224--1241},
  Volume                   = {2},

  Doi                      = {10.1214/08-EJS327},
  Fjournal                 = {Electronic Journal of Statistics},
  ISSN                     = {1935-7524},
  Mrclass                  = {62G08},
  Mrnumber                 = {2461900 (2010b:62142)},
  Mrreviewer               = {Jan Hannig},
  Url                      = {http://dx.doi.org/10.1214/08-EJS327}
}

@Article{Besag1974Spatial,
  Title                    = {Spatial interaction and the statistical analysis of lattice systems},
  Author                   = {Julian Besag},
  Journal                  = JRSSB_s,
  Year                     = {1974},
  Pages                    = {192--236},

  Owner                    = {mkolar},
  Publisher                = {JSTOR},
  Timestamp                = {2014.05.05}
}

@Article{Besag1975Statistical,
  Title                    = {Statistical Analysis of Non-Lattice Data},
  Author                   = {Julian Besag},
  Journal                  = {J. R. Stat. Soc. D (The Statistician)},
  Year                     = {1975},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {179},
  Volume                   = {24},

  Doi                      = {10.2307/2987782},
  ISSN                     = {0039-0526},
  Owner                    = {mkolar},
  Publisher                = {JSTOR},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.2307/2987782}
}

@Article{Bhamidi2012Energy,
  Title                    = {Energy Landscape for large average submatrix detection problems in Gaussian random matrices},
  Author                   = {Shankar Bhamidi and Partha S. Dey and } # anobel,
  Journal                  = {arXiv preprint arXiv:1211.2284},
  Year                     = {2012},

  Month                    = nov,

  Abstract                 = {The problem of finding large average submatrices of a real-valued matrix arises in the exploratory analysis of data from a variety of disciplines, ranging from genomics to social sciences. In this paper we provide a detailed asymptotic analysis of large average submatrices of an $n \times n$ Gaussian random matrix. The first part of the paper addresses global maxima. For fixed $k$ we identify the average and the joint distribution of the $k \times k$ submatrix having largest average value. As a dual result, we establish that the size of the largest square sub-matrix with average bigger than a fixed positive constant is, with high probability, equal to one of two consecutive integers that depend on the threshold and the matrix dimension $n$. The second part of the paper addresses local maxima. Specifically we consider submatrices with dominant row and column sums that arise as the local optima of iterative search procedures for large average submatrices. For fixed $k$, we identify the limiting average value and joint distribution of a $k \times k$ submatrix conditioned to be a local maxima. In order to understand the density of such local optima and explain the quick convergence of such iterative procedures, we analyze the number $L_n(k)$ of local maxima, beginning with exact asymptotic expressions for the mean and fluctuation behavior of $L_n(k)$. For fixed $k$, the mean of $L_{n}(k)$ is $\Theta(n^{k}/(\log{n})^{(k-1)/2})$ while the standard deviation is $\Theta(n^{2k^2/(k+1)}/(\log{n})^{k^2/(k+1)})$. Our principal result is a Gaussian central limit theorem for $L_n(k)$ that is based on a new variant of Stein's method.},
  Comments                 = {Proofs simplified, 49 pages, 3 figures},
  Eprint                   = {1211.2284},
  Oai2identifier           = {1211.2284},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Billio2012Econometric,
  author =    {Monica Billio and Mila Getmansky and Andrew W. Lo and Loriana Pelizzon},
  title =     {Econometric measures of connectedness and systemic risk in the finance and insurance sectors},
  journal =   {Journal of Financial Economics},
  year =      {2012},
  volume =    {104},
  number =    {3},
  pages =     {535--559},
  month =     {jun},
  doi =       {10.1016/j.jfineco.2011.12.010},
  file =      {:Billio2012Econometric.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier {BV}},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1016/j.jfineco.2011.12.010}
}

@InCollection{birge2001alternative,
  author =    {L. Birg{\'e}},
  title =     {An Alternative Point Of View On {l}epski's Method},
  booktitle = {State of the art in probability and statistics ({L}eiden, 1999)},
  publisher = {Inst. Math. Statist.},
  year =      {2001},
  volume =    {36},
  series =    {IMS Lecture Notes Monogr. Ser.},
  pages =     {113--133},
  address =   {Beachwood, OH},
  doi =       {10.1214/lnms/1215090065},
  file =      {birge2001alternative.pdf:birge2001alternative.pdf:PDF},
  mrnumber =  {1836557 (2002j:62049)},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.1214/lnms/1215090065}
}

@Article{Bissantz2007Non,
  Title                    = {Non-parametric confidence bands in deconvolution density estimation},
  Author                   = {Nicolai Bissantz and Lutz D\"umbgen and Hajo Holzmann and Axel Munk},
  Journal                  = JRSSB_s,
  Year                     = {2007},

  Month                    = {Jun},
  Number                   = {3},
  Pages                    = {483--506},
  Volume                   = {69},

  Doi                      = {10.1111/j.1467-9868.2007.599.x},
  ISSN                     = {1467-9868},
  Owner                    = {mkolar},
  Publisher                = {Wiley-Blackwell},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2007.599.x}
}

@Article{Bodnar08properties,
  Title                    = {Properties of the singular, inverse and generalized inverse partitioned {W}ishart distributions},
  Author                   = {Taras Bodnar and Yarema Okhrin},
  Journal                  = jma_s,
  Year                     = {2008},
  Number                   = {10},
  Pages                    = {2389--2405},
  Volume                   = {99},

  Doi                      = {10.1016/j.jmva.2008.02.024},
  Fjournal                 = {Journal of Multivariate Analysis},
  ISSN                     = {0047-259X},
  Mrclass                  = {62H10 (62E15)},
  Mrnumber                 = {2463397 (2010b:62191)},
  Mrreviewer               = {Shurong Zheng},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1016/j.jmva.2008.02.024}
}

@InCollection{Bogdanov2008complexity,
  author =    {Bogdanov, Andrej and Mossel, Elchanan and Vadhan, Salil},
  title =     {The complexity of distinguishing Markov random fields},
  booktitle = {Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques},
  publisher = {Springer},
  year =      {2008},
  pages =     {331--342}
}

@Article{Bottai2003Confidence,
  author =   {Bottai, Matteo},
  title =    {Confidence regions when the {F}isher information is zero},
  journal =  {Biometrika},
  year =     {2003},
  volume =   {90},
  number =   {1},
  pages =    {73--84},
  coden =    {BIOKAX},
  doi =      {10.1093/biomet/90.1.73},
  file =     {Bottai2003Confidence.pdf:Bottai2003Confidence.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62F25},
  mrnumber = {1966551},
  url =      {http://dx.doi.org/10.1093/biomet/90.1.73}
}

@Article{Bousquet2002Bennett,
  Title                    = {A Bennett concentration inequality and its application to suprema of empirical processes},
  Author                   = {Olivier Bousquet},
  Journal                  = {Comptes Rendus Mathematique},
  Year                     = {2002},

  Month                    = {Jan},
  Number                   = {6},
  Pages                    = {495--500},
  Volume                   = {334},

  Doi                      = {10.1016/s1631-073x(02)02292-6},
  ISSN                     = {1631-073X},
  Owner                    = {mkolar},
  Publisher                = {Elsevier BV},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1016/S1631-073X(02)02292-6}
}

@InCollection{Bousquet2008Tradeoffs,
  author =    {Olivier Bousquet and Bottou, L\'{e}on},
  title =     {The Tradeoffs of Large Scale Learning},
  booktitle = {Advances in Neural Information Processing Systems 20},
  publisher = {Curran Associates, Inc.},
  year =      {2008},
  editor =    {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
  pages =     {161--168},
  url =       {http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf}
}

@Book{boyd04convex,
  Title                    = {Convex Optimization},
  Author                   = {S.~P. Boyd and L. Vandenberghe},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},

  Address                  = {Cambridge},

  ISBN                     = {0-521-83378-7},
  Mrnumber                 = {2061575 (2005d:90002)},
  Owner                    = {mkolar},
  Pages                    = {xiv+716},
  Timestamp                = {2014.02.18}
}

@Article{Bradic2013Support,
  Title                    = {Support recovery via weighted maximum-contrast subagging},
  Author                   = {Jelena Bradic},
  Journal                  = {ArXiv e-prints, arXiv:1306.3494},
  Year                     = {2013},

  Month                    = jun,

  Abstract                 = {In this paper, we consider subagging for non-smooth estimation and model selection in sparse linear regression settings. Proposed weighted maximum-contrast subagging scales with datasets of arbitrary size, yet manages to achieve excellent support recovery. This makes it particularly relevant for computation over complete datasets of extremely large scale, where using traditional methods might be impractical. We develop theory in support of the claim that proposed method has tight error control over both false positives and false negatives, regardless of the size of dataset. Unlike existing methods, it allows for oracle-like properties, even in cases of non-oracle-like properties of aggregated estimators. Moreover, we show limitations of traditional subagging in cases where subsamples are of much smaller order relative to the size of original data. In such situations, it results in discontinuous estimated support set and never recovers sparsity set when at least one of aggregated estimators has probability of support recovery strictly less than 1. Furthermore, we design an adaptive procedure for selecting tuning parameters and optimal weighting scheme. It simultaneously alleviates overall computational burden and relaxes eigenvalue conditions on the design matrix. Finally, we validate our theoretical findings through simulation study and analysis of a part of million-song-challenge dataset.},
  Eprint                   = {1306.3494},
  Oai2identifier           = {1306.3494}
}

@Article{Bradic2011Penalized,
  Title                    = {Penalized composite quasi-likelihood for ultrahigh dimensional
 variable selection},
  Author                   = {Jelena Bradic and } # jfan # { and Weiwei Wang},
  Journal                  = JRSSB_s,
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {325--349},
  Volume                   = {73},

  Doi                      = {10.1111/j.1467-9868.2010.00764.x},
  Fjournal                 = {Journal of the Royal Statistical Society. Series B.
 Statistical Methodology},
  ISSN                     = {1369-7412},
  Mrclass                  = {62J07 (62F35 62J05)},
  Mrnumber                 = {2815779 (2012h:62267)},
  Mrreviewer               = {Juan C. Ag{\"u}ero},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2010.00764.x}
}

@Article{Braverman2015Communication,
  author =         {Mark Braverman and Ankit Garg and Tengyu Ma and Huy L. Nguyen and David P. Woodruff},
  title =          {Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality},
  journal =        {ArXiv e-prints, arXiv:1506.07216},
  year =           {2015},
  month =          jun,
  abstract =       {We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the $m$ machines receives $n$ data points from a $d$-dimensional Gaussian distribution with unknown mean $\theta$ which is promised to be $k$-sparse. The machines communicate by message passing and aim to estimate the mean $\theta$. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed \textit{sparse linear regression} problem: to achieve the statistical minimax error, the total communication is at least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that each machine receives and $d$ is the ambient dimension. These lower results improve upon [Sha14,SD'14] by allowing multi-round iterative communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation. As our main technique, we prove a \textit{distributed data processing inequality}, as a generalization of usual data processing inequalities, which might be of independent interest and useful for other problems.},
  comments =       {To appear at STOC 2016. Fixed typos in theorem 4.5 and incorporated reviewers' suggestions},
  eprint =         {1506.07216},
  file =           {:Braverman2015Communication.pdf:PDF},
  oai2identifier = {1506.07216}
}

@Article{Breheny2011Coordinate,
  Title                    = {Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection},
  Author                   = {Breheny, Patrick and Huang, Jian},
  Journal                  = aoas_s,
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {232--253},
  Volume                   = {5},

  Doi                      = {10.1214/10-AOAS388},
  Owner                    = {mkolar},
  Publisher                = {NIH Public Access},
  Timestamp                = {2013.10.16},
  Url                      = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1300715189}
}

@Article{breiman96heuristics,
  Title                    = {Heuristics Of Instability And Stabilization In Model Selection},
  Author                   = {L. Breiman},
  Journal                  = aos_s,
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {2350--2383},
  Volume                   = {24},

  Doi                      = {10.1214/aos/1032181158},
  ISSN                     = {0090-5364},
  Mrnumber                 = {1425957 (97j:62093)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/aos/1032181158}
}

@InProceedings{Bresler2015Efficiently,
  author =       {Bresler, Guy},
  title =        {Efficiently learning Ising models on arbitrary graphs},
  booktitle =    {Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing},
  year =         {2015},
  pages =        {771--782},
  organization = {ACM}
}

@Article{bresler07reconstruction,
  Title                    = {Reconstruction Of {m}arkov {r}andom {f}ields From {s}amples: {s}ome {o}bservations And {a}lgorithms},
  Author                   = {G. Bresler and E. Mossel and A. Sly},
  Journal                  = {SIAM J. Comput.},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {563--578},
  Volume                   = {42},

  Doi                      = {10.1137/100796029},
  ISSN                     = {0097-5397},
  Mrnumber                 = {3037003},
  Newspaper                = {SIAM J. Comput.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1137/100796029}
}

@Article{Bretagnolle1989Hungarian,
  author =   {Bretagnolle, J. and Massart, P.},
  title =    {Hungarian constructions from the nonasymptotic viewpoint},
  journal =  {Ann. Probab.},
  year =     {1989},
  volume =   {17},
  number =   {1},
  pages =    {239--256},
  coden =    {APBYAE},
  file =     {Bretagnolle1989Hungarian.pdf:Bretagnolle1989Hungarian.pdf:PDF},
  fjournal = {The Annals of Probability},
  issn =     {0091-1798},
  mrclass =  {60F17 (62G30)},
  mrnumber = {972783},
  url =      {http://links.jstor.org/sici?sici=0091-1798(198901)17:1<239:HCFTNV>2.0.CO;2-B&origin=MSN}
}

@InProceedings{Broderick2013Streaming,
  Title                    = {Streaming Variational Bayes},
  Author                   = {Tamara Broderick and Nicholas Boyd and Andre Wibisono and Ashia C Wilson and } # mjordan,
  Booktitle                = NIPS_s,
  Year                     = {2013},
  Editor                   = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  Pages                    = {1727--1735}
}

@Article{brucker1984n,
  Title                    = {An {$O(n)$} Algorithm For Quadratic Knapsack Problems},
  Author                   = {P. Brucker},
  Journal                  = {Oper. Res. Lett.},
  Year                     = {1984},
  Number                   = {3},
  Pages                    = {163--166},
  Volume                   = {3},

  Doi                      = {10.1016/0167-6377(84)90010-5},
  ISSN                     = {0167-6377},
  Mrnumber                 = {761510},
  Newspaper                = {Oper. Res. Lett.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1016/0167-6377(84)90010-5}
}

@Article{bunea08honest,
  Title                    = {Honest Variable Selection In Linear And Logistic Regression Models Via $\ell_1$ And $\ell_1+\ell_2$ Penalization},
  Author                   = {F. Bunea},
  Journal                  = ejs_s,
  Year                     = {2008},
  Pages                    = {1153--1194},
  Volume                   = {2},

  Doi                      = {10.1214/08-EJS287},
  ISSN                     = {1935-7524},
  Mrnumber                 = {2461898 (2010b:62143)},
  Newspaper                = {Electron. J. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/08-EJS287}
}

@Article{Butucea2013Detection,
  Title                    = {Detection of a sparse submatrix of a high-dimensional noisy matrix},
  Author                   = {Cristina Butucea and } # yingster,
  Journal                  = {Bernoulli},
  Year                     = {2013},
  Number                   = {5B},
  Pages                    = {2652--2688},
  Volume                   = {19},

  Owner                    = {mkolar},
  Publisher                = {Bernoulli Society for Mathematical Statistics and Probability},
  Timestamp                = {2014.02.18},
  Url                      = {http://projecteuclid.org/euclid.bj/1386078616}
}

@Article{Butucea2013Sharp,
  Title                    = {Sharp Variable Selection of a Sparse Submatrix in a High-Dimensional Noisy Matrix},
  Author                   = {Cristina Butucea and } # yingster # { and Irina Suslina},
  Journal                  = {arXiv preprint arXiv;1303.5647},
  Year                     = {2013},

  Month                    = mar,

  Abstract                 = {We observe a $N\times M$ matrix of independent, identically distributed Gaussian random variables which are centered except for elements of some submatrix of size $n\times m$ where the mean is larger than some $a>0$. The submatrix is sparse in the sense that $n/N$ and $m/M$ tend to 0, whereas $n,\, m, \, N$ and $M$ tend to infinity. We consider the problem of selecting the random variables with significantly large mean values. We give sufficient conditions on $a$ as a function of $n,\, m,\,N$ and $M$ and construct a uniformly consistent procedure in order to do sharp variable selection. We also prove the minimax lower bounds under necessary conditions which are complementary to the previous conditions. The critical values $a^*$ separating the necessary and sufficient conditions are sharp (we show exact constants). We note a gap between the critical values $a^*$ for selection of variables and that of detecting that such a submatrix exists given by Butucea and Ingster (2012). When $a^*$ is in this gap, consistent detection is possible but no consistent selector of the corresponding variables can be found.},
  Eprint                   = {1303.5647},
  Oai2identifier           = {1303.5647},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Cai2016Joint,
  author =    {T. Tony Cai and Hongzhe Li and Weidong Liu and Jichun Xie},
  title =     {Joint estimation of multiple high-dimensional precision matrices},
  journal =   statsin_s,
  year =      {2016},
  doi =       {10.5705/ss.2014.256},
  file =      {:Cai2016Joint.pdf:PDF},
  publisher = {Institute of Statistical Science},
  url =       {http://dx.doi.org/10.5705/ss.2014.256}
}

@Article{Callaert1978Berry,
  Title                    = {The {B}erry-{E}sseen theorem for {$U$}-statistics},
  Author                   = {Herman Callaert and Paul Janssen},
  Journal                  = aos_s,
  Year                     = {1978},
  Number                   = {2},
  Pages                    = {417--421},
  Volume                   = {6},

  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {60F05},
  Mrnumber                 = {0464359 (57 \#4290)},
  Mrreviewer               = {W. Hoeffding}
}

@Article{Canu2006Kernel,
  Title                    = {Kernel methods and the exponential family},
  Author                   = {St\'{e}phane Canu and Alex Smola},
  Journal                  = {Neurocomputing},
  Year                     = {2006},

  Month                    = {Mar},
  Number                   = {7-9},
  Pages                    = {714--720},
  Volume                   = {69},

  Doi                      = {10.1016/j.neucom.2005.12.009},
  ISSN                     = {0925-2312},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.neucom.2005.12.009}
}

@Article{Caponnetto2007Optimal,
  author =   {Caponnetto, A. and De Vito, E.},
  title =    {Optimal rates for the regularized least-squares algorithm},
  journal =  {Found. Comput. Math.},
  year =     {2007},
  volume =   {7},
  number =   {3},
  pages =    {331--368},
  doi =      {10.1007/s10208-006-0196-8},
  file =     {:Caponnetto2007Optimal.pdf:PDF},
  fjournal = {Foundations of Computational Mathematics. The Journal of the Society for the Foundations of Computational Mathematics},
  issn =     {1615-3375},
  mrclass =  {68T05 (62J02 65K10 93E24)},
  mrnumber = {2335249},
  url =      {http://dx.doi.org/10.1007/s10208-006-0196-8}
}

@InProceedings{Caron2002method,
  Title                    = {A method for detecting artificial objects in natural environments},
  Author                   = {Yves Caron and Pascal Makris and Nicole Vincent},
  Booktitle                = PROC # { 16th Int. Conf. Pattern Recogn.},
  Year                     = {2002},
  Organization             = {IEEE},
  Pages                    = {600--603},
  Volume                   = {1},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1044812}
}

@Article{carroll1988effect,
  Title                    = {The Effect Of Estimating Weights In Weighted Least Squares},
  Author                   = {R.~J. Carroll and C.~F.~J. Wu and D. Ruppert},
  Journal                  = jasa_s,
  Year                     = {1988},
  Number                   = {404},
  Pages                    = {1045--1054},
  Volume                   = {83},

  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Publisher                = {Taylor \& Francis Group},
  Timestamp                = {2014.02.18}
}

@Article{caruana1997multitask,
  author =    {R. Caruana},
  title =     {Multitask Learning},
  journal =   {Mach. Learn.},
  year =      {1997},
  volume =    {28},
  number =    {1},
  pages =     {41--75},
  file =      {:caruana1997multitask.pdf:PDF},
  newspaper = {Mach. Learn.},
  owner =     {mkolar},
  publisher = {Springer},
  timestamp = {2014.02.18}
}

@Article{Carvalho2007Dynamic,
  author =    {Carlos M. Carvalho and Mike West},
  title =     {Dynamic matrix-variate graphical models},
  journal =   {Bayesian Anal.},
  year =      {2007},
  volume =    {2},
  number =    {1},
  pages =     {69--97},
  month =     {mar},
  doi =       {10.1214/07-ba204},
  file =      {Carvalho2007Dynamic.pdf:Carvalho2007Dynamic.pdf:PDF},
  owner =     {mkolar},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1214/07-BA204}
}

@Article{Castro2014Detection,
  author   = {Castro, Rui M. and Lugosi, G{\'a}bor and Savalle, Pierre-Andr{\'e}},
  title    = {Detection of correlations with adaptive sensing},
  journal  = IEEEit_s,
  year     = {2014},
  volume   = {60},
  number   = {12},
  pages    = {7913--7927},
  issn     = {0018-9448},
  doi      = {10.1109/TIT.2014.2364713},
  file     = {:Castro2014Detection.pdf:PDF},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass  = {94A12},
  mrnumber = {3285754},
  url      = {http://dx.doi.org/10.1109/TIT.2014.2364713},
}

@Article{Catoni2012Challenging,
  author =     {Catoni, Olivier},
  title =      {Challenging the empirical mean and empirical variance: a deviation study},
  journal =    {Ann. Inst. Henri Poincar\'e Probab. Stat.},
  year =       {2012},
  volume =     {48},
  number =     {4},
  pages =      {1148--1185},
  doi =        {10.1214/11-AIHP454},
  file =       {Catoni2012Challenging.pdf:Catoni2012Challenging.pdf:PDF},
  fjournal =   {Annales de l'Institut Henri Poincar\'e Probabilit\'es et Statistiques},
  issn =       {0246-0203},
  mrclass =    {62G05 (62G35)},
  mrnumber =   {3052407},
  mrreviewer = {Fernanda O. Figueiredo},
  url =        {http://dx.doi.org/10.1214/11-AIHP454}
}

@Article{Chandrasekaran2012Latent,
  author =     {Chandrasekaran, Venkat and Parrilo, Pablo A. and Willsky, Alan S.},
  title =      {Latent variable graphical model selection via convex optimization},
  journal =    {Ann. Statist.},
  year =       {2012},
  volume =     {40},
  number =     {4},
  pages =      {1935--1967},
  doi =        {10.1214/11-AOS949},
  file =       {:Chandrasekaran2012Latent.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62F30 (62F12 62H12 62H99)},
  mrnumber =   {3059067},
  mrreviewer = {Sanjay Chaudhuri},
  url =        {http://dx.doi.org/10.1214/11-AOS949}
}

@Article{Chang2014Cramer,
  author =         {Jinyuan Chang and Qi-Man Shao and Wen-Xin Zhou},
  title =          {Cram\'er type moderate deviations for two-sample Studentized $U$-statistics with applications},
  journal =        {ArXiv e-prints, arXiv:1407.4546},
  year =           {2014},
  month =          jul,
  abstract =       {Two-sample $U$-statistics are widely used in a broad range of applications, including those in the fields of biostatistics and econometrics. In this paper, we establish sharp Cram\'er type moderate deviation theorems for Studentized two-sample $U$-statistics in a general framework, including the two-sample $t$-statistic and Studentized Mann-Whitney test statistic as prototypical examples. In particular, a refined moderate deviation theorem with second-order accuracy is established for the two-sample $t$-statistic. These results extend the applicability of the existing statistical methodologies from the one-sample $t$-statistic to more general nonlinear statistics. Applications to two-sample large-scale multiple testing problems with false discovery rate control and the regularized bootstrap method are also discussed.},
  eprint =         {1407.4546},
  file =           {:Chang2014Cramer.pdf:PDF},
  oai2identifier = {1407.4546}
}

@Article{Chang2014Simulation,
  author =         {Jinyuan Chang and Wen Zhou and Wen-Xin Zhou},
  title =          {Simulation-Based Hypothesis Testing of High Dimensional Means Under Covariance Heterogeneity},
  journal =        {ArXiv e-prints, arXiv:1406.1939},
  year =           {2014},
  month =          jun,
  abstract =       {In this paper, we study testing the population mean vector of high dimensional multivariate data for both one-sample and two-sample problems. The proposed simulation-based testing procedures employ maximum-type statistics and use the Gaussian approximation techniques to obtain corresponding critical values. Different from peer tests that heavily rely on the structural conditions on the unknown covariance matrices, the proposed tests allow very general forms of the covariance structures of data and therefore enjoy wide scope of applicability in practice. To enhance powers of the tests against sparse alternatives, we further propose two-step procedures with a preliminary feature screening step. Theoretical properties of the proposed tests are investigated. Extensive numerical experiments on synthetic datasets and empirical applications on identifying diseases-associated gene-sets are provided to support the theoretical results. The proposed tests are easily implemented and computationally efficient in practice.},
  comments =       {34 pages, 10 figures},
  eprint =         {1406.1939},
  file =           {:Chang2014Simulation.pdf:PDF},
  oai2identifier = {1406.1939}
}

@Article{Chang2015Bootstrap,
  author =         {Jinyuan Chang and Wen Zhou and Wen-Xin Zhou},
  title =          {Bootstrap Tests on High Dimensional Covariance Matrices with Applications to Understanding Gene Clustering},
  journal =        {ArXiv e-prints, arXiv:1505.04493},
  year =           {2015},
  month =          may,
  abstract =       {Recent advancements in genomic study and clinical research have drew growing attention to understanding how relationships among genes, such as dependencies or co-regulations, vary between different biological states. Complex and unknown dependency among genes, along with the large number of measurements, imposes methodological challenge in studying genes relationships between different states. Starting from an interrelated problem, we propose a bootstrap procedure for testing the equality of two unspecified covariance matrices in high dimensions, which turns out to be an important tool in understanding the change of gene relationships between states. The two-sample bootstrap test takes maximum advantage of the dependence structures given the data, and gives rise to powerful tests with desirable size in finite samples. The theoretical and numerical studies show that the bootstrap test is powerful against sparse alternatives and more importantly, it is robust against highly correlated and nonparametric sampling distributions. Encouraged by the wide applicability of the proposed bootstrap test, we design a gene clustering algorithm to understand gene clustering structures. We apply the bootstrap test and gene clustering algorithm to the analysis of a human asthma dataset, for which some interesting biological implications are discussed.},
  eprint =         {1505.04493},
  file =           {:Chang2015Bootstrap.pdf:PDF},
  oai2identifier = {1505.04493}
}

@InProceedings{Chapelle2010Multi,
  author =       {Chapelle, Olivier and Shivaswamy, Pannagadatta and Vadrevu, Srinivas and Weinberger, Kilian and Zhang, Ya and Tseng, Belle},
  title =        {Multi-task learning for boosting with application to web search ranking},
  booktitle =    {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year =         {2010},
  pages =        {1189--1198},
  organization = {ACM},
  file =         {:Chapelle2010Multi.pdf:PDF}
}

@Article{Chatterjee2011Bootstrapping,
  author =   {A. Chatterjee and S. N. Lahiri},
  title =    {Bootstrapping lasso estimators},
  journal =  jasa_s,
  year =     {2011},
  volume =   {106},
  number =   {494},
  pages =    {608--625},
  coden =    {JSTNAL},
  doi =      {10.1198/jasa.2011.tm10159},
  file =     {Chatterjee2011Bootstrapping.pdf:Chatterjee2011Bootstrapping.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {62J07 (60B10 62F12 62F25 62F35 62F40 62J10 62P10)},
  mrnumber = {2847974 (2012i:62199)},
  url =      {http://dx.doi.org/10.1198/jasa.2011.tm10159}
}

@Article{Chatterjee2013Rates,
  Title                    = {Rates of convergence of the adaptive {LASSO} estimators to the
 oracle distribution and higher order refinements by the
 bootstrap},
  Author                   = {A.~Chatterjee and S.~N. Lahiri},
  Journal                  = aos_s,
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {1232--1259},
  Volume                   = {41},

  Doi                      = {10.1214/13-AOS1106},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J07 (62E20 62G09)},
  Mrnumber                 = {3113809},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1106}
}

@Article{Chaudhuri2007Estimation,
  author =   {Chaudhuri, Sanjay and Drton, Mathias and Richardson, Thomas S.},
  title =    {Estimation of a covariance matrix with zeros},
  journal =  {Biometrika},
  year =     {2007},
  volume =   {94},
  number =   {1},
  pages =    {199--216},
  coden =    {BIOKAX},
  doi =      {10.1093/biomet/asm007},
  file =     {:Chaudhuri2007Estimation.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62J10 (62F10 62P10)},
  mrnumber = {2307904},
  url =      {http://dx.doi.org/10.1093/biomet/asm007}
}

@Article{Chen2015Extended,
  author =         {Caihua Chen and Min Li and Xin Liu and Yinyu Ye},
  title =          {Extended ADMM and BCD for Nonseparable Convex Minimization Models with Quadratic Coupling Terms: Convergence Analysis and Insights},
  journal =        {ArXiv e-prints, arXiv:1508.00193},
  year =           {2015},
  month =          aug,
  abstract =       {In this paper, we establish the convergence of the alternating direction method of multipliers (ADMM) and block coordinate descent (BCD) for large-scale nonseparable minimization models with quadratic coupling terms. The novel convergence results presented in this paper answer several open questions that have been the subject of considerable discussion. We firstly extend the 2-block ADMM to linearly constrained convex optimization with a coupled quadratic objective function, an area where theoretical understanding is currently lacking, and prove that the sequence generated by the ADMM converges in point-wise manner to a primal-dual solution pair. Moreover, we apply randomly permuted ADMM (RPADMM) to nonseparable multi-block convex optimization, and prove its expected convergence for a class of nonseparable quadratic programming problems. When the linear constraint vanishes, the 2-block ADMM and RPADMM reduce to the 2-block cyclic BCD method and randomly permuted BCD (RPBCD). Our study provides the first iterate convergence result for 2-block cyclic BCD without assuming the boundedness of the iterates. Under the same setting, the sublinear convergence rate of the function values can also be verified. We also theoretically establish the expected iterate convergence result concerning multi-block RPBCD for convex quadratic optimization. In addition, we demonstrate that RPBCD may have a worse convergence rate than cyclic BCD for 2-block convex quadratic minimization problems. Although the results on RPADMM and RPBCD are restricted to quadratic minimization models, they provide some interesting insights: 1) random permutation makes ADMM and BCD more robust for multi-block convex minimization problems; 2) cyclic BCD may outperform RPBCD for "nice" problems, and therefore RPBCD should be applied with caution when solving general convex optimization problems.},
  eprint =         {1508.00193},
  file =           {:Chen2015Extended.pdf:PDF},
  oai2identifier = {1508.00193}
}

@Article{Chen2010Compatibility,
  Title                    = {Compatibility of conditionally specified models},
  Author                   = {Hua Yun Chen},
  Journal                  = statprobl_s,
  Year                     = {2010},
  Number                   = {7-8},
  Pages                    = {670--677},
  Volume                   = {80},

  Coden                    = {SPLTDC},
  Doi                      = {10.1016/j.spl.2009.12.025},
  Fjournal                 = {Statistics \& Probability Letters},
  ISSN                     = {0167-7152},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2595145},
  Url                      = {http://dx.doi.org/10.1016/j.spl.2009.12.025}
}

@Article{Chen08extended,
  Title                    = {Extended {b}ayesian Information Criteria For Model Selection With Large Model Spaces},
  Author                   = {J. Chen and Z. Chen},
  Journal                  = {Biometrika},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {759--771},
  Volume                   = {95},

  Doi                      = {10.1093/biomet/asn034},
  ISSN                     = {0006-3444},
  Mrnumber                 = {2443189},
  Newspaper                = {Biometrika},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1093/biomet/asn034}
}

@Article{Chen2012Learning,
  author =    {Chen, Jianhui and Liu, Ji and Ye, Jieping},
  title =     {Learning incoherent sparse and low-rank patterns from multiple tasks},
  journal =   {ACM Transactions on Knowledge Discovery from Data (TKDD)},
  year =      {2012},
  volume =    {5},
  number =    {4},
  pages =     {22},
  file =      {:Chen2012Learning.pdf:PDF},
  publisher = {ACM}
}

@InProceedings{Chen2009convex,
  author =       {Chen, Jianhui and Tang, Lei and Liu, Jun and Ye, Jieping},
  title =        {A convex formulation for learning shared structures from multiple tasks},
  booktitle =    {Proceedings of the 26th Annual International Conference on Machine Learning},
  year =         {2009},
  pages =        {137--144},
  organization = {ACM},
  file =         {:Chen2009convex.pdf:PDF}
}

@InProceedings{Chen2011Integrating,
  author =       {Chen, Jianhui and Zhou, Jiayu and Ye, Jieping},
  title =        {Integrating low-rank and group-sparse structures for robust multi-task learning},
  booktitle =    {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year =         {2011},
  pages =        {42--50},
  organization = {ACM},
  file =         {:Chen2011Integrating.pdf:PDF}
}

@Article{chen2009conditional,
  Title                    = {Conditional Variance Estimation In Heteroscedastic Regression Models},
  Author                   = {L.-H. Chen and M.-Y. Cheng and L. Peng},
  Journal                  = jspi_s,
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {236--245},
  Volume                   = {139},

  Doi                      = {10.1016/j.jspi.2008.04.020},
  ISSN                     = {0378-3758},
  Mrnumber                 = {2474001 (2010c:60115)},
  Newspaper                = {J. Statist. Plann. Inference},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1016/j.jspi.2008.04.020}
}

@Article{Chen2013Asymptotically,
  author =  {Mengjie Chen and Zhao Ren and Hongyu Zhao and } # hzhou,
  title =   {Asymptotically Normal and Efficient Estimation of Covariate-Adjusted Gaussian Graphical Model},
  journal = {Journal of the American Statistical Association},
  year =    {2015},
  volume =  {0},
  number =  {ja},
  pages =   {00-00},
  doi =     {10.1080/01621459.2015.1010039},
  eprint =  { http://dx.doi.org/10.1080/01621459.2015.1010039 },
  file =    {:Chen2013Asymptotically.pdf:PDF},
  url =     { 
        http://dx.doi.org/10.1080/01621459.2015.1010039
    
}
}

@Article{Chen2013Selection,
  author    = {Shizhe Chen and } # dwitten #{ and Ali Shojaie},
  title     = {Selection and Estimation for Mixed Graphical Models},
  journal   = {Biometrika},
  year      = {2015},
  volume    = {102},
  number    = {1},
  pages     = {47--64},
  issn      = {0006-3444},
  doi       = {10.1093/biomet/asu051},
  file      = {:Chen2013Selection.pdf:PDF},
  fjournal  = {Biometrika},
  mrclass   = {62H12 (62F12 62H15)},
  mrnumber  = {3335095},
  timestamp = {2018.05.01},
  url       = {https://doi.org/10.1093/biomet/asu051},
}

@Article{chen1999atomic,
  Title                    = {Atomic Decomposition By Basis Pursuit},
  Author                   = {S.~S. Chen and } # ddonoho # { and M.~A. Saunders},
  Journal                  = {SIAM J. Sci. Comput.},
  Year                     = {1998},
  Number                   = {1},
  Pages                    = {33--61},
  Volume                   = {20},

  Doi                      = {10.1137/S1064827596304010},
  ISSN                     = {1064-8275},
  Mrnumber                 = {1639094 (99h:94013)},
  Newspaper                = {SIAM J. Sci. Comput.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1137/S1064827596304010}
}

@Article{Chen2013Covariance,
  Title                    = {Covariance and precision matrix estimation for high-dimensional time series},
  Author                   = {Xiaohui Chen and Mengyu Xu and } # wbwu,
  Journal                  = aos_s,
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {2994-3021},
  Volume                   = {41},

  Doi                      = {10.1214/13-AOS1182},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.02.17},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1182}
}

@Article{Chen2015Adaptive,
  author =    {Chen, Yixin and Wang, Qin and Yao, Weixin},
  title =     {Adaptive estimation for varying coefficient models},
  journal =   jma_s,
  year =      {2015},
  volume =    {137},
  pages =     {17--31},
  file =      {:Chen2015Adaptive.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier},
  timestamp = {2016.02.08}
}

@Article{Cheng2015Computational,
  author =         {Guang Cheng and Zuofeng Shang},
  title =          {Computational Limits of Divide-and-Conquer Method},
  journal =        {ArXiv e-prints, arXiv:1512.09226},
  year =           {2015},
  month =          dec,
  abstract =       {This theoretical note explores statistical versus computational trade-off to address a basic question in the application of divide-and-conquer method: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds capture intrinsic computational limits of divide-and-conquer method, which turn out to be fully determined by the smoothness of the regression function. As a side remark, we argue that sample spitting may be viewed as a new form of regularization, playing a similar role as smoothing parameter.},
  comments =       {37 pages},
  eprint =         {1512.09226},
  file =           {:Cheng2015Computational.pdf:PDF},
  oai2identifier = {1512.09226}
}

@Article{Cheng2013High,
  Title                    = {High-dimensional Mixed Graphical Models},
  Author                   = {Jie Cheng and } # elevina # { and Ji Zhu},
  Journal                  = {ArXiv e-prints, arXiv:1304.2810},
  Year                     = {2013},

  Month                    = apr,

  Abstract                 = {While graphical models for continuous data (Gaussian graphical models) and discrete data (Ising models) have been extensively studied, there is little work on graphical models linking both continuous and discrete variables (mixed data), which are common in many scientific applications. We propose a novel graphical model for mixed data, which is simple enough to be suitable for high-dimensional data, yet flexible enough to represent all possible graph structures. We develop a computationally efficient regression-based algorithm for fitting the model by focusing on the conditional log-likelihood of each variable given the rest. The parameters have a natural group structure, and sparsity in the fitted graph is attained by incorporating a group lasso penalty, approximated by a weighted $\ell_1$ penalty for computational efficiency. We demonstrate the effectiveness of our method through an extensive simulation study and apply it to a music annotation data set (CAL500), obtaining a sparse and interpretable graphical model relating the continuous features of the audio signal to categorical variables such as genre, emotions, and usage associated with particular songs. While we focus on binary discrete variables, we also show that the proposed methodology can be easily extended to general discrete variables.},
  Comments                 = {21 pages, 6 figures},
  Eprint                   = {1304.2810},
  Oai2identifier           = {1304.2810},
  Owner                    = {mkolar},
  Timestamp                = {2014.05.05}
}

@Article{Cheng2012Sparse,
  author =         {Jie Cheng and Elizaveta Levina and Pei Wang and Ji Zhu},
  title =          {Sparse Ising Models with Covariates},
  journal =        {ArXiv e-prints, arXiv:1209.6342},
  year =           {2012},
  month =          sep,
  abstract =       {There has been a lot of work fitting Ising models to multivariate binary data in order to understand the conditional dependency relationships between the variables. However, additional covariates are frequently recorded together with the binary data, and may influence the dependence relationships. Motivated by such a dataset on genomic instability collected from tumor samples of several types, we propose a sparse covariate dependent Ising model to study both the conditional dependency within the binary data and its relationship with the additional covariates. This results in subject-specific Ising models, where the subject's covariates influence the strength of association between the genes. As in all exploratory data analysis, interpretability of results is important, and we use L1 penalties to induce sparsity in the fitted graphs and in the number of selected covariates. Two algorithms to fit the model are proposed and compared on a set of simulated data, and asymptotic results are established. The results on the tumor dataset and their biological significance are discussed in detail.},
  comments =       {32 pages (including 5 pages of appendix), 3 figures, 2 tables},
  eprint =         {1209.6342},
  file =           {:Cheng2012Sparse.pdf:PDF},
  oai2identifier = {1209.6342}
}

@Article{Chernozhukov2013Gaussian,
  author =   {Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  title =    {Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors},
  journal =  aos_s,
  year =     {2013},
  volume =   {41},
  number =   {6},
  pages =    {2786--2819},
  doi =      {10.1214/13-AOS1161},
  file =     {:Chernozhukov2013Gaussian.pdf:PDF},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62E17 (62F40)},
  mrnumber = {3161448},
  url =      {http://dx.doi.org/10.1214/13-AOS1161}
}

@Article{Chernozhukov2014Anti,
  author =    {Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  title =     {Anti-concentration and honest, adaptive confidence bands},
  journal =   aos_s,
  year =      {2014},
  volume =    {42},
  number =    {5},
  pages =     {1787--1818},
  month =     {Oct},
  doi =       {10.1214/14-aos1235},
  file =      {:Chernozhukov2014Anti.pdf:PDF},
  issn =      {0090-5364},
  owner =     {mkolar},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.11.25},
  url =       {http://dx.doi.org/10.1214/14-AOS1235}
}

@Article{Chernozhukov2014Gaussian,
  author =    {Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  title =     {{Gauss}ian approximation of suprema of empirical processes},
  journal =   aos_s,
  year =      {2014},
  volume =    {42},
  number =    {4},
  pages =     {1564--1597},
  month =     {Aug},
  doi =       {10.1214/14-aos1230},
  file =      {:Chernozhukov2014Gaussian.pdf:PDF},
  issn =      {0090-5364},
  owner =     {mkolar},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.11.25},
  url =       {http://dx.doi.org/10.1214/14-AOS1230}
}

@Article{Chernozhukov2015Empirical,
  author =         {Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  title =          {Empirical and multiplier bootstraps for suprema of empirical processes of increasing complexity, and related Gaussian couplings},
  journal =        {ArXiv e-prints, arXiv:1502.00352},
  year =           {2015},
  month =          feb,
  abstract =       {We derive strong approximations to the supremum of the non-centered empirical process indexed by a possibly unbounded VC-type class of functions by the suprema of the Gaussian and bootstrap processes. The bounds of these approximations are non-asymptotic, which allows us to work with classes of functions whose complexity increases with the sample size. The construction of couplings is not of the Hungarian type and is instead based on the Slepian-Stein methods and Gaussian comparison inequalities. The increasing complexity of classes of functions and non-centrality of the processes make the results useful for applications in modern nonparametric statistics (Gin\'{e} and Nickl, 2015), in particular allowing us to study the power properties of nonparametric tests using Gaussian and bootstrap approximations.},
  eprint =         {1502.00352},
  file =           {:Chernozhukov2015Empirical.pdf:PDF},
  oai2identifier = {1502.00352},
  owner =          {mkolar},
  timestamp =      {2016.04.04}
}

@Article{Chiappori2015Nonparametric,
  author     = {Chiappori, Pierre-Andr{\'e} and Komunjer, Ivana and Kristensen, Dennis},
  title      = {Nonparametric identification and estimation of transformation models},
  journal    = {J. Econometrics},
  year       = {2015},
  volume     = {188},
  number     = {1},
  pages      = {22--39},
  issn       = {0304-4076},
  doi        = {10.1016/j.jeconom.2015.01.001},
  file       = {:Chiappori2015Nonparametric.pdf:PDF},
  fjournal   = {Journal of Econometrics},
  mrclass    = {62G05 (62G08)},
  mrnumber   = {3371659},
  mrreviewer = {Dimitris A. Ioannides},
  url        = {http://dx.doi.org/10.1016/j.jeconom.2015.01.001},
}

@InCollection{Chickering96learningbayesian,
  Title                    = {Learning Bayesian Networks Is Np-complete},
  Author                   = {D.~M. Chickering},
  Booktitle                = {Learning from Data},
  Publisher                = {Springer New York},
  Year                     = {1996},
  Editor                   = {Doug Fisher and Hans~J. Lenz},
  Pages                    = {121-130},
  Series                   = {Lecture Notes in Statistics},
  Volume                   = {112},

  Doi                      = {10.1007/978-1-4612-2404-4_12},
  ISBN                     = {978-0-387-94736-5},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/978-1-4612-2404-4_12}
}

@Article{Chiquet2011Inferring,
  Title                    = {Inferring Multiple Graphical Structures},
  Author                   = {J. Chiquet and Y. Grandvalet and C. Ambroise},
  Journal                  = {Stat. Comput.},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {537--553},
  Volume                   = {21},

  Newspaper                = {Stat. Comput.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{chow68approximating,
  author =    {C.~K. Chow and C.~N. Liu},
  title =     {Approximating Discrete Probability Distributions With Dependence Trees},
  journal =   IEEEit_s,
  year =      {1968},
  volume =    {14},
  number =    {3},
  pages =     {462--467},
  newspaper = {IEEE Trans. Inf. Theory},
  owner =     {mkolar},
  publisher = {IEEE},
  timestamp = {2014.02.18}
}

@Article{Chun2013Joint,
  author =    {Hyonho Chun and Min Chen and Bing Li and Hongyu Zhao},
  title =     {Joint conditional {Gauss}ian graphical models with multiple sources of genomic data},
  journal =   {Frontiers in Genetics},
  year =      {2013},
  volume =    {4},
  doi =       {10.3389/fgene.2013.00294},
  file =      {Chun2013Joint.pdf:Chun2013Joint.pdf:PDF},
  owner =     {mkolar},
  publisher = {Frontiers Media SA},
  timestamp = {2014.02.13},
  url =       {http://dx.doi.org/10.3389/fgene.2013.00294}
}

@Article{Claeskens2003Bootstrap,
  Title                    = {Bootstrap confidence bands for regression curves and their derivatives},
  Author                   = {Gerda Claeskens and Ingrid {{V}an {K}eilegom}},
  Journal                  = aos_s,
  Year                     = {2003},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {1852--1884},
  Volume                   = {31},

  Doi                      = {10.1214/aos/1074290329},
  ISSN                     = {0090-5364},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.11.25},
  Url                      = {http://dx.doi.org/10.1214/aos/1074290329}
}

@Article{Clemmensen2011Sparse,
  Title                    = {Sparse discriminant analysis},
  Author                   = {Line Clemmensen and } # thastie # { and } # dwitten # { and Bjarne Ersb{\o}ll},
  Journal                  = {Technometrics},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {406--413},
  Volume                   = {53},

  Coden                    = {TCMTA2},
  Doi                      = {10.1198/TECH.2011.08118},
  Fjournal                 = {Technometrics. A Journal of Statistics for the Physical, Chemical and Engineering Sciences},
  ISSN                     = {0040-1706},
  Mrclass                  = {62H30 (62P10)},
  Mrnumber                 = {2850472 (2012m:62171)},
  Mrreviewer               = {Santiago Velilla},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1198/TECH.2011.08118}
}

@InProceedings{Cleveland91local,
  Title                    = {Local Regression Models},
  Author                   = {W.~S. Cleveland and E. Grosse and W.~M. Shyu},
  Booktitle                = {Statistical Models in S},
  Year                     = {1991},
  Editor                   = {J. M. Chambers and Trevor J. Hastie},
  Pages                    = {309--376},

  Bibsource                = {ftp://ftp.math.utah.edu/pub/bibnet/authors/g/grosse-eric.bib},
  Lccn                     = {QA276.4 .S65 1991},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InCollection{Clifford1990Markov,
  author =     {Clifford, Peter},
  title =      {Markov random fields in statistics},
  booktitle =  {Disorder in physical systems},
  publisher =  {Oxford Univ. Press, New York},
  year =       {1990},
  series =     {Oxford Sci. Publ.},
  pages =      {19--32},
  mrclass =    {60G60 (62M05)},
  mrnumber =   {1064553 (91j:60094)},
  mrreviewer = {Bernard Prum}
}

@InProceedings{Collobert2008unified,
  author =       {Collobert, Ronan and Weston, Jason},
  title =        {A unified architecture for natural language processing: Deep neural networks with multitask learning},
  booktitle =    {Proceedings of the 25th international conference on Machine learning},
  year =         {2008},
  pages =        {160--167},
  organization = {ACM},
  file =         {:Collobert2008unified.pdf:PDF}
}

@Article{Collobert2011Natural,
  author =    {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  title =     {Natural language processing (almost) from scratch},
  journal =   jmlr_s,
  year =      {2011},
  volume =    {12},
  pages =     {2493--2537},
  file =      {:Collobert2011Natural.pdf:PDF},
  publisher = {JMLR. org}
}

@Article{Comminges2012Tight,
  Title                    = {Tight conditions for consistency of variable selection in the
 context of high dimensionality},
  Author                   = {La{\"e}titia Comminges and } # adalalyan,
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {2667--2696},
  Volume                   = {40},

  Doi                      = {10.1214/12-AOS1046},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G08 (42A16 62G20 62H12)},
  Mrnumber                 = {3097616},
  Mrreviewer               = {Ewa Skubalska-Rafaj{\l}owicz},
  Url                      = {http://dx.doi.org/10.1214/12-AOS1046}
}

@Article{cooper1977note,
  Title                    = {A Note On The Estimation Of The Parameters Of The Autoregressive-moving Average Process},
  Author                   = {D.~M. Cooper and R. Thompson},
  Journal                  = {Biometrika},
  Year                     = {1977},
  Number                   = {3},
  Pages                    = {625--628},
  Volume                   = {64},

  Newspaper                = {Biometrika},
  Owner                    = {mkolar},
  Publisher                = {Biometrika Trust},
  Timestamp                = {2014.02.18}
}

@Article{Corander2006Bayesian,
  author =    {Jukka Corander and Mattias Villani},
  title =     {A {Bayes}ian Approach to Modelling Graphical Vector Autoregressions},
  journal =   {Journal of Time Series Analysis},
  year =      {2006},
  volume =    {27},
  number =    {1},
  pages =     {141--156},
  month =     {jan},
  doi =       {10.1111/j.1467-9892.2005.00460.x},
  file =      {:Corander2006Bayesian.pdf:PDF},
  owner =     {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1111/j.1467-9892.2005.00460.x}
}

@Article{cotter99forward,
  Title                    = {Forward Sequential Algorithms For Best Basis Selection},
  Author                   = {S.~F. Cotter and B.~D. Rao and K. {Kreutz-Delgado} and J. Adler},
  Journal                  = {IEE Proc. Vision, Image and Signal Proces.},
  Year                     = {1999},
  Number                   = {5},
  Pages                    = {235--244},
  Volume                   = {146},

  Newspaper                = {IEE Proc. Vision, Image and Signal Proces.},
  Owner                    = {mkolar},
  Publisher                = {IET},
  Timestamp                = {2014.02.18}
}

@Article{Cox1992note,
  author =     {Cox, D. R. and Reid, N.},
  title =      {A note on the difference between profile and modified profile likelihood},
  journal =    {Biometrika},
  year =       {1992},
  volume =     {79},
  number =     {2},
  pages =      {408--411},
  coden =      {BIOKAX},
  doi =        {10.1093/biomet/79.2.408},
  file =       {Cox1992note.pdf:Cox1992note.pdf:PDF},
  fjournal =   {Biometrika},
  issn =       {0006-3444},
  mrclass =    {62A10},
  mrnumber =   {1185142},
  mrreviewer = {Franti{\v{s}}ek Rubl{\'{\i}}k},
  url =        {http://dx.doi.org/10.1093/biomet/79.2.408}
}

@Article{Csoergo1984Two,
  Title                    = {Two approaches to constructing simultaneous confidence bounds
 for quantiles},
  Author                   = {M. Cs{\"o}rg{\H{o}} and P. R{\'e}v{\'e}sz},
  Journal                  = {Probab. Math. Statist.},
  Year                     = {1984},
  Number                   = {2},
  Pages                    = {221--236},
  Volume                   = {4},

  Fjournal                 = {Probability and Mathematical Statistics},
  ISSN                     = {0208-4147},
  Mrclass                  = {62G15},
  Mrnumber                 = {792787 (87d:62090)},
  Mrreviewer               = {P. K. Sen}
}

@Article{igraph,
  Title                    = {The igraph software package for complex network research},
  Author                   = {Gabor Csardi and Tamas Nepusz},
  Journal                  = {InterJournal},
  Year                     = {2006},
  Pages                    = {1695},
  Volume                   = {Complex Systems},

  Url                      = {http://igraph.org}
}

@Article{csiszar06consistent,
  Title                    = {Consistent Estimation Of The Basic Neighborhood Of {m}arkov Random Fields},
  Author                   = {I. Csisz{\'a}r and Z. Talata},
  Journal                  = aos_s,
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {123--145},
  Volume                   = {34},

  Doi                      = {10.1214/009053605000000912},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2275237 (2008d:62087)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053605000000912}
}

@Article{Cucker2002mathematical,
  author =     {Cucker, Felipe and Smale, Steve},
  title =      {On the mathematical foundations of learning},
  journal =    {Bull. Amer. Math. Soc. (N.S.)},
  year =       {2002},
  volume =     {39},
  number =     {1},
  pages =      {1--49 (electronic)},
  coden =      {BAMOAD},
  doi =        {10.1090/S0273-0979-01-00923-5},
  file =       {:Cucker2002mathematical.pdf:PDF},
  fjournal =   {American Mathematical Society. Bulletin. New Series},
  issn =       {0273-0979},
  mrclass =    {68T05 (68T10 91E40 94A20)},
  mrnumber =   {1864085},
  mrreviewer = {Andrei Mart{\'{\i}}nez Finkelshtein},
  url =        {http://dx.doi.org/10.1090/S0273-0979-01-00923-5}
}

@Article{Dai2013Multivariate,
  Title                    = {Multivariate Bernoulli distribution},
  Author                   = {Bin Dai and Shilin Ding and Grace Wahba},
  Journal                  = {Bernoulli},
  Year                     = {2013},

  Month                    = {Sep},
  Number                   = {4},
  Pages                    = {1465--1483},
  Volume                   = {19},

  Doi                      = {10.3150/12-bejsp10},
  ISSN                     = {1350-7265},
  Owner                    = {mkolar},
  Publisher                = {Project Euclid},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.3150/12-BEJSP10}
}

@InCollection{Dai2014Scalable,
  author =    {Bo Dai and Bo Xie and Niao He and Yingyu Liang and Anant Raj and } # mbalcan #{ and Le Song},
  title =     {Scalable Kernel Methods via Doubly Stochastic Gradients},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year =      {2014},
  editor =    {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
  pages =     {3041--3049},
  url =       {http://papers.nips.cc/paper/5318-scalable-kernel-methods-via-doubly-stochastic-gradients.pdf}
}

@InProceedings{Dalalyan2013Learning,
  author =    {Arnak S. Dalalyan and Mohamed Hebiri and Katia Meziani and Joseph Salmon},
  title =     { Learning Heteroscedastic Models by Convex Programming under Group Sparsity },
  booktitle = jmlr_s #{ - W{\&}CP 28(3) (ICML 2013)},
  year =      {2013},
  pages =     {379--387},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       { http://hal-enpc.archives-ouvertes.fr/docs/00/81/39/08/PDF/Var_adap_ICML2013.pdf }
}

@Article{Danaher2011Joint,
  author    = {P. {Danaher} and P. {Wang} and } # dwitten,
  title     = {The Joint Graphical Lasso For Inverse Covariance Estimation Across Multiple Classes},
  journal   = JRSSB_s,
  year      = {2014},
  volume    = {76},
  number    = {2},
  pages     = {373-397},
  month     = {Mar},
  doi       = {10.1111/rssb.12033},
  file      = {Danaher2011Joint.pdf:Danaher2011Joint.pdf:PDF},
  owner     = {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2014.02.18},
  url       = {http://dx.doi.org/10.1111/rssb.12033},
}

@InProceedings{Dasgupta99learning,
  Title                    = {Learning Polytrees},
  Author                   = {S. Dasgupta},
  Booktitle                = {Proc. of UAI},
  Year                     = {1999},

  Address                  = {San Francisco, CA, USA},
  Pages                    = {134--141},
  Publisher                = {Morgan Kaufmann Publishers Inc.},

  ISBN                     = {1-55860-614-9},
  Location                 = {Stockholm, Sweden},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dl.acm.org/citation.cfm?id=2073796.2073812}
}

@Article{Dauphin2014Identifying,
  author =         {Yann Dauphin and Razvan Pascanu and Caglar Gulcehre and Kyunghyun Cho and Surya Ganguli and Yoshua Bengio},
  title =          {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  year =           {2014},
  month =          jun,
  abstract =       {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  comments =       {The theoretical review and analysis in this article draw heavily from arXiv:1405.4604 [cs.LG]},
  eprint =         {1406.2572},
  file =           {:Dauphin2014Identifying.pdf:PDF},
  oai2identifier = {1406.2572}
}

@Book{davidson,
  Title                    = {Genomic Regulatory Systems: In Development And Evolution},
  Author                   = {E.~H. Davidson},
  Publisher                = {Academic Press},
  Year                     = {2001},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InCollection{davidson01local,
  Title                    = {Local Operator Theory, Random Matrices And {b}anach Spaces},
  Author                   = {K.~R. Davidson and S.~J. Szarek},
  Booktitle                = {Handbook of the geometry of {B}anach spaces, {V}ol. {I}},
  Publisher                = {North-Holland},
  Year                     = {2001},

  Address                  = {Amsterdam},
  Pages                    = {317--366},

  Doi                      = {10.1016/S1874-5849(01)80010-3},
  Mrnumber                 = {1863696 (2004f:47002a)},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1016/S1874-5849(01)80010-3}
}

@Article{Davis2012Sparse,
  author =         {Richard A. Davis and Pengfei Zang and Tian Zheng},
  title =          {Sparse Vector Autoregressive Modeling},
  journal =        {ArXiv e-prints, arXiv:1207.0520},
  year =           {2012},
  month =          jul,
  abstract =       {The vector autoregressive (VAR) model has been widely used for modeling temporal dependence in a multivariate time series. For large (and even moderate) dimensions, the number of AR coefficients can be prohibitively large, resulting in noisy estimates, unstable predictions and difficult-to-interpret temporal dependence. To overcome such drawbacks, we propose a 2-stage approach for fitting sparse VAR (sVAR) models in which many of the AR coefficients are zero. The first stage selects non-zero AR coefficients based on an estimate of the partial spectral coherence (PSC) together with the use of BIC. The PSC is useful for quantifying the conditional relationship between marginal series in a multivariate process. A refinement second stage is then applied to further reduce the number of parameters. The performance of this 2-stage approach is illustrated with simulation results. The 2-stage approach is also applied to two real data examples: the first is the Google Flu Trends data and the second is a time series of concentration levels of air pollutants.},
  comments =       {39 pages, 7 figures},
  eprint =         {1207.0520},
  file =           {:Davis2012Sparse.pdf:PDF},
  oai2identifier = {1207.0520}
}

@Book{Boor2001practical,
  Title                    = {A practical guide to splines},
  Author                   = {Carl {de Boor}},
  Publisher                = {Springer-Verlag, New York},
  Year                     = {2001},
  Edition                  = {Revised},
  Series                   = {Applied Mathematical Sciences},
  Volume                   = {27},

  ISBN                     = {0-387-95366-3},
  Mrclass                  = {41-01 (41A15 65D05 65D07 65D10)},
  Mrnumber                 = {1900298 (2003f:41001)},
  Mrreviewer               = {Gerlind Plonka-Hoch},
  Pages                    = {xviii+346}
}

@Article{Dehling1994Random,
  author =     {Dehling, Herold and Mikosch, Thomas},
  title =      {Random quadratic forms and the bootstrap for {$U$}-statistics},
  journal =    jma_s,
  year =       {1994},
  volume =     {51},
  number =     {2},
  pages =      {392--413},
  doi =        {10.1006/jmva.1994.1069},
  file =       {Dehling1994Random.pdf:Dehling1994Random.pdf:PDF},
  fjournal =   {Journal of Multivariate Analysis},
  issn =       {0047-259X},
  mrclass =    {62G09 (62G20)},
  mrnumber =   {1321305},
  mrreviewer = {Erich Haeusler},
  url =        {http://dx.doi.org/10.1006/jmva.1994.1069}
}

@Article{Dekel2012Optimal,
  author =   {Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  title =    {Optimal distributed online prediction using mini-batches},
  journal =  jmlr_s,
  year =     {2012},
  volume =   {13},
  pages =    {165--202},
  file =     {:Dekel2012Optimal.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  issn =     {1532-4435},
  mrclass =  {68W15 (60E15 68W27)},
  mrnumber = {2913697}
}

@Article{Dempster1972Covariance,
  Title                    = {Covariance Selection},
  Author                   = {A.~P. Dempster},
  Journal                  = {Biometrics},
  Year                     = {1972},
  Pages                    = {157-175},
  Volume                   = {28},

  Newspaper                = {Biometrics},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{devroye1996probabilistic,
  Title                    = {A probabilistic theory of pattern recognition},
  Author                   = {Luc Devroye and L{\'a}szl{\'o} Gy{\"o}rfi and G{\'a}bor Lugosi},
  Publisher                = {Springer-Verlag},
  Year                     = {1996},

  Address                  = {New York},
  Series                   = {Applications of Mathematics (New York)},
  Volume                   = {31},

  ISBN                     = {0-387-94618-7},
  Mrclass                  = {68T10 (62G05 62H30 68T05)},
  Mrnumber                 = {1383093 (97d:68196)},
  Mrreviewer               = {Margaret P. Gessaman},
  Owner                    = {mkolar},
  Pages                    = {xvi+636},
  Timestamp                = {2014.02.18}
}

@Article{Dias1998Density,
  Title                    = {Density estimation via hybrid splines},
  Author                   = {Ronaldo Dias},
  Journal                  = {J. Statist. Comput. Simulation},
  Year                     = {1998},
  Number                   = {4},
  Pages                    = {277--293},
  Volume                   = {60},

  Coden                    = {JSCSAT},
  Doi                      = {10.1080/00949659808811893},
  Fjournal                 = {Journal of Statistical Computation and Simulation},
  ISSN                     = {0094-9655},
  Mrclass                  = {62G07},
  Mrnumber                 = {1704852},
  Url                      = {http://dx.doi.org/10.1080/00949659808811893}
}

@Article{Dinuzzo2011Client,
  author =    {Dinuzzo, Francesco and Pillonetto, Gianluigi and De Nicolao, Giuseppe},
  title =     {Client-server multitask learning from distributed datasets},
  journal =   {IEEE Transactions on Neural Networks},
  year =      {2011},
  volume =    {22},
  number =    {2},
  pages =     {290--303},
  file =      {:Dinuzzo2011Client.pdf:PDF},
  publisher = {IEEE}
}

@Article{dobigeon2007joint,
  Title                    = {Joint Segmentation Of Piecewise Constant Autoregressive Processes By Using A Hierarchical Model And A Bayesian Sampling Approach},
  Author                   = {N. Dobigeon and J.~Y. Tourneret and M. Davy},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {1251--1263},
  Volume                   = {55},

  Newspaper                = {IEEE Trans. Signal Proces.},
  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18}
}

@Article{Dobra2011Copula,
  Title                    = {Copula {G}aussian graphical models and their application to
 modeling functional disability data},
  Author                   = {Dobra, Adrian and Lenkoski, Alex},
  Journal                  = {Ann. Appl. Stat.},
  Year                     = {2011},
  Number                   = {2A},
  Pages                    = {969--993},
  Volume                   = {5},

  Doi                      = {10.1214/10-AOAS397},
  Fjournal                 = {The Annals of Applied Statistics},
  ISSN                     = {1932-6157},
  Mrclass                  = {62H20 (62F15 62G10 62H17 62P10)},
  Mrnumber                 = {2840183 (2012h:62223)},
  Mrreviewer               = {Xiaoping A. Shen},
  Url                      = {http://dx.doi.org/10.1214/10-AOAS397}
}

@Article{dondelinger12nonhomogeneous,
  Title                    = {Non-homogeneous Dynamic Bayesian Networks With Bayesian Regularization For Inferring Gene Regulatory Networks With Gradually Time-varying Structure},
  Author                   = {F. Dondelinger and S. L\'{e}bre and D. Husmeier},
  Journal                  = {Mach. Learn.},
  Year                     = {2012},
  Pages                    = {1-40},

  Doi                      = {10.1007/s10994-012-5311-x},
  ISSN                     = {0885-6125},
  Language                 = {English},
  Newspaper                = {Mach. Learn.},
  Owner                    = {mkolar},
  Publisher                = {Springer US},
  Timestamp                = {2014.02.18}
}

@InProceedings{dondelinger2010heterogeneous,
  Title                    = {Heterogeneous Continuous Dynamic Bayesian Networks With Flexible Structure And Inter-time Segment Information Sharing},
  Author                   = {F. Dondelinger and S. Lebre and D. Husmeier},
  Booktitle                = {Proc. of ICML},
  Year                     = {2010},

  Address                  = {Haifa, Israel},
  Editor                   = {Johannes F{\"u}rnkranz and Thorsten Joachims},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Dornhege04Boosting,
  Title                    = {Boosting Bit Rates In Non-invasive \textsc{eeg} Single-trial Classifications By Feature Combination And Multi-class Paradigms},
  Author                   = {G. Dornhege and B. Blankertz and G. Curio and K. M\"{u}ller},
  Journal                  = {IEEE Trans. Biomed. Eng.},
  Year                     = {2004},
  Pages                    = {993-1002},
  Volume                   = {51},

  Newspaper                = {IEEE Trans. Biomed. Eng.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Drton2011Global,
  author =     {Drton, Mathias and Foygel, Rina and Sullivant, Seth},
  title =      {Global identifiability of linear structural equation models},
  journal =    aos_s,
  year =       {2011},
  volume =     {39},
  number =     {2},
  pages =      {865--886},
  coden =      {ASTSC7},
  doi =        {10.1214/10-AOS859},
  file =       {:Drton2011Global.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62H05 (62E10 62J05)},
  mrnumber =   {2816341},
  mrreviewer = {Sanjay Chaudhuri},
  url =        {http://dx.doi.org/10.1214/10-AOS859}
}

@Article{Drton2016Structure,
  author    = {Mathias Drton and Marloes H. Maathuis},
  title     = {Structure Learning in Graphical Modeling},
  journal   = {Annual Review of Statistics and Its Application},
  year      = {2017},
  volume    = {4},
  number    = {1},
  pages     = {365--393},
  month     = {mar},
  doi       = {10.1146/annurev-statistics-060116-053803},
  publisher = {Annual Reviews},
  url       = {https://doi.org/10.1146/annurev-statistics-060116-053803},
}

@Article{Drton2008Graphical,
  author =     {Drton, Mathias and Richardson, Thomas S.},
  title =      {Graphical methods for efficient likelihood inference in {G}aussian covariance models},
  journal =    jmlr_s,
  year =       {2008},
  volume =     {9},
  pages =      {893--914},
  file =       {:Drton2008Graphical.pdf:PDF},
  fjournal =   {Journal of Machine Learning Research (JMLR)},
  issn =       {1532-4435},
  mrclass =    {68T05 (05C20 05C80 05C85 62F10)},
  mrnumber =   {2417257},
  mrreviewer = {C. Andy Tsao}
}

@InProceedings{Duarte2006Sparse,
  Title                    = {Sparse Signal Detection from Incoherent Projections},
  Author                   = {M.~F. Duarte and } # mdavenport # { and } # mwainw # { and } # rbaraniuk,
  Booktitle                = {Proc. IEEE Int. Conf. Acoustics Speed and Signal Processing},
  Year                     = {2006},
  Pages                    = {III-305-III-308},

  Doi                      = {10.1109/ICASSP.2006.1660651},
  ISBN                     = {1-4244-0469-X},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1109/ICASSP.2006.1660651}
}

@Article{Duchi2012Dual,
  author =   {Duchi, John C. and Agarwal, Alekh and Wainwright, Martin J.},
  title =    {Dual averaging for distributed optimization: convergence analysis and network scaling},
  journal =  {IEEE Trans. Automat. Control},
  year =     {2012},
  volume =   {57},
  number =   {3},
  pages =    {592--606},
  coden =    {IETAA9},
  doi =      {10.1109/TAC.2011.2161027},
  file =     {:Duchi2012Dual.pdf:PDF},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Automatic Control},
  issn =     {0018-9286},
  mrclass =  {90C29 (68W15 90C25)},
  mrnumber = {2932818},
  url =      {http://dx.doi.org/10.1109/TAC.2011.2161027}
}

@Article{Duchi2014Optimality,
  author         = {John C. Duchi and Michael I. Jordan and Martin J. Wainwright and Yuchen Zhang},
  title          = {Optimality guarantees for distributed statistical estimation},
  journal        = {ArXiv e-prints, arXiv:1405.0782},
  year           = {2014},
  month          = may,
  abstract       = {Large data sets often require performing distributed statistical estimation, with a full data set split across multiple machines and limited communication between machines. To study such scenarios, we define and study some refinements of the classical minimax risk that apply to distributed settings, comparing to the performance of estimators with access to the entire data. Lower bounds on these quantities provide a precise characterization of the minimum amount of communication required to achieve the centralized minimax risk. We study two classes of distributed protocols: one in which machines send messages independently over channels without feedback, and a second allowing for interactive communication, in which a central server broadcasts the messages from a given machine to all other machines. We establish lower bounds for a variety of problems, including location estimation in several families and parameter estimation in different types of regression models. Our results include a novel class of quantitative data-processing inequalities used to characterize the effects of limited communication.},
  comments       = {34 pages, 1 figure. Preliminary version appearing in Neural Information Processing Systems 2013 (http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints)},
  eprint         = {1405.0782},
  file           = {:Duchi2014Optimality.pdf:PDF},
  oai2identifier = {1405.0782},
}

@Book{edwards00introduction,
  Title                    = {Introduction To Graphical Modelling},
  Author                   = {D. Edwards},
  Publisher                = {Springer-Verlag},
  Year                     = {2000},

  Address                  = {New York},
  Edition                  = {Second},
  Series                   = {Springer Texts in Statistics},

  Doi                      = {10.1007/978-1-4612-0493-0},
  ISBN                     = {0-387-95054-0},
  Mrnumber                 = {1880319},
  Owner                    = {mkolar},
  Pages                    = {xvi+333},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/978-1-4612-0493-0}
}

@Article{efron04lars,
  Title                    = {Least Angle Regression},
  Author                   = {B. Efron and } # thastie # { and } # ijohnstone # { and } # rtibs,
  Journal                  = aos_s,
  Year                     = {2004},
  Note                     = {With discussion, and a rejoinder by the authors},
  Number                   = {2},
  Pages                    = {407--499},
  Volume                   = {32},

  Doi                      = {10.1214/009053604000000067},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2060166 (2005d:62116)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053604000000067}
}

@Article{Ehm2012Local,
  Title                    = {Local proper scoring rules of order two},
  Author                   = {Werner Ehm and Tilmann Gneiting},
  Journal                  = aos_s,
  Year                     = {2012},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {609--637},
  Volume                   = {40},

  Doi                      = {10.1214/12-aos973},
  ISSN                     = {0090-5364},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Url                      = {http://dx.doi.org/10.1214/12-AOS973}
}

@Article{Eichelsbacher2000Moderate,
  author =     {Eichelsbacher, Peter},
  title =      {Moderate deviations for degenerate {$U$}-processes},
  journal =    {Stochastic Process. Appl.},
  year =       {2000},
  volume =     {87},
  number =     {2},
  pages =      {255--279},
  coden =      {STOPB7},
  doi =        {10.1016/S0304-4149(99)00112-X},
  file =       {Eichelsbacher2000Moderate.pdf:Eichelsbacher2000Moderate.pdf:PDF},
  fjournal =   {Stochastic Processes and their Applications},
  issn =       {0304-4149},
  mrclass =    {60F10 (60B12)},
  mrnumber =   {1757115},
  mrreviewer = {Miguel A. Arcones},
  url =        {http://dx.doi.org/10.1016/S0304-4149(99)00112-X}
}

@InProceedings{eicker1967limit,
  Title                    = {Limit Theorems For Regressions With Unequal And Dependent Errors},
  Author                   = {F. Eicker},
  Booktitle                = PROC_s # { 5th Berkeley Symp. Math. Stat. Probab.},
  Year                     = {1967},
  Number                   = {1},
  Pages                    = {59--82},
  Volume                   = {1},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Einmahl2005Uniform,
  author =     {Uwe Einmahl and } # dmason,
  title =      {Uniform in bandwidth consistency of kernel-type function estimators},
  journal =    aos_s,
  year =       {2005},
  volume =     {33},
  number =     {3},
  pages =      {1380--1403},
  coden =      {ASTSC7},
  doi =        {10.1214/009053605000000129},
  file =       {:Einmahl2005Uniform.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G08 (60F15 62G07)},
  mrnumber =   {2195639 (2006j:62041)},
  mrreviewer = {No{\"e}l Veraverbeke},
  url =        {http://dx.doi.org/10.1214/009053605000000129}
}

@Article{el2011safe,
  Title                    = {Safe Feature Elimination In Sparse Supervised Learning},
  Author                   = {L. {El Ghaoui} and V. Viallon and T. Rabbani},
  Journal                  = {Pac. J. Optim.},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {667--698},
  Volume                   = {8},

  ISSN                     = {1348-9151},
  Mrnumber                 = {3026449},
  Newspaper                = {Pac. J. Optim.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Karoui08operator,
  Title                    = {Operator Norm Consistent Estimation Of Large-dimensional Sparse Covariance Matrices},
  Author                   = {N. {El Karoui}},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {6},
  Pages                    = {2717--2756},
  Volume                   = {36},

  Doi                      = {10.1214/07-AOS559},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2485011 (2010d:62132)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/07-AOS559}
}

@Article{Ellis2008Learning,
  author =   {Ellis, Byron and Wong, Wing Hung},
  title =    {Learning causal {B}ayesian network structures from experimental data},
  journal =  jasa_s,
  year =     {2008},
  volume =   {103},
  number =   {482},
  pages =    {778--789},
  coden =    {JSTNAL},
  doi =      {10.1198/016214508000000193},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {Database Expansion Item},
  mrnumber = {2524009},
  url =      {http://dx.doi.org/10.1198/016214508000000193}
}

@InCollection{Embrechts2003Modelling,
  Title                    = {Modelling dependence with copulas and applications to risk management},
  Author                   = {Paul Embrechts and Filip Lindskog and Alexander McNeil},
  Booktitle                = {Handbook of heavy tailed distributions in finance},
  Publisher                = {Elsevier},
  Year                     = {2003},
  Editor                   = {S. T. Rachev},
  Pages                    = {329--384},

  Url                      = {http://books.google.com/books?hl=en&amp;lr=&amp;id=sv8jGSVFra8C&amp;oi=fnd&amp;pg=PA329&amp;dq=Modelling+dependence+with+copulas+and+applications+to+risk+management&amp;ots=Yvg5prGUXz&amp;sig=b3Qs3l2OC8JgSQKPxUnY6Lq2Ql8}
}

@Article{Engle1982Autoregressive,
  Title                    = {Autoregressive conditional heteroscedasticity with estimates
 of the variance of {U}nited {K}ingdom inflation},
  Author                   = {Engle, Robert F.},
  Journal                  = {Econometrica},
  Year                     = {1982},
  Number                   = {4},
  Pages                    = {987--1007},
  Volume                   = {50},

  Coden                    = {ECMTA7},
  Doi                      = {10.2307/1912773},
  Fjournal                 = {Econometrica. Journal of the Econometric Society},
  ISSN                     = {0012-9682},
  Mrclass                  = {62P20},
  Mrnumber                 = {666121 (83j:62158)},
  Url                      = {http://dx.doi.org/10.2307/1912773}
}

@InProceedings{Evgeniou2005Learning,
  author =    {Evgeniou, Theodoros and Micchelli, Charles A and Pontil, Massimiliano},
  title =     {Learning multiple tasks with kernel methods},
  booktitle = jmlr_s,
  year =      {2005},
  pages =     {615--637},
  file =      {:Evgeniou2005Learning.pdf:PDF}
}

@InProceedings{Evgeniou2004Regularized,
  author =       {Evgeniou, Theodoros and Pontil, Massimiliano},
  title =        {Regularized multi-task learning},
  booktitle =    {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  year =         {2004},
  pages =        {109--117},
  organization = {ACM},
  file =         {:Evgeniou2004Regularized.pdf:PDF}
}

@Article{Fan2015Conditional,
  author =         {Jianqing Fan and Yang Feng and Lucy Xia},
  title =          {A Conditional Dependence Measure with Applications to Undirected Graphical Models},
  journal =        {ArXiv e-prints, arXiv:1501.01617},
  year =           {2015},
  month =          jan,
  abstract =       {Measuring conditional dependence is an important topic in statistics with broad applications including graphical models. Under a factor model setting, a new conditional dependence measure is proposed. The measure is derived by using distance covariance after adjusting the common observable factors or covariates. The corresponding conditional independence test is given with the asymptotic null distribution unveiled. The latter gives a somewhat surprising result: the estimating errors in factor loading matrices, while of root$-n$ order, do not have material impact on the asymptotic null distribution of the test statistic, which is also in the root$-n$ domain. It is also shown that the new test has strict control over the asymptotic significance level and can be calculated efficiently. A generic method for building dependency graphs using the new test is elaborated. Numerical results and real data analysis show the superiority of the new method.},
  comments =       {35 pages, 5 figures},
  eprint =         {1501.01617},
  file =           {:Fan2015Conditional.pdf:PDF},
  oai2identifier = {1501.01617}
}

@Article{Fang2014Testing,
  Title                    = {Testing and Confidence Intervals for High Dimensional Proportional Hazards Model},
  Author                   = {Ethan X. Fang and Yang Ning and } # hliu,
  Journal                  = {ArXiv e-prints, arXiv:1412.5158},
  Year                     = {2014},

  Owner                    = {mkolar},
  Timestamp                = {2015.10.07}
}

@Book{Fang1990Symmetric,
  Title                    = {Symmetric multivariate and related distributions},
  Author                   = {Kai Tai Fang and Samuel Kotz and Kai Wang Ng},
  Publisher                = {Chapman and Hall, Ltd., London},
  Year                     = {1990},
  Series                   = {Monographs on Statistics and Applied Probability},
  Volume                   = {36},

  Doi                      = {10.1007/978-1-4899-2937-2},
  ISBN                     = {0-412-31430-4},
  Mrclass                  = {62H05 (60E05)},
  Mrnumber                 = {1071174 (91i:62070)},
  Mrreviewer               = {Pawe{\l}J. Szab{\l}owski},
  Pages                    = {x+220},
  Url                      = {http://dx.doi.org/10.1007/978-1-4899-2937-2}
}

@Article{fearnhead2006exact,
  Title                    = {Exact And Efficient Bayesian Inference For Multiple Changepoint Problems},
  Author                   = {P. Fearnhead},
  Journal                  = {Stat. Comput.},
  Year                     = {2006},
  Number                   = {2},
  Pages                    = {203--213},
  Volume                   = {16},

  Newspaper                = {Stat. Comput.},
  Publisher                = {Springer}
}

@Article{Fellinghauer2013Stable,
  Title                    = {Stable graphical model estimation with Random Forests for discrete, continuous, and mixed variables},
  Author                   = {Bernd Fellinghauer and } # pbuhl # { and Martin Ryffel and Michael {von Rhein} and Jan D. Reinhardt},
  Journal                  = csda_s,
  Year                     = {2013},

  Month                    = {Aug},
  Pages                    = {132--152},
  Volume                   = {64},

  Doi                      = {10.1016/j.csda.2013.02.022},
  ISSN                     = {0167-9473},
  Owner                    = {mkolar},
  Publisher                = {Elsevier BV},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1016/j.csda.2013.02.022}
}

@Article{Ferger2001Analysis,
  author =     {Ferger, Dietmar},
  title =      {Analysis of change-point estimators under the null hypothesis},
  journal =    {Bernoulli},
  year =       {2001},
  volume =     {7},
  number =     {3},
  pages =      {487--506},
  doi =        {10.2307/3318498},
  file =       {Ferger2001Analysis.pdf:Ferger2001Analysis.pdf:PDF},
  fjournal =   {Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability},
  issn =       {1350-7265},
  mrclass =    {62G05 (60F17 62G20)},
  mrnumber =   {1836742},
  mrreviewer = {M. Hu{\v{s}}kov{\'a}},
  url =        {http://dx.doi.org/10.2307/3318498}
}

@Article{Finegold2011Robust,
  Title                    = {Robust graphical modeling of gene networks using classical and alternative t-distributions},
  Author                   = {Michael Finegold and } # mdrton,
  Journal                  = aoas_s,
  Year                     = {2011},

  Month                    = {Jun},
  Number                   = {2A},
  Pages                    = {1057--1080},
  Volume                   = {5},

  Doi                      = {10.1214/10-aoas410},
  ISSN                     = {1932-6157},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1214/10-AOAS410}
}

@Article{Forbes2013Linear,
  author =     {Forbes, Peter G. M. and } # slauritzen,
  title =      {Linear estimating equations for exponential families with application to {G}aussian linear concentration models},
  journal =    {Linear Algebra Appl.},
  year =       {2015},
  volume =     {473},
  pages =      {261--283},
  doi =        {10.1016/j.laa.2014.08.015},
  file =       {:Forbes2013Linear.pdf:PDF},
  fjournal =   {Linear Algebra and its Applications},
  issn =       {0024-3795},
  mrclass =    {62H12 (62F10)},
  mrnumber =   {3338335},
  mrreviewer = {Ivan {\v{Z}}e{\v{z}}ula},
  url =        {http://dx.doi.org/10.1016/j.laa.2014.08.015}
}

@Article{fox2011bayesian,
  Title                    = {Bayesian Nonparametric Inference Of Switching Dynamic Linear Models},
  Author                   = {E. Fox and E.~B. Sudderth and } # mjordan # { and A.~S. Willsky},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {1569--1585},
  Volume                   = {59},

  Newspaper                = {IEEE Trans. Signal Proces.},
  Publisher                = {IEEE}
}

@Article{Barber2014Controlling,
  Title                    = {Controlling the False Discovery Rate via Knockoffs},
  Author                   = {Rina {Foygel Barber} and } # ecandes,
  Journal                  = {ArXiv e-prints, arXiv:1404.5609},
  Year                     = {2014},

  Month                    = apr,

  Abstract                 = {In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR)---the expected fraction of false discoveries among all discoveries---is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, and the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap---their construction does not require any new data---and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high. We also apply the knockoff filter to HIV data with the goal of identifying those mutations associated with a form of resistance to treatment plans.},
  Eprint                   = {1404.5609},
  Oai2identifier           = {1404.5609}
}

@Article{Frank1956algorithm,
  author =    {Frank, Marguerite and Wolfe, Philip},
  title =     {An algorithm for quadratic programming},
  journal =   {Naval research logistics quarterly},
  year =      {1956},
  volume =    {3},
  number =    {1-2},
  pages =     {95--110},
  file =      {:Frank1956algorithm.pdf:PDF},
  publisher = {Wiley Online Library}
}

@Article{Friedman2001Greedy,
  author =   {Friedman, Jerome H.},
  title =    {Greedy function approximation: a gradient boosting machine},
  journal =  {Ann. Statist.},
  year =     {2001},
  volume =   {29},
  number =   {5},
  pages =    {1189--1232},
  coden =    {ASTSC7},
  doi =      {10.1214/aos/1013203451},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62-02 (62-07 62G08 62H30)},
  mrnumber = {1873328},
  url =      {http://dx.doi.org/10.1214/aos/1013203451}
}

@Article{Fu2014Adaptive,
  author =         {Fei Fu and Jiaying Gu and Qing Zhou},
  title =          {Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data},
  journal =        {ArXiv e-prints, arXiv:1403.2310},
  year =           {2014},
  month =          mar,
  abstract =       {We develop in this article a penalized likelihood method to estimate sparse Bayesian networks from categorical data. The structure of a Bayesian network is represented by a directed acyclic graph (DAG). We model the conditional distribution of a node given its parents by the multi-logit regression and estimate the structure of a DAG via maximizing a regularized likelihood. The adaptive group Lasso penalty is employed to encourage sparsity by selecting grouped dummy variables encoding the level of a factor. We develop a blockwise coordinate descent algorithm to solve the penalized likelihood problem subject to the acyclicity constraint of a DAG. When intervention data are available, our method may construct a causal network, in which a directed edge represents a causal relation. We apply our method to various simulated networks and a real biological network. The results show that our method is very competitive, compared to other existing methods, in DAG estimation from both interventional and high-dimensional observational data. We also establish consistency in parameter and structure estimation for our method when the number of nodes is fixed.},
  comments =       {40 pages, 2 figures, 5 tables},
  eprint =         {1403.2310},
  file =           {:Fu2014Adaptive.pdf:PDF},
  oai2identifier = {1403.2310}
}

@Article{Fu2013Learning,
  author =   {Fu, Fei and Zhou, Qing},
  title =    {Learning sparse causal {G}aussian networks with experimental intervention: regularization and coordinate descent},
  journal =  jasa_s,
  year =     {2013},
  volume =   {108},
  number =   {501},
  pages =    {288--300},
  doi =      {10.1080/01621459.2012.754359},
  file =     {:Fu2013Learning_supp.pdf:PDF;Fu2013Learning.pdf:Fu2013Learning.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {62H12 (62F15 62H99 62J07)},
  mrnumber = {3174620},
  url =      {http://dx.doi.org/10.1080/01621459.2012.754359}
}

@Article{fuchs2005recovery,
  Title                    = {Recovery Of Exact Sparse Representations In The Presence Of Bounded Noise},
  Author                   = {J.~J. Fuchs},
  Journal                  = {IEEEit},
  Year                     = {2005},
  Number                   = {10},
  Pages                    = {3601--3608},
  Volume                   = {51},

  Doi                      = {10.1109/TIT.2005.855614},
  ISSN                     = {0018-9448},
  Mrnumber                 = {2237526 (2007a:94037)},
  Newspaper                = {IEEEit},
  Url                      = {http://dx.doi.org/10.1109/TIT.2005.855614}
}

@Article{fujita2007time,
  Title                    = {Time-varying Modeling Of Gene Expression Regulatory Networks Using The Wavelet Dynamic Vector Autoregressive Method},
  Author                   = {A. Fujita and J.~R. Sato and H.~M. Garay-Malpartida and P.~A. Morettin and M.~C. Sogayar and C.~E. Ferreira},
  Journal                  = {Bioinformatics},
  Year                     = {2007},
  Number                   = {13},
  Pages                    = {1623--1630},
  Volume                   = {23},

  Newspaper                = {Bioinformatics},
  Publisher                = {Oxford Univ Press}
}

@Article{Fukumizu2007Statistical,
  author =   {Kenji Fukumizu and } # fbach #{ and Arthur Gretton},
  title =    {Statistical consistency of kernel canonical correlation analysis},
  journal =  jmlr_s,
  year =     {2007},
  volume =   {8},
  pages =    {361--383},
  file =     {:Fukumizu2007Statistical.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  issn =     {1532-4435},
  mrclass =  {62H30},
  mrnumber = {2320675}
}

@Article{Fukumizu2009Kernel,
  author =     {Fukumizu, Kenji and Bach, Francis R. and Jordan, Michael I.},
  title =      {Kernel dimension reduction in regression},
  journal =    {Ann. Statist.},
  year =       {2009},
  volume =     {37},
  number =     {4},
  pages =      {1871--1905},
  coden =      {ASTSC7},
  doi =        {10.1214/08-AOS637},
  file =       {Fukumizu2009Kernel.pdf:Fukumizu2009Kernel.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62H99 (62J02)},
  mrnumber =   {2533474},
  mrreviewer = {Wojciech Zieli{\'n}ski},
  url =        {http://dx.doi.org/10.1214/08-AOS637}
}

@Article{GSell2013Adaptive,
  Title                    = {Adaptive testing for the graphical lasso},
  Author                   = {Max Grazier G'Sell and } # jtaylor # { and } # rtibs,
  Journal                  = {ArXiv e-prints, arXiv:1307.4765},
  Year                     = {2013},

  Month                    = jul,

  Abstract                 = {We consider tests of significance in the setting of the graphical lasso for inverse covariance matrix estimation. We propose a simple test statistic based on a subsequence of the knots in the graphical lasso path. We show that this statistic has an exponential asymptotic null distribution, under the null hypothesis that the model contains the true connected components. Though the null distribution is asymptotic, we show through simulation that it provides a close approximation to the true distribution at reasonable sample sizes. Thus the test provides a simple, tractable test for the significance of new edges as they are introduced into the model. Finally, we show connections between our results and other results for regularized regression, as well as extensions of our results to other correlation matrix based methods like single-linkage clustering.},
  Comments                 = {33 pages, 8 figures. Submitted to Annals of Statistics},
  Eprint                   = {1307.4765},
  Oai2identifier           = {1307.4765}
}

@Article{Gao2010Asymptotic,
  Title                    = {Asymptotic analysis of high-dimensional {LAD} regression with {L}asso},
  Author                   = {Xiaoli Gao and } # jhuang,
  Journal                  = statsin_s,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1485--1506},
  Volume                   = {20},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62J07 (60F17 62F12 62G20)},
  Mrnumber                 = {2777333 (2011k:62206)},
  Mrreviewer               = {Fengrong Wei}
}

@Article{Gautier2011High,
  author =         {Eric Gautier and } # atsybakov,
  title =          {High-dimensional instrumental variables regression and confidence sets},
  journal =        {arXiv preprint arXiv:1105.2454},
  year =           {2011},
  month =          may,
  abstract =       {We propose an instrumental variables method for estimation in linear models with endogenous regressors in the high-dimensional setting where the sample size n can be smaller than the number of possible regressors K, and L>=K instruments. We allow for heteroscedasticity and we do not need a prior knowledge of variances of the errors. We suggest a new procedure called the STIV (Self Tuning Instrumental Variables) estimator, which is realized as a solution of a conic optimization program. The main results of the paper are upper bounds on the estimation error of the vector of coefficients in l_p-norms for 1<= p<=\infty that hold with probability close to 1, as well as the corresponding confidence intervals. All results are non-asymptotic. These bounds are meaningful under the assumption that the true structural model is sparse, i.e., the vector of coefficients has few non-zero coordinates (less than the sample size n) or many coefficients are too small to matter. In our IV regression setting, the standard tools from the literature on sparsity, such as ther restricted eigenvalue assumption are inapplicable. Therefore, for our analysis we develop a new approach based on data-driven sensitivity characteristics. We show that, under appropriate assumptions, a thresholded STIV estimator correctly selects the non-zero coefficients with probability close to 1. The price to pay for not knowing which coefficients are non-zero and which instruments to use is of the order \sqrt{\log(L)} in the rate of convergence. We extend the procedure to deal with high-dimensional problems where some instruments can be non-valid. We obtain confidence intervals for non-validity indicators and we suggest a procedure, which correctly detects the non-valid instruments with probability close to 1.},
  eprint =         {1105.2454},
  file =           {:Gautier2011High.pdf:PDF},
  oai2identifier = {1105.2454}
}

@InCollection{Gautier2013Pivotal,
  author =    {Eric Gautier and } # atsybakov,
  title =     {Pivotal estimation in high-dimensional regression via linear programming},
  booktitle = {Empirical inference},
  publisher = {Springer, Heidelberg},
  year =      {2013},
  pages =     {195--204},
  doi =       {10.1007/978-3-642-41136-6_17},
  file =      {:Gautier2013Pivotal.pdf:PDF},
  mrclass =   {62G08 (90C05)},
  mrnumber =  {3236866},
  url =       {http://dx.doi.org/10.1007/978-3-642-41136-6_17}
}

@Article{Gaynanova2014Optimal,
  author =     {Irina Gaynanova and } # mkolar,
  title =      {Optimal variable selection in multi-group sparse discriminant analysis},
  journal =    {Electron. J. Stat.},
  year =       {2015},
  volume =     {9},
  number =     {2},
  pages =      {2007--2034},
  doi =        {10.1214/15-EJS1064},
  fjournal =   {Electronic Journal of Statistics},
  issn =       {1935-7524},
  mrclass =    {62H30 (62J07 68Q32)},
  mrnumber =   {3393602},
  mrreviewer = {Changyi Park},
  url =        {http://dx.doi.org/10.1214/15-EJS1064}
}

@Article{Gaynanova2014Simultaneous,
  Title                    = {Simultaneous sparse estimation of canonical vectors in the $p \gg N$ setting},
  Author                   = {Irina Gaynanova and James G. Booth and Martin T. Wells},
  Journal                  = {ArXiv e-prints, arXiv:1403.6095},
  Year                     = {2014},

  Month                    = mar,

  Abstract                 = {This article considers the problem of sparse estimation of canonical vectors in linear discriminant analysis when $p\gg N$. Several methods have been proposed in the literature that estimate one canonical vector in the two-group case. However, $G-1$ canonical vectors can be considered if the number of groups is $G$. In the multi-group context, it is common to estimate canonical vectors in a sequential fashion. Moreover, separate prior estimation of the covariance structure is often required. We propose a novel methodology for direct estimation of canonical vectors. In contrast to existing techniques, the proposed method estimates all canonical vectors at once, performs variable selection across all the vectors and comes with theoretical guarantees on the variable selection and classification consistency. First, we highlight the fact that in the $N>p$ setting the canonical vectors can be expressed in a closed form up to an orthogonal transformation. Secondly, we propose an extension of this form to the $p\gg N$ setting and achieve feature selection by using a group penalty. The resulting optimization problem is convex and can be solved using a block-coordinate descent algorithm. The practical performance of the method is evaluated through simulation studies as well as real data applications.},
  Comments                 = {Added classification consistency},
  Eprint                   = {1403.6095},
  Oai2identifier           = {1403.6095},
  Owner                    = {mkolar},
  Timestamp                = {2014.11.12}
}

@Article{geary1966teacher,
  Title                    = {The Teacher's Corner: A Note On Residual Heterovariance And Estimation Efficiency In Regression},
  Author                   = {R.~C. Geary},
  Journal                  = {Am. Stat.},
  Year                     = {1966},
  Number                   = {4},
  Pages                    = {30--31},
  Volume                   = {20},

  Newspaper                = {The American Statistician},
  Publisher                = {Taylor \& Francis}
}

@Article{AndrewGelman1991Note,
  author =    {Andrew Gelman and Xiao-Li Meng},
  title =     {A Note on Bivariate Distributions That Are Conditionally Normal},
  journal =   {The American Statistician},
  year =      {1991},
  volume =    {45},
  number =    {2},
  pages =     {125-126},
  abstract =  {It is possible for a nonnormal bivariate distribution to have conditional distribution functions that are normal in both directions. This article presents several examples, with graphs, including a counterintuitive bimodal joint density. The graphs simultaneously display the joint density and the conditional density functions, which appear as Gaussian curves in the three-dimensional plots.},
  issn =      {00031305},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  url =       {http://www.jstor.org/stable/2684374}
}

@Article{Genovese2004stochastic,
  author =     {Christopher Genovese and } # lwasser,
  title =      {A stochastic process approach to false discovery control},
  journal =    aos_s,
  year =       {2004},
  volume =     {32},
  number =     {3},
  pages =      {1035--1061},
  coden =      {ASTSC7},
  doi =        {10.1214/009053604000000283},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62H15 (62G10)},
  mrnumber =   {2065197},
  mrreviewer = {M. Hu{\v{s}}kov{\'a}},
  url =        {http://dx.doi.org/10.1214/009053604000000283}
}

@Article{genovese2009revisiting,
  Title                    = {A Comparison Of The Lasso And Marginal Regression},
  Author                   = {C.~R. Genovese and J. J.in and } # lwasser # { and Z. Yao},
  Journal                  = jmlr_s,
  Year                     = {2012},
  Pages                    = {2107--2143},
  Volume                   = {13},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2956354},
  Newspaper                = {J. Mach. Learn. Res.}
}

@Book{getoor07introduction,
  Title                    = {Introduction To Statistical Relational Learning},
  Author                   = {L. Getoor and B. Taskar},
  Publisher                = {MIT Press},
  Year                     = {2007},

  Address                  = {Cambridge, MA},
  Series                   = {Adaptive Computation and Machine Learning},

  ISBN                     = {978-0-262-07288-5; 0-262-07288-2},
  Mrnumber                 = {2391486 (2009i:68002)},
  Pages                    = {xiv+587}
}

@Article{Ghosh1972bounded,
  Title                    = {On bounded length confidence interval for the regression coefficient based on a class of rank statistics},
  Author                   = {Malay Ghosh and Pranab Kumar Sen},
  Journal                  = {Sankhy\=a Ser. A},
  Year                     = {1972},
  Pages                    = {33--52},
  Volume                   = {34},

  Fjournal                 = {Sankhy\=a (Statistics). The Indian Journal of Statistics.
 Series A},
  ISSN                     = {0581-572X},
  Mrclass                  = {62G15},
  Mrnumber                 = {0365892 (51 \#2144)},
  Mrreviewer               = {Jana Jureckova}
}

@Article{Gine2010Confidence,
  author =     {Gin{\'e}, Evarist and Nickl, Richard},
  title =      {Confidence bands in density estimation},
  journal =    {Ann. Statist.},
  year =       {2010},
  volume =     {38},
  number =     {2},
  pages =      {1122--1170},
  coden =      {ASTSC7},
  doi =        {10.1214/09-AOS738},
  file =       {Gine2010Confidence.pdf:Gine2010Confidence.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G07 (60F05 62G15)},
  mrnumber =   {2604707},
  mrreviewer = {No{\"e}l Veraverbeke},
  url =        {http://dx.doi.org/10.1214/09-AOS738}
}

@Article{Gine2009exponential,
  author    = {Evarist Gin\'{e} and Richard Nickl},
  title     = {An exponential inequality for the distribution function of the kernel density estimator, with applications to adaptive estimation},
  journal   = {Probab. Theory Related Fields},
  year      = {2009},
  volume    = {143},
  number    = {3--4},
  pages     = {569--596},
  month     = {Mar},
  issn      = {1432-2064},
  doi       = {10.1007/s00440-008-0137-y},
  file      = {:Gine2009exponential.pdf:PDF},
  publisher = {Springer Science + Business Media},
  url       = {http://dx.doi.org/10.1007/s00440-008-0137-y},
}

@Article{Giudici2016Graphical,
  author =    {P. Giudici and A. Spelta},
  title =     {Graphical Network Models for International Financial Flows},
  journal =   jbes_s,
  year =      {2016},
  volume =    {34},
  number =    {1},
  pages =     {128--138},
  month =     {jan},
  doi =       {10.1080/07350015.2015.1017643},
  file =      {Giudici2016Graphical.pdf:Giudici2016Graphical.pdf:PDF},
  owner =     {mkolar},
  publisher = {Informa {UK} Limited},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1080/07350015.2015.1017643}
}

@Article{Giurcanu2012Bootstrapping,
  author =     {Giurcanu, Mihai C.},
  title =      {Bootstrapping in non-regular smooth function models},
  journal =    jma_s,
  year =       {2012},
  volume =     {111},
  pages =      {78--93},
  doi =        {10.1016/j.jmva.2012.04.016},
  file =       {Giurcanu2012Bootstrapping.pdf:Giurcanu2012Bootstrapping.pdf:PDF},
  fjournal =   {Journal of Multivariate Analysis},
  issn =       {0047-259X},
  mrclass =    {62G09 (62G15 62G20)},
  mrnumber =   {2944407},
  mrreviewer = {Michael Falk},
  url =        {http://dx.doi.org/10.1016/j.jmva.2012.04.016}
}

@Article{Golbabaee2012Compressed,
  Title                    = {Compressed Sensing of Simultaneous Low-Rank and Joint-Sparse Matrices},
  Author                   = {Mohammad Golbabaee and Pierre Vandergheynst},
  Journal                  = {arXiv preprint arXiv:1211.5058},
  Year                     = {2012},

  Month                    = nov,

  Abstract                 = {In this paper we consider the problem of recovering a high dimensional data matrix from a set of incomplete and noisy linear measurements. We introduce a new model that can efficiently restrict the degrees of freedom of the problem and is generic enough to find a lot of applications, for instance in multichannel signal compressed sensing (e.g. sensor networks, hyperspectral imaging) and compressive sparse principal component analysis (s-PCA). We assume data matrices have a simultaneous low-rank and joint sparse structure, and we propose a novel approach for efficient compressed sensing (CS) of such data. Our CS recovery approach is based on a convex minimization problem that incorporates this restrictive structure by jointly regularizing the solutions with their nuclear (trace) norm and l2/l1 mixed norm. Our theoretical analysis uses a new notion of restricted isometry property (RIP) and shows that, for sampling schemes satisfying RIP, our approach can stably recover all low-rank and joint-sparse matrices. For a certain class of random sampling schemes satisfying a particular concentration bound (e.g. the subgaussian ensembles) we derive a lower bound on the number of CS measurements indicating the near-optimality of our recovery approach as well as a significant enhancement compared to the state-of-the-art. We introduce an iterative algorithm based on proximal calculus in order to solve the joint nuclear and l2/l1 norms minimization problem and, finally, we illustrate the empirical recovery phase transition of this approach by series of numerical experiments.},
  Comments                 = {32 pages},
  Eprint                   = {1211.5058},
  Oai2identifier           = {1211.5058},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Good1971Nonparametric,
  Title                    = {Nonparametric roughness penalties for probability densities},
  Author                   = {I. J. Good and R. A. Gaskins},
  Journal                  = {Biometrika},
  Year                     = {1971},
  Pages                    = {255--277},
  Volume                   = {58},

  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {62G05},
  Mrnumber                 = {0319314 (47 \#7858)},
  Mrreviewer               = {D. R. Truax}
}

@InCollection{Gorham2015Measuring,
  author =    {Gorham, Jackson and Mackey, Lester},
  title =     {Measuring Sample Quality with Stein's Method},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {226--234},
  file =      {Gorham2015Measuring.pdf:Gorham2015Measuring.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5768-measuring-sample-quality-with-steins-method.pdf}
}

@Article{Gorski2007Biconvex,
  author =     {Gorski, Jochen and Pfeuffer, Frank and Klamroth, Kathrin},
  title =      {Biconvex sets and optimization with biconvex functions: a survey and extensions},
  journal =    {Math. Methods Oper. Res.},
  year =       {2007},
  volume =     {66},
  number =     {3},
  pages =      {373--407},
  doi =        {10.1007/s00186-007-0161-1},
  file =       {Gorski2007Biconvex.pdf:Gorski2007Biconvex.pdf:PDF},
  fjournal =   {Mathematical Methods of Operations Research},
  issn =       {1432-2994},
  mrclass =    {90C26 (26B25)},
  mrnumber =   {2357657},
  mrreviewer = {W. W. Breckner},
  url =        {http://dx.doi.org/10.1007/s00186-007-0161-1}
}

@Article{Gould2006Brain,
  Title                    = {Brain Mechanisms Of Successful Compensation During Learning In Alzheimer Disease},
  Author                   = {R.~L. Gould and B. Arroyo and R.~G. Brown and A.~M. Owen and E.~T. Bullmore and R.~J. Howard},
  Journal                  = {Neurology},
  Year                     = {2006},
  Number                   = {6},
  Pages                    = {1011--1017},
  Volume                   = {67},

  Newspaper                = {Neurology}
}

@Misc{Grant2012cvx,
  Title                    = {{cvx}: Matlab Software For Disciplined Convex Programming, Version 2.0 Beta},

  Author                   = {M.~C. Grant and S.~P. Boyd},
  HowPublished             = {\url{http://cvxr.com/cvx}},
  Month                    = {September},
  Year                     = {2012},

  Monthno                  = {9},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Greenshtein2006Best,
  Title                    = {Best subset selection, persistence in high-dimensional
 statistical learning and optimization under {$l\sb 1$}
 constraint},
  Author                   = {Eitan Greenshtein},
  Journal                  = aos_s,
  Year                     = {2006},
  Number                   = {5},
  Pages                    = {2367--2386},
  Volume                   = {34},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/009053606000000768},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62M20 (62C12)},
  Mrnumber                 = {2291503 (2008e:62154)},
  Mrreviewer               = {Yurij S. Kharin},
  Url                      = {http://dx.doi.org/10.1214/009053606000000768}
}

@Article{Greenshtein2004Persistence,
  Title                    = {Persistence in high-dimensional linear predictor selection and the virtue of overparametrization},
  Author                   = {Eitan Greenshtein and Ya'acov Ritov},
  Journal                  = {Bernoulli},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {971--988},
  Volume                   = {10},

  Doi                      = {10.3150/bj/1106314846},
  Fjournal                 = {Bernoulli. Official Journal of the Bernoulli Society for
 Mathematical Statistics and Probability},
  ISSN                     = {1350-7265},
  Mrclass                  = {62J05 (62F30)},
  Mrnumber                 = {2108039 (2005i:62102)},
  Mrreviewer               = {Ji{\v{r}}{\'{\i}} And{\v{e}}l},
  Url                      = {http://dx.doi.org/10.3150/bj/1106314846}
}

@Article{Greicius2004Default,
  Title                    = {Default-mode Network Activity Distinguishes Alzheimer's Disease From Healthy Aging: Evidence From Functional {mri}},
  Author                   = {M.~D. Greicius and G. Srivastava and A.~L. Reiss and V. Menon},
  Journal                  = pnas_s,
  Year                     = {2004},
  Number                   = {13},
  Pages                    = {4637--4642},
  Volume                   = {101},

  Newspaper                = {Proc. Natl. Acad. Sci. U.S.A.}
}

@Article{Gruet1996nonparametric,
  author =     {Gruet, Marie-Anne},
  title =      {A nonparametric calibration analysis},
  journal =    aos_s,
  year =       {1996},
  volume =     {24},
  number =     {4},
  pages =      {1474--1492},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1032298278},
  file =       {Gruet1996nonparametric.pdf:Gruet1996nonparametric.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G05 (62G20)},
  mrnumber =   {1416643},
  mrreviewer = {Wolfgang Polonik},
  url =        {http://dx.doi.org/10.1214/aos/1032298278}
}

@InProceedings{grzegorczyk2012bayesian,
  Title                    = {Bayesian Regularization Of Non-homogeneous Dynamic Bayesian Networks By Globally Coupling Interaction Parameters},
  Author                   = {M. Grzegorczyk and D. Husmeier},
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2012},
  Editor                   = {Neil Lawrence and Mark Girolami },
  Pages                    = {467--476}
}

@Article{grzegorczyk2011improvements,
  Title                    = {Improvements In The Reconstruction Of Time-varying Gene Regulatory Networks: Dynamic Programming And Regularization By Information Sharing Among Genes},
  Author                   = {M. Grzegorczyk and D. Husmeier},
  Journal                  = {Bioinformatics},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {693--699},
  Volume                   = {27},

  Newspaper                = {Bioinformatics},
  Publisher                = {Oxford Univ Press}
}

@Article{Grzegorczyk2011Nonhomogeneous,
  Title                    = {Non-homogeneous Dynamic Bayesian Networks For Continuous Data},
  Author                   = {M. Grzegorczyk and D. Husmeier},
  Journal                  = {Mach. Learn.},
  Year                     = {2011},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {355--419},
  Volume                   = {83},

  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s10994-010-5230-7},
  ISSN                     = {0885-6125},
  Monthno                  = {6},
  Newspaper                = {Mach. Learn.},
  Publisher                = {Kluwer Academic Publishers}
}

@Article{Gu1993Smoothing,
  Title                    = {Smoothing spline density estimation: a dimensionless automatic
 algorithm},
  Author                   = {Gu, Chong},
  Journal                  = jasa_s,
  Year                     = {1993},
  Number                   = {422},
  Pages                    = {495--504},
  Volume                   = {88},

  Coden                    = {JSTNAL},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62G05},
  Mrnumber                 = {1224374},
  Url                      = {http://links.jstor.org/sici?sici=0162-1459(199306)88:422<495:SSDEAD>2.0.CO;2-U&origin=MSN}
}

@Article{Gu1995Smoothing,
  Title                    = {Smoothing spline density estimation: conditional distribution},
  Author                   = {Gu, Chong},
  Journal                  = statsin_s,
  Year                     = {1995},
  Number                   = {2},
  Pages                    = {709--726},
  Volume                   = {5},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62G05},
  Mrnumber                 = {1347615 (96d:62056)}
}

@Article{gu2013nonparametric,
  author =  {Chong Gu and Yongho Jeon and Yi Lin},
  title =   {Nonparametric density estimation in high-dimensions},
  journal = statsin_s,
  year =    {2013},
  volume =  {23},
  pages =   {1131--1153},
  file =    {gu2013nonparametric.pdf:gu2013nonparametric.pdf:PDF}
}

@Article{Gu1993Smoothinga,
  Title                    = {Smoothing spline density estimation: theory},
  Author                   = {Gu, Chong and Qiu, Chunfu},
  Journal                  = aos_s,
  Year                     = {1993},
  Number                   = {1},
  Pages                    = {217--234},
  Volume                   = {21},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/aos/1176349023},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (65D07 65D10)},
  Mrnumber                 = {1212174 (94c:62064)},
  Url                      = {http://dx.doi.org/10.1214/aos/1176349023}
}

@Article{Gu2003Penalized,
  Title                    = {Penalized likelihood density estimation: direct
 cross-validation and scalable approximation},
  Author                   = {Gu, Chong and Wang, Jingyuan},
  Journal                  = statsin_s,
  Year                     = {2003},
  Number                   = {3},
  Pages                    = {811--826},
  Volume                   = {13},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62G07},
  Mrnumber                 = {1997175}
}

@Article{Gu2015Local,
  Title                    = {Local and Global Inference for High Dimensional Gaussian Copula Graphical Models},
  Author                   = {{Gu}, Q. and {Cao}, Y. and {Ning}, Y. and } # hliu,
  Journal                  = {ArXiv e-prints, arXiv:1502.02347},
  Year                     = {2015},

  Month                    = feb,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2015arXiv150202347G},
  Archiveprefix            = {arXiv},
  Eprint                   = {1502.02347},
  Keywords                 = {Statistics - Machine Learning},
  Primaryclass             = {stat.ML}
}

@InProceedings{guohannekefuxing2007,
  Title                    = {Recovering Temporally Rewiring Networks: A Model-based Approach},
  Author                   = {F. Guo and S. Hanneke and W. Fu and } # epxing,
  Booktitle                = {Proc. of ICML},
  Year                     = {2007},

  Address                  = {Corvalis, Oregon},
  Pages                    = {321--328},

  ISBN                     = {978-1-59593-793-3}
}

@TechReport{Guo2011Asymptotic,
  author =      {Jian Guo and } # elevina #{ and George Michailidis and } # jzhu,
  title =       {Asymptotic Properties of the Joint Neighborhood Selection Method for Estimating Categorical Markov Networks},
  institution = {University of Michigan},
  year =        {2011},
  file =        {:Guo2011Asymptotic.pdf:PDF},
  journal =     {arXiv preprint math.PR/0000000}
}

@Article{guo10joint,
  author =    {J. Guo and } # elevina #{ and G. Michailidis and J. Zhu},
  title =     {Joint Structure Estimation For Categorical Markov Networks},
  journal =   {Unpublished manuscript},
  year =      {2010},
  file =      {:guo10joint.pdf:PDF},
  newspaper = {Unpublished manuscript}
}

@Article{Guo2011Joint,
  Title                    = {Joint Estimation Of Multiple Graphical Models},
  Author                   = {J. Guo and } # elevina # { and G. Michailidis and J. Zhu},
  Journal                  = {Biometrika},
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {1-15},
  Volume                   = {98},

  Newspaper                = {Biometrika}
}

@Article{Guo2015Estimating,
  author =   {Guo, Jian and Cheng, Jie and Levina, Elizaveta and Michailidis, George and Zhu, Ji},
  title =    {Estimating heterogeneous graphical models for discrete data with an application to roll call voting},
  journal =  aoas_s,
  year =     {2015},
  volume =   {9},
  number =   {2},
  pages =    {821--848},
  doi =      {10.1214/13-AOAS700},
  file =     {Guo2015Estimating.pdf:Guo2015Estimating.pdf:PDF},
  fjournal = {The Annals of Applied Statistics},
  issn =     {1932-6157},
  mrclass =  {62-09 (62G05 62H30 62J07 62P25)},
  mrnumber = {3371337},
  url =      {http://dx.doi.org/10.1214/13-AOAS700}
}

@Article{Gutenbrunner1992Regression,
  author =     {C. Gutenbrunner and J. Jure{\v{c}}kov{\'a}},
  title =      {Regression rank scores and regression quantiles},
  journal =    aos_s,
  year =       {1992},
  volume =     {20},
  number =     {1},
  pages =      {305--330},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1176348524},
  file =       {Gutenbrunner1992Regression.pdf:Gutenbrunner1992Regression.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G05 (62J05)},
  mrnumber =   {1150346 (93c:62069)},
  mrreviewer = {Gutti J. Babu},
  url =        {http://dx.doi.org/10.1214/aos/1176348524}
}

@Article{Gutmann2012Noise,
  Title                    = {Noise-contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics},
  Author                   = {Michael U. Gutmann and Aapo Hyv\"{a}rinen},
  Journal                  = jmlr_s,
  Year                     = {2012},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {307--361},
  Volume                   = {13},

  Acmid                    = {2188396},
  ISSN                     = {1532-4435},
  Issue_date               = {January 2012},
  Keywords                 = {computation, estimation, natural image statistics, partition function, unnormalized models},
  Numpages                 = {55},
  Publisher                = {JMLR.org},
  Url                      = {http://dl.acm.org/citation.cfm?id=2503308.2188396}
}

@Article{Hoefling2009Estimation,
  Title                    = {Estimation of sparse binary pairwise markov networks using pseudo-likelihoods},
  Author                   = {Holger H{\"o}fling and } # rtibs,
  Journal                  = jmlr_s,
  Year                     = {2009},
  Pages                    = {883--906},
  Volume                   = {10},

  Owner                    = {mkolar},
  Publisher                = {JMLR. org},
  Timestamp                = {2014.05.05},
  Url                      = {http://dl.acm.org/citation.cfm?id=1577101}
}

@Article{Han2013Sparse,
  Title                    = {Sparse Median Graphs Estimation in a High Dimensional Semiparametric Model},
  Author                   = {Fang Han and } # hliu # { and } # bcaffo,
  Journal                  = {ArXiv e-prints, arXiv:1310.3223},
  Year                     = {2013},

  Month                    = oct,

  Abstract                 = {In this manuscript a unified framework for conducting inference on complex aggregated data in high dimensional settings is proposed. The data are assumed to be a collection of multiple non-Gaussian realizations with underlying undirected graphical structures. Utilizing the concept of median graphs in summarizing the commonality across these graphical structures, a novel semiparametric approach to modeling such complex aggregated data is provided along with robust estimation of the median graph, which is assumed to be sparse. The estimator is proved to be consistent in graph recovery and an upper bound on the rate of convergence is given. Experiments on both synthetic and real datasets are conducted to illustrate the empirical usefulness of the proposed models and methods.},
  Eprint                   = {1310.3223},
  Oai2identifier           = {1310.3223}
}

@Article{Han2013Optimal,
  Title                    = {Optimal Rates of Convergence for Latent Generalized Correlation Matrix Estimation in Transelliptical Distribution},
  Author                   = {Fang Han and Han Liu},
  Journal                  = {ArXiv e-prints, arXiv:1305.6916},
  Year                     = {2013},

  Month                    = may,

  Abstract                 = {Correlation matrix plays a key role in many multivariate methods (e.g., graphical model estimation and factor analysis). The current state-of-the-art in estimating large correlation matrices focuses on the use of Pearson's sample correlation matrix. Although Pearson's sample correlation matrix enjoys various good properties under Gaussian models, its not an effective estimator when facing heavy-tail distributions with possible outliers. As a robust alternative, \cite{han2012transelliptical} advocated the use of a transformed version of the Kendall's tau sample correlation matrix in estimating high dimensional latent generalized correlation matrix under the transelliptical distribution family (or elliptical copula). The transelliptical family assumes that after unspecified marginal monotone transformations, the data follow an elliptical distribution. In this paper, we study the theoretical properties of the Kendall's tau sample correlation matrix and its transformed version proposed in \cite{han2012transelliptical} for estimating the population Kendall's tau correlation matrix and the latent Pearson's correlation matrix under both spectral and restricted spectral norms. With regard to the spectral norm, we highlight the role of "effective rank" in quantifying the rate of convergence. With regard to the restricted spectral norm, we for the first time present a "sign subgaussian condition" which is sufficient to guarantee that the rank-based correlation matrix estimator attains the optimal rate of convergence. In both cases, we do not need any moment condition.},
  Comments                 = {34 pages},
  Eprint                   = {1305.6916},
  Oai2identifier           = {1305.6916}
}

@Article{Han2014Distribution,
  author =         {Fang Han and Han Liu},
  title =          {Distribution-Free Tests of Independence with Applications to Testing More Structures},
  journal =        {ArXiv e-prints, arXiv:1410.4179},
  year =           {2014},
  month =          oct,
  abstract =       {We consider the problem of testing mutual independence of all entries in a d-dimensional random vector X=(X_1,...,X_d)^T based on n independent observations. For this, we consider two families of distribution-free test statistics that converge weakly to an extreme value type I distribution. We further study the powers of the corresponding tests against certain alternatives. In particular, we show that the powers tend to one when the maximum magnitude of the pairwise Pearson's correlation coefficients is larger than C(log d/n)^{1/2} for some absolute constant C. This result is rate optimal. As important examples, we show that the tests based on Kendall's tau and Spearman's rho are rate optimal tests of independence. For further generalization, we consider accelerating the rate of convergence via approximating the exact distributions of the test statistics. We also study the tests of two more structural hypotheses: m-dependence and data homogeneity. For these, we propose two rank-based tests and show their optimality.},
  comments =       {45 pages},
  eprint =         {1410.4179},
  file =           {:Han2014Distribution.pdf:PDF},
  oai2identifier = {1410.4179}
}

@Article{xingSNA06,
  Title                    = {Discrete Temporal Models Of Social Networks},
  Author                   = {S. Hanneke and W. Fu and } # epxing,
  Journal                  = ejs_s,
  Year                     = {2010},
  Pages                    = {585--605},
  Volume                   = {4},

  Doi                      = {10.1214/09-EJS548},
  ISSN                     = {1935-7524},
  Mrnumber                 = {2660534 (2011h:62011)},
  Newspaper                = {Electron. J. Stat.},
  Url                      = {http://dx.doi.org/10.1214/09-EJS548}
}

@Article{Hara2013Learning,
  Title                    = {Learning a common substructure of multiple graphical Gaussian models},
  Author                   = {Satoshi Hara and Takashi Washio},
  Journal                  = {Neural Networks},
  Year                     = {2013},
  Pages                    = {23--38},
  Volume                   = {38},

  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S089360801200281X}
}

@InProceedings{harchaoui09kernel,
  author =    {Z. Harchaoui and } # fbach #{ and E. Moulines},
  title =     {Kernel Change-point Analysis},
  booktitle = {Proc. of NIPS},
  year =      {2009},
  editor =    {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  pages =     {609--616}
}

@Article{harchaoui10multiple,
  Title                    = {Multiple Change-point Estimation With A Total Variation Penalty},
  Author                   = {Z. Harchaoui and C. L{\'e}vy-Leduc},
  Journal                  = jasa_s,
  Year                     = {2010},
  Number                   = {492},
  Pages                    = {1480--1493},
  Volume                   = {105},

  Doi                      = {10.1198/jasa.2010.tm09181},
  ISSN                     = {0162-1459},
  Mrnumber                 = {2796565 (2012g:62368)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Url                      = {http://dx.doi.org/10.1198/jasa.2010.tm09181}
}

@Article{Hardt2013Understanding,
  author =         {Moritz Hardt},
  title =          {Understanding Alternating Minimization for Matrix Completion},
  year =           {2013},
  month =          dec,
  abstract =       {Alternating Minimization is a widely used and empirically successful heuristic for matrix completion and related low-rank optimization problems. Theoretical guarantees for Alternating Minimization have been hard to come by and are still poorly understood. This is in part because the heuristic is iterative and non-convex in nature. We give a new algorithm based on Alternating Minimization that provably recovers an unknown low-rank matrix from a random subsample of its entries under a standard incoherence assumption. Our results reduce the sample size requirements of the Alternating Minimization approach by at least a quartic factor in the rank and the condition number of the unknown matrix. These improvements apply even if the matrix is only close to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in the dimension of the matrix and, in a broad range of parameters, gives the strongest sample bounds among all subquadratic time algorithms that we are aware of. Underlying our work is a new robust convergence analysis of the well-known Power Method for computing the dominant singular vectors of a matrix. This viewpoint leads to a conceptually simple understanding of Alternating Minimization. In addition, we contribute a new technique for controlling the coherence of intermediate solutions arising in iterative algorithms based on a smoothed analysis of the QR factorization. These techniques may be of interest beyond their application here.},
  comments =       {Slightly improved main theorem and a correction: The tail bound stated in Lemma A.5 of the previous version is incorrect. See manuscript for fix},
  eprint =         {1312.0925},
  file =           {:Hardt2013Understanding.pdf:PDF},
  oai2identifier = {1312.0925}
}

@Article{hartley71analysis,
  Title                    = {The Analysis Of Incomplete Data},
  Author                   = {H.~O. Hartley and R.~R. Hocking},
  Journal                  = {Biometrics},
  Year                     = {1971},
  Number                   = {4},
  Pages                    = {783--823},
  Volume                   = {27},

  Newspaper                = {Biometrics},
  Publisher                = {JSTOR}
}

@Article{harvey76estimating,
  Title                    = {Estimating Regression Models With Multiplicative Heteroscedasticity},
  Author                   = {A.~C. Harvey},
  Journal                  = {Econometrica},
  Year                     = {1976},
  Number                   = {3},
  Pages                    = {461--465},
  Volume                   = {44},

  ISSN                     = {0012-9682},
  Mrnumber                 = {0411063 (53 \#14802)},
  Newspaper                = {Econometrica}
}

@Article{harville1974bayesian,
  Title                    = {Bayesian Inference For Variance Components Using Only Error Contrasts},
  Author                   = {D.~A. Harville},
  Journal                  = {Biometrika},
  Year                     = {1974},
  Number                   = {2},
  Pages                    = {383--385},
  Volume                   = {61},

  Newspaper                = {Biometrika},
  Publisher                = {Biometrika Trust}
}

@Article{Haslbeck2015Structure,
  author =         {Jonas M. B. Haslbeck and Lourens J. Waldorp},
  title =          {Structure estimation for mixed graphical models in high-dimensional data},
  journal =        {ArXiv e-prints, arXiv:1510.05677},
  year =           {2015},
  month =          oct,
  abstract =       {Undirected graphical models are a key component in the analysis of complex observational data in a large variety of disciplines. In many of these applications one is interested in estimating the undirected graphical model underlying a distribution over variables with different domains. Despite the pervasive need for such an estimation method, to date there is no such method that models all variables on their proper domain. We close this methodological gap by combining a new class of mixed graphical models with a structure estimation approach based on generalized covariance matrices. We report the performance of our methods using simulations, illustrate the method with a dataset on Autism Spectrum Disorder (ASD) and provide an implementation as an R-package.},
  eprint =         {1510.05677},
  file =           {:Haslbeck2015Structure.pdf:PDF},
  oai2identifier = {1510.05677}
}

@Article{He2012Convergence,
  author =    {He, Bingsheng and Yuan, Xiaoming},
  title =     {On the $O(1/n)$ Convergence Rate of the Douglas-Rachford Alternating Direction Method},
  journal =   {SIAM Journal on Numerical Analysis},
  year =      {2012},
  volume =    {50},
  number =    {2},
  pages =     {700--709},
  file =      {:He2012Convergence.pdf:PDF},
  publisher = {SIAM}
}

@Article{Hedden2009Disruption,
  Title                    = {Disruption Of Functional Connectivity In Clinically Normal Older Adults Harboring Amyloid Burden },
  Author                   = {T. Hedden and K.~R.~A.~V. Dijk and J.~A. Becker and A. Mehta and R.~A. Sperling and K.~A. Johnson and R.~L. Buckner},
  Journal                  = {J. Neurosci.},
  Year                     = {2009},
  Number                   = {40},
  Pages                    = {12686--12694},
  Volume                   = {29},

  Newspaper                = {J. Neurosci.}
}

@Unpublished{pollard93,
  Title                    = {Asymptotics for minimizers of convex processes},
  Author                   = {Nils Lid Hjort and David Pollard},
  Year                     = {1993}
}

@Article{hocking68estimation,
  Title                    = {Estimation Of Parameters In The Multivariate Normal Distribution With Missing Observations},
  Author                   = {R.~R. Hocking and W.~B. Smith},
  Journal                  = jasa_s,
  Year                     = {1968},
  Pages                    = {159--173},
  Volume                   = {63},

  ISSN                     = {0162-1459},
  Mrnumber                 = {0226784 (37 \#2371)},
  Newspaper                = {J. Am. Stat. Assoc.}
}

@Article{Hoeffding1963Probability,
  Title                    = {Probability inequalities for sums of bounded random variables},
  Author                   = {Wassily Hoeffding},
  Journal                  = jasa_s,
  Year                     = {1963},
  Pages                    = {13--30},
  Volume                   = {58},

  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {60.20},
  Mrnumber                 = {0144363 (26 \#1908)},
  Mrreviewer               = {E. H. Lehman, Jr.}
}

@Article{Hong2012linear,
  author =  {Hong, Mingyi and Luo, Zhi-Quan},
  title =   {On the linear convergence of the alternating direction method of multipliers},
  journal = {ArXiv e-prints, arXiv:1208.3922},
  year =    {2012},
  file =    {:Hong2012linear.pdf:PDF}
}

@InProceedings{Honorio2010Multi,
  Title                    = {Multi-task Learning Of {gaussian} Graphical Models},
  Author                   = {J. Honorio and D. Samaras},
  Booktitle                = {Proc. of ICML},
  Year                     = {2010},

  Address                  = {Haifa, Israel},
  Editor                   = {Johannes F{\"u}rnkranz and Thorsten Joachims},
  Month                    = {June},
  Pages                    = {447--454},
  Publisher                = {Omnipress},

  Monthno                  = {6},
  Url                      = {http://www.icml2010.org/papers/35.pdf}
}

@InProceedings{Hsieh2011,
  Title                    = {Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation},
  Author                   = {C.-J. Hsieh and M.~A. Sustik and I.~S. Dhillon and P. Ravikumar},
  Booktitle                = {Proc. of NIPS},
  Year                     = {2011},
  Pages                    = {2330--2338},
  Publisher                = {http://nips.cc/},

  Url                      = {http:\www.cs.utexas.edu/users/sustik/QUIC}
}

@Article{Hsu2012tail,
  Title                    = {A tail inequality for quadratic forms of subgaussian random vectors},
  Author                   = {Daniel Hsu and Sham Kakade and } # tzhang,
  Journal                  = ecp_s,
  Year                     = {2012},
  Number                   = {52},
  Pages                    = {1-6},
  Volume                   = {17},

  Abstract                 = {This article proves an exponential probability tail inequality for positive semidefinite quadratic forms in a subgaussian random vector. The bound is analogous to one that holds when the vector has independent Gaussian entries.},
  Doi                      = {10.1214/ECP.v17-2079},
  Fjournal                 = {Electronic Communications in Probability},
  ISSN                     = {1083-589X},
  Keywords                 = {Tail inequality; quadratic form; subgaussian random vectors; subgaussian chaos},
  Url                      = {http://ecp.ejpecp.org/article/view/2079}
}

@Article{Hu2016review,
  author =         {Jiang Hu and Zhidong Bai},
  title =          {A review of 20 years of naive tests of significance for high-dimensional mean vectors and covariance matrices},
  journal =        {ArXiv e-prints, arXiv:1603.01003},
  year =           {2016},
  month =          mar,
  abstract =       {In this paper, we will introduce the so called naive tests and give a brief review on the newly development. Naive testing methods are easy to understand and performs robust especially when the dimension is large. In this paper, we mainly focus on reviewing some naive testing methods for the mean vectors and covariance matrices of high dimensional populations and believe this naive test idea can be wildly used in many other testing problems.},
  eprint =         {1603.01003},
  file =           {:Hu2016review.pdf:PDF},
  oai2identifier = {1603.01003}
}

@Article{Hu2012Adaptive,
  Title                    = {Adaptive semi-varying coefficient model selection},
  Author                   = {Tao Hu and Yingcun Xia},
  Journal                  = statsin_s,
  Year                     = {2012},

  Month                    = {Apr},
  Number                   = {2},
  Volume                   = {22},

  Doi                      = {10.5705/ss.2010.105},
  Owner                    = {mkolar},
  Publisher                = {Institute of Statistical Science, Academia Sinica},
  Timestamp                = {2014.02.13},
  Url                      = {http://dx.doi.org/10.5705/ss.2010.105}
}

@Article{Huskova1993Consistency,
  author =     {Hu{\v{s}}kov{\'a}, Marie and Janssen, Paul},
  title =      {Consistency of the generalized bootstrap for degenerate {$U$}-statistics},
  journal =    {Ann. Statist.},
  year =       {1993},
  volume =     {21},
  number =     {4},
  pages =      {1811--1823},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1176349399},
  file =       {Huskova1993Consistency.pdf:Huskova1993Consistency.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62E20 (60F05 62G09)},
  mrnumber =   {1245770},
  mrreviewer = {Gutti J. Babu},
  url =        {http://dx.doi.org/10.1214/aos/1176349399}
}

@Article{Huang2015Distributed,
  author =         {Cheng Huang and Xiaoming Huo},
  title =          {A Distributed One-Step Estimator},
  journal =        {ArXiv e-prints, arXiv:1511.01443},
  year =           {2015},
  month =          nov,
  abstract =       {Distributed statistical inference has recently attracted enormous attention. Many existing work focuses on the averaging estimator. We propose a one-step approach to enhance a simple-averaging based distributed estimator. We derive the corresponding asymptotic properties of the newly proposed estimator. We find that the proposed one-step estimator enjoys the same asymptotic properties as the centralized estimator. The proposed one-step approach merely requires one additional round of communication in relative to the averaging estimator; so the extra communication burden is insignificant. In finite sample cases, numerical examples show that the proposed estimator outperforms the simple averaging estimator with a large margin in terms of the mean squared errors. A potential application of the one-step approach is that one can use multiple machines to speed up large scale statistical inference with little compromise in the quality of estimators. The proposed method becomes more valuable when data can only be available at distributed machines with limited communication bandwidth.},
  comments =       {31 pages},
  eprint =         {1511.01443},
  file =           {:Huang2015Distributed.pdf:PDF},
  oai2identifier = {1511.01443}
}

@Article{Huang2010benefit,
  Title                    = {The benefit of group sparsity},
  Author                   = {Junzhou Huang and } # tzhang,
  Journal                  = aos_s,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1978--2004},
  Volume                   = {38},

  Doi                      = {10.1214/09-AOS778},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62J05 62J07)},
  Mrnumber                 = {2676881 (2011f:62029)},
  Mrreviewer               = {Yuehua Wu},
  Url                      = {http://dx.doi.org/10.1214/09-AOS778}
}

@Article{Huang2011Learning,
  Title                    = {Learning with structured sparsity},
  Author                   = {Junzhou Huang and } # tzhang # { and Dimitris Metaxas},
  Journal                  = jmlr_s,
  Year                     = {2011},
  Pages                    = {3371--3412},
  Volume                   = {12},

  Owner                    = {mkolar},
  Publisher                = {JMLR. org},
  Timestamp                = {2014.02.18},
  Url                      = {http://dl.acm.org/citation.cfm?id=2078213}
}

@Article{huang08asymptotic,
  Title                    = {Asymptotic Properties Of Bridge Estimators In Sparse High-dimensional Regression Models},
  Author                   = {J. Huang and J.~L. Horowitz and S. Ma},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {587--613},
  Volume                   = {36},

  Doi                      = {10.1214/009053607000000875},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2396808 (2009g:62094)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/009053607000000875}
}

@Article{huang08adaptive,
  Title                    = {Adaptive {l}asso For Sparse High-dimensional Regression Models},
  Author                   = {J. Huang and S. Ma and } # chzhang,
  Journal                  = statsin,
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {1603--1618},
  Volume                   = {18},

  ISSN                     = {1017-0405},
  Mrnumber                 = {2469326 (2010a:62214)},
  Newspaper                = {Stat. Sinica}
}

@Article{Huang2004Polynomial,
  Title                    = {Polynomial spline estimation and inference for varying coefficient models with longitudinal data},
  Author                   = {Jianhua Z. Huang and Colin O. Wu and Lan Zhou},
  Journal                  = statsin_s,
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {763--788},
  Volume                   = {14},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62G05 (62G20 62J10)},
  Mrnumber                 = {2087972 (2005k:62090)},
  Mrreviewer               = {Stefano Maria Iacus}
}

@InProceedings{Huang2009Learning,
  Title                    = {Learning Brain Connectivity Of Alzheimer's Disease From Neuroimaging Data},
  Author                   = {S. Huang and J. Li and L. Sun and J. Liu and T. Wu and K. Chen and A. Fleisher and E. Reiman and J. Ye},
  Booktitle                = {Proc. of NIPS},
  Year                     = {2009},
  Editor                   = {Y. Bengio and D. Schuurmans and John D. Lafferty and C. K. I. Williams and A. Culotta},
  Pages                    = {808--816}
}

@InProceedings{husmeier10Intertime,
  Title                    = {Inter-time Segment Information Sharing For Non-homogeneous Dynamic Bayesian Networks},
  Author                   = {D. Husmeier and F. Dondelinger and S. L\'{e}bre},
  Booktitle                = {Proc. of NIPS},
  Year                     = {2010},
  Editor                   = {John D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.~S. Zemel and A. Culotta},
  Pages                    = {901--909}
}

@Article{hyvarinen2005estimation,
  Title                    = {Estimation of Non-Normalized Statistical Models by Score Matching},
  Author                   = {Aapo Hyv{\"a}rinen},
  Journal                  = jmlr_s,
  Year                     = {2005},
  Pages                    = {695--709},
  Volume                   = {6}
}

@Article{Hyvaerinen2007Some,
  Title                    = {Some extensions of score matching},
  Author                   = {Aapo Hyv{\"a}rinen},
  Journal                  = csda_s,
  Year                     = {2007},
  Number                   = {5},
  Pages                    = {2499--2512},
  Volume                   = {51},

  Owner                    = {mkolar},
  Publisher                = {Elsevier},
  Timestamp                = {2014.05.02},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947306003264}
}

@Article{Jacho-Chavez2010Identification,
  author =   {Jacho-Ch{\'a}vez, David and Lewbel, Arthur and Linton, Oliver},
  title =    {Identification and nonparametric estimation of a transformed additively separable model},
  journal =  {J. Econometrics},
  year =     {2010},
  volume =   {156},
  number =   {2},
  pages =    {392--407},
  coden =    {JECMB6},
  doi =      {10.1016/j.jeconom.2009.11.008},
  file =     {:Jacho-Chavez2010Identification.pdf:PDF},
  fjournal = {Journal of Econometrics},
  issn =     {0304-4076},
  mrclass =  {62G05 (62G08 62G20 62P20)},
  mrnumber = {2609941},
  url =      {http://dx.doi.org/10.1016/j.jeconom.2009.11.008}
}

@InProceedings{Jaggi2013Revisiting,
  author =    {Jaggi, Martin},
  title =     {Revisiting Frank-Wolfe: Projection-free sparse convex optimization},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
  year =      {2013},
  pages =     {427--435},
  file =      {:Jaggi2013Revisiting.pdf:PDF}
}

@InProceedings{Jaggi2014Communication,
  author =    {Jaggi, Martin and Smith, Virginia and Tak{\'a}c, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I},
  title =     {Communication-efficient distributed dual coordinate ascent},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2014},
  pages =     {3068--3076},
  file =      {:Jaggi2014Communication.pdf:PDF}
}

@InProceedings{Jalali2011learning,
  author =    {Jalali, Ali and } # pravik #{ and Vasuki, Vishvas and Sanghavi, Sujay},
  title =     {On learning discrete graphical models using group-sparse regularization},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year =      {2011},
  pages =     {378--387}
}

@InProceedings{Jalali2010dirty,
  author =    {Ali Jalali and Sujay Sanghavi and Chao Ruan and } # pravik,
  title =     {A dirty model for multi-task learning},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2010},
  pages =     {964--972},
  file =      {:Jalali2010dirty.pdf:PDF}
}

@Article{Jankova2014Confidence,
  author     = {Jankov\'{a}, Jana and van de Geer, Sara},
  title      = {Confidence intervals for high-dimensional inverse covariance estimation},
  journal    = {Electron. J. Stat.},
  year       = {2015},
  volume     = {9},
  number     = {1},
  pages      = {1205--1229},
  issn       = {1935-7524},
  doi        = {10.1214/15-EJS1031},
  fjournal   = {Electronic Journal of Statistics},
  mrclass    = {62J07 (62F12)},
  mrnumber   = {3354336},
  mrreviewer = {Tianxiao Pang},
  url        = {https://doi.org/10.1214/15-EJS1031},
}

@Article{Janofsky2015Exponential,
  author         = {Eric Janofsky},
  title          = {Exponential Series Approaches for Nonparametric Graphical Models},
  journal        = {ArXiv e-prints, arXiv:1506.03537},
  year           = {2015},
  month          = jun,
  abstract       = {This thesis studies high-dimensional, continuous-valued pairwise Markov Random Fields. We are particularly interested in approximating pairwise densities whose logarithm belongs to a Sobolev space. For this problem we propose the method of exponential series [Crain, 1974; Barron and Sheu, 1991], which approximates the log density by a finite- dimensional exponential family with the number of sufficient statistics increasing with the sample size. We consider two approaches to estimating these models. The first is regularized maximum likelihood. This involves optimizing the sum of the log-likelihood of the data and a sparsity-inducing regularizer. We provide consistency and edge selection guarantees for this method. We then propose a variational approximation to the likelihood based on tree- reweighted, nonparametric message passing. We then consider estimation using regularized score matching. This approach uses an alternative scoring rule to the log-likelihood, which obviates the need to compute the normalizing constant of the distribution. For general continuous-valued exponential families, we provide parameter and edge consistency results. We then describe results for model selection in the nonparametric pairwise model using exponential series. The regularized score matching problem is shown to be a convex program; we provide scalable algorithms based on consensus Alternating Direction Method of Multipliers (ADMM, [Boyd et al., 2011]) and Coordinate-wise Descent. We compare our method to others in the literature as well as the aforementioned TRW estimator using simulated data.},
  comments       = {Thesis},
  eprint         = {1506.03537},
  file           = {:Janofsky2015Exponential.pdf:PDF},
  oai2identifier = {1506.03537},
}

@Article{janson87maximal,
  Title                    = {Maximal Spacings In Several Dimensions},
  Author                   = {S. Janson},
  Journal                  = {Ann. Probab.},
  Year                     = {1987},
  Number                   = {1},
  Pages                    = {274--280},
  Volume                   = {15},

  Newspaper                = {Ann. Probab.}
}

@Article{Janssen1994Weighted,
  author =     {Janssen, Paul},
  title =      {Weighted bootstrapping of {$U$}-statistics},
  journal =    {J. Statist. Plann. Inference},
  year =       {1994},
  volume =     {38},
  number =     {1},
  pages =      {31--41},
  coden =      {JSPIDN},
  doi =        {10.1016/0378-3758(92)00156-X},
  file =       {Janssen1994Weighted.pdf:Janssen1994Weighted.pdf:PDF},
  fjournal =   {Journal of Statistical Planning and Inference},
  issn =       {0378-3758},
  mrclass =    {62E20 (60F05 62G09)},
  mrnumber =   {1256846},
  mrreviewer = {Zuzana Pr{\'a}{\v{s}}kov{\'a}},
  url =        {http://dx.doi.org/10.1016/0378-3758(92)00156-X}
}

@Article{Javanmard2013Hypothesis,
  Title                    = {Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory},
  Author                   = {Adel Javanmard and Andrea Montanari},
  Journal                  = {arXiv preprint arXiv:1301.4240},
  Year                     = {2013},

  Timestamp                = {2013.10.25},
  Url                      = {http://arxiv.org/abs/1301.4240}
}

@Article{Javanmard2013Nearly,
  Title                    = {Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional Regression},
  Author                   = {Adel Javanmard and Andrea Montanari},
  Journal                  = {arXiv preprint arXiv:1311.0274},
  Year                     = {2013},

  Month                    = nov,

  Abstract                 = {We consider the problem of fitting the parameters of a high-dimensional linear regression model. In the regime where the number of parameters $p$ is comparable to or exceeds the sample size $n$, a successful approach uses an $\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately, unlike for linear estimators (e.g., ordinary least squares), no well-established method exists to compute confidence intervals or p-values on the basis of the Lasso estimator. Very recently, a line of work \cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this problem by constructing a debiased version of the Lasso estimator. In this paper, we study this approach for random design model, under the assumption that a good estimator exists for the precision matrix of the design. Our analysis improves over the state of the art in that it establishes nearly optimal \emph{average} testing power if the sample size $n$ asymptotically dominates $s_0 (\log p)^2$, with $s_0$ being the sparsity level (number of non-zero coefficients). Earlier work obtains provable guarantees only for much larger sample size, namely it requires $n$ to asymptotically dominate $(s_0 \log p)^2$. In particular, for random designs with a sparse precision matrix we show that an estimator thereof having the required properties can be computed efficiently. Finally, we evaluate this approach on synthetic data and compare it with earlier proposals.},
  Comments                 = {21 pages, short version appears in Annual Allerton Conference on Communication, Control and Computing, 2013},
  Eprint                   = {1311.0274},
  Oai2identifier           = {1311.0274}
}

@Article{Javanmard2013Confidence,
  author =  {Adel Javanmard and Andrea Montanari},
  title =   {Confidence Intervals and Hypothesis Testing for High-Dimensional Regression},
  journal = jmlr_s,
  year =    {2014},
  volume =  {15},
  number =  {Oct},
  pages =   {2869-2909},
  file =    {:Javanmard2013Confidence.pdf:PDF},
  url =     {http://jmlr.org/papers/v15/javanmard14a.html}
}

@Article{Jeon2006effective,
  author    = {Yongho Jeon and Yi Lin},
  title     = {An effective method for high-dimensional log-density ANOVA estimation, with application to nonparametric graphical model building},
  journal   = statsin_s,
  year      = {2006},
  volume    = {16},
  number    = {2},
  pages     = {353--374},
  file      = {:Jeon2006effective.pdf:PDF},
  publisher = {C/O DR HC HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN},
}

@Article{ji10upsdelivers,
  Title                    = {U{ps} Delivers Optimal Phase Diagram In High-dimensional Variable Selection},
  Author                   = {P. Ji and J. Jin},
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {73--103},
  Volume                   = {40},

  Doi                      = {10.1214/11-AOS947},
  ISSN                     = {0090-5364},
  Mrnumber                 = {3013180},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/11-AOS947}
}

@InProceedings{Ji2009accelerated,
  author =       {Ji, Shuiwang and Ye, Jieping},
  title =        {An accelerated gradient method for trace norm minimization},
  booktitle =    {Proceedings of the 26th annual international conference on machine learning},
  year =         {2009},
  pages =        {457--464},
  organization = {ACM},
  file =         {:Ji2009accelerated.pdf:PDF}
}

@Article{yi10constructing,
  Title                    = {Constructing Non-stationary Dynamic Bayesian Networks With A Flexible Lag Choosing Mechanism},
  Author                   = {Y. Jia and J. Huan},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Number                   = {Suppl 6},
  Pages                    = {S27},
  Volume                   = {11},

  Doi                      = {10.1186/1471-2105-11-S6-S27},
  ISSN                     = {1471-2105},
  Newspaper                = {BMC Bioinformatics},
  Url                      = {http://www.biomedcentral.com/1471-2105/11/S6/S27}
}

@InProceedings{jiang12bayesian,
  Title                    = {A Bayesian Markov-switching Model For Sparse Dynamic Network Estimation},
  Author                   = {Huijing Jiang and } # alozano # { and Fei Liu},
  Booktitle                = {Proc. 2012 SIAM Int. Conf. Data Mining},
  Year                     = {2012},
  Pages                    = {506--515},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Johnson2012High,
  author =    {C. Johnson and A. Jalali and } # pravik,
  title =     {High-dimensional Sparse Inverse Covariance Estimation Using Greedy Methods},
  booktitle = {Proc. of AISTATS},
  year =      {2012},
  editor =    {Neil Lawrence and Mark Girolami },
  pages =     {574--582},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Book{Johnson2004Information,
  title =      {Information theory and the central limit theorem},
  publisher =  {Imperial College Press, London},
  year =       {2004},
  author =     {Johnson, Oliver},
  doi =        {10.1142/9781860945373},
  isbn =       {1-86094-473-6},
  mrclass =    {60-02 (46L54 60B15 60E07 60F05 94-02 94A15)},
  mrnumber =   {2109042},
  mrreviewer = {Riccardo Gatto},
  pages =      {xiv+209},
  url =        {http://dx.doi.org/10.1142/9781860945373}
}

@Article{Johnston1982Probabilities,
  Title                    = {Probabilities of maximal deviations for nonparametric regression function estimates},
  Author                   = {Gordon J. Johnston},
  Journal                  = jma_s,
  Year                     = {1982},
  Number                   = {3},
  Pages                    = {402--414},
  Volume                   = {12},

  Coden                    = {JMVAAI},
  Doi                      = {10.1016/0047-259X(82)90074-4},
  Fjournal                 = {Journal of Multivariate Analysis},
  ISSN                     = {0047-259X},
  Mrclass                  = {62G05 (62J05)},
  Mrnumber                 = {666014 (83k:62057)},
  Mrreviewer               = {G{\'e}rard Collomb},
  Url                      = {http://dx.doi.org/10.1016/0047-259X(82)90074-4}
}

@Article{Johnstone2004Needles,
  author =     {Johnstone, Iain M. and Silverman, Bernard W.},
  title =      {Needles and straw in haystacks: empirical {B}ayes estimates of possibly sparse sequences},
  journal =    aos_s,
  year =       {2004},
  volume =     {32},
  number =     {4},
  pages =      {1594--1649},
  coden =      {ASTSC7},
  doi =        {10.1214/009053604000000030},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62C12 (62G05 62G08)},
  mrnumber =   {2089135},
  mrreviewer = {Fran{\c{c}}oise Garcia-Brouaye},
  url =        {http://dx.doi.org/10.1214/009053604000000030}
}

@Article{Juditsky2012Accuracy,
  Title                    = {Accuracy guaranties for {$\ell\sb 1$} recovery of block-sparse
 signals},
  Author                   = {Anatoli Juditsky and Fatma {Kilin{\c{c}} Karzan} and Arkadi Nemirovski and Boris Polyak},
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {3077--3107},
  Volume                   = {40},

  Doi                      = {10.1214/12-AOS1057},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {94A12 (62G08 62H12 62J07)},
  Mrnumber                 = {3097970},
  Mrreviewer               = {Patrick L. Combettes},
  Url                      = {http://dx.doi.org/10.1214/12-AOS1057}
}

@Article{Jureckova2010Rank,
  author =    {Jana Jure{\v{c}}kov{\'{a}} and Jan Picek and A.K.Md. Ehsanes Saleh},
  title =     {Rank tests and regression rank score tests in measurement error models},
  journal =   {Computational Statistics {\&} Data Analysis},
  year =      {2010},
  volume =    {54},
  number =    {12},
  pages =     {3108--3120},
  month =     {dec},
  doi =       {10.1016/j.csda.2009.08.020},
  file =      {Jureckova2010Rank.pdf:Jureckova2010Rank.pdf:PDF},
  owner =     {mkolar},
  publisher = {Elsevier {BV}},
  timestamp = {2016.03.02},
  url =       {http://dx.doi.org/10.1016/j.csda.2009.08.020}
}

@Book{Jureckova2012Methodology,
  title =     {Methodology in Robust and Nonparametric Statistics},
  publisher = {CRC Press},
  year =      {2012},
  author =    {Jana Jure{\v{c}}kov{\'a} and Pranab Kumar Sen and Jan Picek}
}

@Article{Kai2011New,
  Title                    = {New efficient estimation and variable selection methods for
 semiparametric varying-coefficient partially linear models},
  Author                   = {Bo Kai and } # rli # { and } # hzhou,
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {305--332},
  Volume                   = {39},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/10-AOS842},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62F12 62G08 62G20 62P10)},
  Mrnumber                 = {2797848 (2012d:62092)},
  Mrreviewer               = {Yong Ming Li},
  Url                      = {http://dx.doi.org/10.1214/10-AOS842}
}

@Article{Kainkaryam2009Pooling,
  Title                    = {Pooling in high-throughput drug screening},
  Author                   = {Raghunandan M. Kainkaryam and Peter J. Woolf},
  Journal                  = {Current opinion in drug discovery \& development},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {339--350},
  Volume                   = {12},

  Owner                    = {mkolar},
  Publisher                = {NIH Public Access},
  Timestamp                = {2014.02.18}
}

@Article{Kaiser1997Modeling,
  Title                    = {Modeling Poisson variables with positive spatial dependence},
  Author                   = {Mark S Kaiser and Noel Cressie},
  Journal                  = statprobl_s,
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {423--432},
  Volume                   = {35},

  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167715297000412}
}

@Article{Kakade2009Learning,
  author =  {Kakade, Sham M and Shamir, Ohad and Sridharan, Karthik and Tewari, Ambuj},
  title =   {Learning exponential families in high-dimensions: Strong convexity and sparsity},
  journal = {arXiv preprint arXiv:0911.0054},
  year =    {2009},
  file =    {:Kakade2009Learning.pdf:PDF}
}

@Article{Kalisch2007Estimating,
  author =    {Markus Kalisch and } # pbuhl,
  title =     {Estimating high-dimensional directed acyclic graphs with the PC-algorithm},
  journal =   jmlr_s,
  year =      {2007},
  volume =    {8},
  pages =     {613--636},
  file =      {:Kalisch2007Estimating.pdf:PDF},
  publisher = {JMLR. org}
}

@Article{Kang2007Demystifying,
  author   = {Kang, Joseph D. Y. and Schafer, Joseph L.},
  title    = {Demystifying double robustness: a comparison of alternative strategies for estimating a population mean from incomplete data},
  journal  = statsci_s,
  year     = {2007},
  volume   = {22},
  number   = {4},
  pages    = {523--539},
  issn     = {0883-4237},
  doi      = {10.1214/07-STS227},
  file     = {:Kang2007Demystifying.pdf:PDF},
  fjournal = {Statistical Science. A Review Journal of the Institute of Mathematical Statistics},
  mrclass  = {Database Expansion Item},
  mrnumber = {2420458},
  url      = {http://dx.doi.org/10.1214/07-STS227},
}

@Article{Katenka2011Multi,
  Title                    = {Multi-attribute Networks And The Impact Of Partial Information On Inference And Characterization},
  Author                   = {N. Katenka and E.~D. Kolaczyk},
  Journal                  = aoas_s,
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {1068--1094},
  Volume                   = {6},

  Newspaper                = {Ann. Appl. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Kato2012Two,
  Title                    = {Two-step estimation of high dimensional additive models},
  Author                   = {Kengo Kato},
  Journal                  = {ArXiv e-prints, arXiv:1207.5313},
  Year                     = {2012},

  Month                    = jul,

  Abstract                 = {This paper investigates the two-step estimation of a high dimensional additive regression model, in which the number of nonparametric additive components is potentially larger than the sample size but the number of significant additive components is sufficiently small. The approach investigated consists of two steps. The first step implements the variable selection, typically by the group Lasso, and the second step applies the penalized least squares estimation with Sobolev penalties to the selected additive components. Such a procedure is computationally simple to implement and, in our numerical experiments, works reasonably well. Despite its intuitive nature, the theoretical properties of this two-step procedure have to be carefully analyzed, since the effect of the first step variable selection is random, and generally it may contain redundant additive components and at the same time miss significant additive components. This paper derives a generic performance bound on the two-step estimation procedure allowing for these situations, and studies in detail the overall performance when the first step variable selection is implemented by the group Lasso.},
  Comments                 = {49 pages, 3 tables; minor errors corrected},
  Eprint                   = {1207.5313},
  Oai2identifier           = {1207.5313},
  Owner                    = {mkolar},
  Timestamp                = {2014.11.25}
}

@InCollection{kerkyacharian2009learning,
  Title                    = {Learning Out Of Leaders},
  Author                   = {G. Kerkyacharian and M. Mougeot and D. Picard and K. Tribouley},
  Booktitle                = {Multiscale, nonlinear and adaptive approximation},
  Publisher                = {Springer},
  Year                     = {2009},

  Address                  = {Berlin},
  Pages                    = {295--324},

  Doi                      = {10.1007/978-3-642-03413-8_9},
  Mrnumber                 = {2648377 (2011k:62114)},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-03413-8_9}
}

@Article{Khare2013convex,
  author   = {Khare, Kshitij and Oh, Sang-Yun and Rajaratnam, Bala},
  title    = {A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence guarantees},
  journal  = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year     = {2015},
  volume   = {77},
  number   = {4},
  pages    = {803--825},
  issn     = {1369-7412},
  doi      = {10.1111/rssb.12088},
  file     = {Khare2013convex.pdf:Khare2013convex.pdf:PDF},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62H99 (62F12 62H12 62J07)},
  mrnumber = {3382598},
  url      = {http://dx.doi.org/10.1111/rssb.12088},
}

@Article{Kim2016Sparse,
  author =   {Minyoung Kim},
  title =    {Sparse Conditional Copula Models for Structured Output Regression },
  journal =  {Pattern Recognition },
  year =     {2016},
  pages =    { - },
  abstract = {Abstract We deal with the multiple output regression task where the central theme is to capture the sparse output correlation among the output variables. Sparse inverse covariance learning of linear Gaussian conditional models has been recently studied, shown to achieve superb prediction performance. However, it can fail when the underlying true input-output process is non-Gaussian and/or non-linear. We introduce a novel sparse conditional copula model to represent the joint density of the output variables. By incorporating a Gaussian copula function, yet modeling univariate marginal densities by (non-Gaussian) mixtures of experts, we achieve high flexibility in representation that admits non-linear and non-Gaussian densities. We then propose a sparse learning method for this copula-based model that effectively imposes sparsity in the conditional dependency among output variables. The learning optimization is efficient as it can be decomposed into gradient-descent marginal density estimation and the sparse inverse covariance learning for the copula function. Improved performance of the proposed approach is demonstrated on several interesting image/vision tasks with high dimensions. },
  doi =      {http://dx.doi.org/10.1016/j.patcog.2016.03.027},
  file =     {Kim2016Sparse.pdf:Kim2016Sparse.pdf:PDF},
  issn =     {0031-3203},
  keywords = {Multiple Output Regression},
  url =      {http://www.sciencedirect.com/science/article/pii/S0031320316300127}
}

@Article{kim09statistical,
  Title                    = {Statistical Estimation Of Correlated Genome Associations To A Quantitative Trait Network},
  Author                   = {S. Kim and } # epxing,
  Journal                  = {PLoS genetics},
  Year                     = {2009},
  Number                   = {8},
  Pages                    = {e1000587},
  Volume                   = {5},

  Newspaper                = {PLoS genetics},
  Owner                    = {mkolar},
  Publisher                = {Public Library of Science},
  Timestamp                = {2014.02.18}
}

@InProceedings{Kim2010Tree,
  author =    {Seyoung Kim and } # epxing,
  title =     {Tree-guided group lasso for multi-task regression with structured sparsity},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year =      {2010},
  pages =     {543--550},
  file =      {Kim2010Tree.pdf:Kim2010Tree.pdf:PDF}
}

@Article{kim09multivariate,
  Title                    = {A Multivariate Regression Approach To Association Analysis Of A Quantitative Trait Network},
  Author                   = {S. Kim and K.-A. Sohn and } # epxing,
  Journal                  = {Bioinformatics},
  Year                     = {2009},
  Number                   = {12},
  Pages                    = {i204--i212},
  Volume                   = {25},

  Newspaper                = {Bioinformatics},
  Owner                    = {mkolar},
  Publisher                = {Oxford Univ Press},
  Timestamp                = {2014.02.18}
}

@Article{Kim2012Tree,
  author =   {Kim, Seyoung and Xing, Eric P.},
  title =    {Tree-guided group lasso for multi-response regression with structured sparsity, with an application to {EQTL} mapping},
  journal =  {Ann. Appl. Stat.},
  year =     {2012},
  volume =   {6},
  number =   {3},
  pages =    {1095--1117},
  doi =      {10.1214/12-AOAS549},
  file =     {Kim2012Tree.pdf:Kim2012Tree.pdf:PDF},
  fjournal = {The Annals of Applied Statistics},
  issn =     {1932-6157},
  mrclass =  {62J07 (62-04 62P10 92B10)},
  mrnumber = {3012522},
  url =      {http://dx.doi.org/10.1214/12-AOAS549}
}

@Article{Kim2008Smoothly,
  Title                    = {Smoothly clipped absolute deviation on high dimensions},
  Author                   = {Yongdai Kim and Hosik Choi and Hee-Seok Oh},
  Journal                  = jasa_s,
  Year                     = {2008},
  Number                   = {484},
  Pages                    = {1665--1673},
  Volume                   = {103},

  Owner                    = {mkolar},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2013.10.15},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1198/016214508000001066}
}

@Article{Kim2012Global,
  Title                    = {Global optimality of nonconvex penalized estimators},
  Author                   = {Yongdai Kim and Sunghoon Kwon},
  Journal                  = {Biometrika},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {315--325},
  Volume                   = {99},

  Owner                    = {mkolar},
  Publisher                = {Biometrika Trust},
  Timestamp                = {2013.10.15},
  Url                      = {http://biomet.oxfordjournals.org/content/99/2/315.short}
}

@Article{Klueppelberg2008Semi,
  Title                    = {Semi-Parametric Models for the Multivariate Tail Dependence Function--the Asymptotically Dependent Case},
  Author                   = {Claudia Kl{\"u}ppelberg and Gabriel Kuhn and Liang Peng},
  Journal                  = sjs_s,
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {701--718},
  Volume                   = {35},

  Publisher                = {Wiley Online Library},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2008.00602.x/full}
}

@Article{Klopp2013Sparse,
  author     = {Klopp, Olga and Pensky, Marianna},
  title      = {Sparse high-dimensional varying coefficient model: nonasymptotic minimax study},
  journal    = {Ann. Statist.},
  year       = {2015},
  volume     = {43},
  number     = {3},
  pages      = {1273--1299},
  issn       = {0090-5364},
  file       = {:Klopp2013Sparse.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62C20 62J05)},
  mrnumber   = {3346703},
  mrreviewer = {Liwen Xu},
  url        = {https://doi.org/10.1214/15-AOS1309},
}

@Article{Knight1998Limiting,
  author =    {Keith Knight},
  title =     {Limiting distributions for $L_1$ regression estimators under general conditions},
  journal =   aos_s,
  year =      {1998},
  volume =    {26},
  number =    {2},
  pages =     {755--770},
  doi =       {10.1214/aos/1028144858},
  file =      {Knight1998Limiting.pdf:Knight1998Limiting.pdf:PDF},
  publisher = {Institute of Mathematical Statistics - care of Project Euclid},
  url =       {http://dx.doi.org/10.1214/aos/1028144858}
}

@Article{Knight2000Asymptotics,
  Title                    = {Asymptotics for lasso-type estimators},
  Author                   = {Keith Knight and Wenjiang Fu},
  Journal                  = aos_s,
  Year                     = {2000},
  Number                   = {5},
  Pages                    = {1356--1378},
  Volume                   = {28},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/aos/1015957397},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J07 (60F05 62E20 62J05)},
  Mrnumber                 = {1805787 (2002a:62099)},
  Mrreviewer               = {Alexander G. Kukush},
  Url                      = {http://dx.doi.org/10.1214/aos/1015957397}
}

@Article{Koenker1984note,
  Title                    = {A note on {$L$}-estimates for linear models},
  Author                   = {Roger Koenker},
  Journal                  = statprobl_s,
  Year                     = {1984},
  Number                   = {6},
  Pages                    = {323--325},
  Volume                   = {2},

  Coden                    = {SPLTDC},
  Doi                      = {10.1016/0167-7152(84)90040-3},
  Fjournal                 = {Statistics \& Probability Letters},
  ISSN                     = {0167-7152},
  Mrclass                  = {62J05 (62E20)},
  Mrnumber                 = {782652 (86m:62125)},
  Mrreviewer               = {Huynh Huynh},
  Url                      = {http://dx.doi.org/10.1016/0167-7152(84)90040-3}
}

@Book{Koenker2005Quantile,
  Title                    = {Quantile regression},
  Author                   = {Roger Koenker},
  Publisher                = {Cambridge University Press, Cambridge},
  Year                     = {2005},
  Series                   = {Econometric Society Monographs},
  Volume                   = {38},

  Doi                      = {10.1017/CBO9780511754098},
  ISBN                     = {978-0-521-60827-5; 0-521-60827-9},
  Mrclass                  = {62-02 (62J02 62J05)},
  Mrnumber                 = {2268657 (2008h:62015)},
  Mrreviewer               = {M. Hu{\v{s}}kov{\'a}},
  Pages                    = {xvi+349},
  Url                      = {http://dx.doi.org/10.1017/CBO9780511754098}
}

@Article{Koenker1978Regression,
  Title                    = {Regression quantiles},
  Author                   = {Koenker, Roger and Bassett, Jr., Gilbert},
  Journal                  = {Econometrica},
  Year                     = {1978},
  Number                   = {1},
  Pages                    = {33--50},
  Volume                   = {46},

  Fjournal                 = {Econometrica. Journal of the Econometric Society},
  ISSN                     = {0012-9682},
  Mrclass                  = {62J05 (62G35)},
  Mrnumber                 = {0474644 (57 \#14279)},
  Mrreviewer               = {P. J. Huber}
}

@Article{Koenker2002Inference,
  author =     {Koenker, Roger and Xiao, Zhijie},
  title =      {Inference on the quantile regression process},
  journal =    {Econometrica},
  year =       {2002},
  volume =     {70},
  number =     {4},
  pages =      {1583--1612},
  coden =      {ECMTA7},
  doi =        {10.1111/1468-0262.00342},
  file =       {Koenker2002Inference.pdf:Koenker2002Inference.pdf:PDF},
  fjournal =   {Econometrica. Journal of the Econometric Society},
  issn =       {0012-9682},
  mrclass =    {62F10 (62G10)},
  mrnumber =   {1929979},
  mrreviewer = {Friedrich Liese},
  url =        {http://dx.doi.org/10.1111/1468-0262.00342}
}

@Article{koh07interior,
  Title                    = {An Interior-point Method For Large-scale {$l\sb 1$}-regularized Logistic Regression},
  Author                   = {K. Koh and S.-J. Kim and S.~P. Boyd},
  Journal                  = jmlr_s,
  Year                     = {2007},
  Pages                    = {1519--1555},
  Volume                   = {8},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2332440},
  Newspaper                = {J. Mach. Learn. Res.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{kolaczyk2009statistical,
  Title                    = {Statistical Analysis Of Network Data},
  Author                   = {E.~D. Kolaczyk},
  Publisher                = {Springer},
  Year                     = {2009},

  Address                  = {New York},
  Note                     = {Methods and models},
  Series                   = {Springer Series in Statistics},

  Doi                      = {10.1007/978-0-387-88146-1},
  ISBN                     = {978-0-387-88145-4},
  Mrnumber                 = {2724362 (2011j:05308)},
  Owner                    = {mkolar},
  Pages                    = {xii+386},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1007/978-0-387-88146-1}
}

@InProceedings{kolar13road,
  Title                    = {Feature Selection in High-Dimensional Classification},
  Author                   = {Mladen Kolar and Han Liu},
  Booktitle                = {JMLR W$\&$CP: } # PROC_s # { } # ICML2013_s,
  Year                     = {2013},
  Pages                    = {100--108},
  Volume                   = {28},

  Editors                  = {Sanjoy Dasgupta and David McAllester},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{Koller2009Probabilistic,
  Title                    = {Probabilistic graphical models: principles and techniques},
  Author                   = {Daphne Koller and Nir Friedman},
  Publisher                = {MIT press},
  Year                     = {2009}
}

@Article{Koltchinskii1994Komlos,
  author =     {Koltchinskii, Vladimir I.},
  title =      {Komlos-{M}ajor-{T}usnady approximation for the general empirical process and {H}aar expansions of classes of functions},
  journal =    {J. Theoret. Probab.},
  year =       {1994},
  volume =     {7},
  number =     {1},
  pages =      {73--118},
  coden =      {JTPREO},
  doi =        {10.1007/BF02213361},
  file =       {:Koltchinskii1994Komlos.pdf:PDF},
  fjournal =   {Journal of Theoretical Probability},
  issn =       {0894-9840},
  mrclass =    {60F17 (60F10 60F15 60G15)},
  mrnumber =   {1256392},
  mrreviewer = {M. Cs{\"o}rg{\H{o}}},
  url =        {http://dx.doi.org/10.1007/BF02213361}
}

@Article{Komlos1975approximation,
  author =     {Koml{\'o}s, J. and Major, P. and Tusn{\'a}dy, G.},
  title =      {An approximation of partial sums of independent {${\rm RV}$}'s and the sample {${\rm DF}$}. {I}},
  journal =    {Z. Wahrscheinlichkeitstheorie und Verw. Gebiete},
  year =       {1975},
  volume =     {32},
  pages =      {111--131},
  file =       {Komlos1975approximation.pdf:Komlos1975approximation.pdf:PDF},
  mrclass =    {60B10 (60G50)},
  mrnumber =   {0375412},
  mrreviewer = {J. Kiefer}
}

@Unpublished{Kozbur2013Inference,
  Title                    = {Inference In Additively Separable Models With A High Dimensional Component},
  Author                   = {Damian Kozbur},
  Note                     = {Job Market Paper},
  Year                     = {2013}
}

@Article{Krishnamurthy2013Recovering,
  Title                    = {Recovering Graph-Structured Activations using Adaptive Compressive Measurements},
  Author                   = {Akshay Krishnamurthy and James Sharpnack and Aarti Singh},
  Journal                  = {arXiv preprint arXiv:1305.0213},
  Year                     = {2013},

  Month                    = may,

  Abstract                 = {We study the localization of a cluster of activated vertices in a graph, from adaptively designed compressive measurements. We propose a hierarchical partitioning of the graph that groups the activated vertices into few partitions, so that a top-down sensing procedure can identify these partitions, and hence the activations, using few measurements. By exploiting the cluster structure, we are able to provide localization guarantees at weaker signal to noise ratios than in the unstructured setting. We complement this performance guarantee with an information theoretic lower bound, providing a necessary signal-to-noise ratio for any algorithm to successfully localize the cluster. We verify our analysis with some simulations, demonstrating the practicality of our algorithm.},
  Eprint                   = {1305.0213},
  Oai2identifier           = {1305.0213},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Kudo1963multivariate,
  Title                    = {A multivariate analogue of the one-sided test},
  Author                   = {Kud{\^o}, Akio},
  Journal                  = {Biometrika},
  Year                     = {1963},
  Pages                    = {403--418},
  Volume                   = {50},

  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {62.25},
  Mrnumber                 = {0163386 (29 \#689)},
  Mrreviewer               = {H. D. Brunk}
}

@Article{lebre10statistical,
  Title                    = {Statistical Inference Of The Time-varying Structure Of Gene-regulation Networks},
  Author                   = {S. L\'{e}bre and J. Becq and F. Devaux and M. Stumpf and G. Lelandais},
  Journal                  = {BMC Systems Biology},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {130},
  Volume                   = {4},

  Doi                      = {10.1186/1752-0509-4-130},
  ISSN                     = {1752-0509},
  Newspaper                = {BMC Systems Biology},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{Laan2003Unified,
  title =      {Unified methods for censored longitudinal data and causality},
  publisher =  {Springer-Verlag, New York},
  year =       {2003},
  author =     {van der Laan, Mark J. and Robins, James M.},
  series =     {Springer Series in Statistics},
  doi =        {10.1007/978-0-387-21700-0},
  isbn =       {0-387-95556-9},
  mrclass =    {62-02 (62H12 62N01 62N02)},
  mrnumber =   {1958123 (2003m:62003)},
  mrreviewer = {P. Rochus},
  pages =      {xii+396},
  url =        {http://dx.doi.org/10.1007/978-0-387-21700-0}
}

@Article{Laber2011Adaptive,
  Title                    = {Adaptive confidence intervals for the test error in classification},
  Author                   = {Eric B. Laber and Susan A. Murphy},
  Journal                  = jasa_s,
  Year                     = {2011},
  Number                   = {495},
  Volume                   = {106},

  Owner                    = {mkolar},
  Timestamp                = {2013.10.25},
  Url                      = {http://amstat.tandfonline.com/doi/full/10.1198/jasa.2010.tm10053}
}

@InProceedings{Lafferty2001Conditional,
  author =    {John D. Lafferty and Andrew McCallum and Fernando C. N. Pereira},
  title =     {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
  booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning {(ICML} 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001},
  year =      {2001},
  editor =    {Carla E. Brodley and Andrea Pohoreckyj Danyluk},
  pages =     {282--289},
  publisher = {Morgan Kaufmann},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp.uni-trier.de/rec/bib/conf/icml/LaffertyMP01},
  file =      {:Lafferty2001Conditional.pdf:PDF},
  timestamp = {Wed, 27 Nov 2002 10:53:35 +0100}
}

@Article{Lam2009Sparsistency,
  author    = {C. Lam and } # jfan,
  title     = {Sparsistency And Rates Of Convergence In Large Covariance Matrix Estimation},
  journal   = aos_s,
  year      = {2009},
  volume    = {37},
  pages     = {4254--4278},
  file      = {:Lam2009Sparsistency.pdf:PDF},
  newspaper = {Ann. Stat.},
  owner     = {mkolar},
  timestamp = {2018.04.30},
}

@Article{lancaster1968grouping,
  Title                    = {Grouping Estimators On Heteroscedastic Data},
  Author                   = {T. Lancaster},
  Journal                  = jasa_s,
  Year                     = {1968},
  Pages                    = {182--191},
  Volume                   = {63},

  ISSN                     = {0162-1459},
  Mrnumber                 = {0224224 (36 \#7268)},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Lapin2014Scalable,
  author =    {Lapin, Maksim and Schiele, Bernt and Hein, Matthias},
  title =     {Scalable multitask representation learning for scene classification},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year =      {2014},
  pages =     {1434--1441}
}

@Article{Laurent00adaptive,
  author =    {B. Laurent and P. Massart},
  title =     {Adaptive Estimation Of A Quadratic Functional By Model Selection},
  journal =   aos_s,
  year =      {2000},
  volume =    {28},
  number =    {5},
  pages =     {1302--1338},
  doi =       {10.1214/aos/1015957395},
  file =      {Laurent00adaptive.pdf:Laurent00adaptive.pdf:PDF},
  issn =      {0090-5364},
  mrnumber =  {1805785 (2002c:62052)},
  newspaper = {Ann. Stat.},
  owner =     {mkolar},
  timestamp = {2014.02.18},
  url =       {http://dx.doi.org/10.1214/aos/1015957395}
}

@Book{Ledoux2005Concentration,
  Title                    = {The Concentration of Measure Phenomenon (Mathematical Surveys \& Monographs)},
  Author                   = {Michel Ledoux},
  Publisher                = {American Mathematical Society},
  Year                     = {2005},

  ISBN                     = {0821837923},
  Url                      = {http://www.amazon.com/Concentration-Measure-Phenomenon-Mathematical-Monographs/dp/0821837923%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0821837923}
}

@Book{Ledoux1991Probability,
  Title                    = {Probability in Banach Spaces: Isoperimetry and Processes},
  Author                   = {Michel Ledoux and Michel Talagrand},
  Publisher                = {Springer},
  Year                     = {1991},

  ISBN                     = {3540520139},
  Url                      = {http://www.amazon.com/Probability-Banach-Spaces-Isoperimetry-Grenzgebiete/dp/3540520139%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D3540520139}
}

@Article{Lee2015Distributed,
  author =         {Jason D. Lee and Qihang Lin and Tengyu Ma and Tianbao Yang},
  title =          {Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity},
  journal =        {ArXiv e-prints, arXiv:1507.07595},
  year =           {2015},
  month =          jul,
  abstract =       {We study distributed optimization algorithms for minimizing the average of convex functions. The applications include empirical risk minimization problems in statistical machine learning where the datasets are large and have to be stored on different machines. We design a distributed stochastic variance reduced gradient algorithm that, under certain conditions on the condition number, simultaneously achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. Our method and its accelerated extension also outperform existing distributed algorithms in terms of the rounds of communication as long as the condition number is not too large compared to the size of data in each machine. We also prove a lower bound for the number of rounds of communication for a broad class of distributed first-order methods including the proposed algorithms in this paper. We show that our accelerated distributed stochastic variance reduced gradient algorithm achieves this lower bound so that it uses the fewest rounds of communication among all distributed first-order algorithms.},
  comments =       {significant addition to both theory and experimental results},
  eprint =         {1507.07595},
  file =           {:Lee2015Distributed.pdf:PDF},
  oai2identifier = {1507.07595}
}

@Article{Lee2016Variable,
  author   = {Lee, Kuang-Yao and Li, Bing and Zhao, Hongyu},
  title    = {Variable selection via additive conditional independence},
  journal  = JRSSB_s,
  year     = {2016},
  pages    = {n/a--n/a},
  issn     = {1467-9868},
  doi      = {10.1111/rssb.12150},
  file     = {Lee2016Variable.pdf:Lee2016Variable.pdf:PDF;:Lee2016Variable_supp.pdf:PDF},
  keywords = {Additive covariance operator, Heterogeneity, Lasso, Regression operator, Reproducing kernel Hilbert space, Sparsity, Variable selection consistency},
  url      = {http://dx.doi.org/10.1111/rssb.12150},
}

@Article{Lee2012Leveraging,
  author =    {Seunghak Lee and } # epxing,
  title =     {Leveraging input and output structures for joint mapping of epistatic and marginal eQTLs},
  journal =   {Bioinformatics},
  year =      {2012},
  volume =    {28},
  number =    {12},
  pages =     {i137--i146},
  file =      {:Lee2012Leveraging.pdf:PDF},
  publisher = {Oxford Univ Press}
}

@InProceedings{Lee2006Efficient,
  Title                    = {Efficient Structure Learning of Markov Networks using $\ell_1$-Regularization},
  Author                   = {Su-In Lee and Varun Ganapathi and Daphne Koller},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2006},
  Pages                    = {817--824},

  Owner                    = {mkolar},
  Timestamp                = {2014.05.05}
}

@Article{Lee2012Multiple,
  author =   {Lee, Wonyul and Du, Ying and Sun, Wei and Hayes, David Neil and Liu, Yufeng},
  title =    {Multiple response regression for {G}aussian mixture models with known labels},
  journal =  {Stat. Anal. Data Min.},
  year =     {2012},
  volume =   {5},
  number =   {6},
  pages =    {493--508},
  doi =      {10.1002/sam.11158},
  file =     {Lee2012Multiple.pdf:Lee2012Multiple.pdf:PDF},
  fjournal = {Statistical Analysis and Data Mining},
  issn =     {1932-1864},
  mrclass =  {62J07 (62H12 62J05 62P10)},
  mrnumber = {3001288},
  url =      {http://dx.doi.org/10.1002/sam.11158}
}

@Article{Lee2012Simultaneous,
  author =   {Lee, Wonyul and Liu, Yufeng},
  title =    {Simultaneous multiple response regression and inverse covariance matrix estimation via penalized {G}aussian maximum likelihood},
  journal =  {J. Multivariate Anal.},
  year =     {2012},
  volume =   {111},
  pages =    {241--255},
  doi =      {10.1016/j.jmva.2012.03.013},
  file =     {Lee2012Simultaneous.pdf:Lee2012Simultaneous.pdf:PDF},
  fjournal = {Journal of Multivariate Analysis},
  issn =     {0047-259X},
  mrclass =  {62H12},
  mrnumber = {2944419},
  url =      {http://dx.doi.org/10.1016/j.jmva.2012.03.013}
}

@Article{Lenk1996Hierarchical,
  author =    {Lenk, Peter J and DeSarbo, Wayne S and Green, Paul E and Young, Martin R},
  title =     {Hierarchical Bayes conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs},
  journal =   {Marketing Science},
  year =      {1996},
  volume =    {15},
  number =    {2},
  pages =     {173--191},
  file =      {:Lenk1996Hierarchical.pdf:PDF},
  publisher = {INFORMS}
}

@Article{Leonard1978Density,
  Title                    = {Density estimation, stochastic processes and prior
 information},
  Author                   = {Leonard, Tom},
  Journal                  = JRSSB_s,
  Year                     = {1978},
  Note                     = {With discussion},
  Number                   = {2},
  Pages                    = {113--146},
  Volume                   = {40},

  Coden                    = {JSTBAJ},
  Fjournal                 = {Journal of the Royal Statistical Society. Series B.
 Methodological},
  ISSN                     = {0035-9246},
  Mrclass                  = {62G05},
  Mrnumber                 = {517434 (80h:62031)},
  Mrreviewer               = {Wolfgang Wertz},
  Url                      = {http://links.jstor.org/sici?sici=0035-9246(1978)40:2<113:DESPAP>2.0.CO;2-B&origin=MSN}
}

@Article{Leucht2012Degenerate,
  author =     {Leucht, Anne},
  title =      {Degenerate {$U$}- and {$V$}-statistics under weak dependence: asymptotic theory and bootstrap consistency},
  journal =    {Bernoulli},
  year =       {2012},
  volume =     {18},
  number =     {2},
  pages =      {552--585},
  doi =        {10.3150/11-BEJ354},
  file =       {:Leucht2012Degenerate.pdf:PDF},
  fjournal =   {Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability},
  issn =       {1350-7265},
  mrclass =    {62G09 (62G10 62G20 62M10)},
  mrnumber =   {2922461},
  mrreviewer = {Simos G. Meintanis},
  url =        {http://dx.doi.org/10.3150/11-BEJ354}
}

@Article{Leucht2013Dependent,
  author =   {Leucht, Anne and Neumann, Michael H.},
  title =    {Dependent wild bootstrap for degenerate {$U$}- and {$V$}-statistics},
  journal =  jma_s,
  year =     {2013},
  volume =   {117},
  pages =    {257--280},
  doi =      {10.1016/j.jmva.2013.03.003},
  file =     {Leucht2013Dependent.pdf:Leucht2013Dependent.pdf:PDF},
  fjournal = {Journal of Multivariate Analysis},
  issn =     {0047-259X},
  mrclass =  {62M07 (62E20 62G09 62M10)},
  mrnumber = {3053547},
  url =      {http://dx.doi.org/10.1016/j.jmva.2013.03.003}
}

@Article{Leung2015Testing,
  author =         {Dennis Leung and Mathias Drton},
  title =          {Testing independence in high dimensions with sums of squares of rank correlations},
  journal =        {ArXiv e-prints, arXiv:1501.01732},
  year =           {2015},
  month =          jan,
  abstract =       {We treat the problem of testing independence between $m$ continuous observations when the available sample size $n$ is comparable to $m$. Making no specific distributional assumptions, we consider two related classes of test statistics. Statistics of the first considered type are formed by summing up the squares of all pairwise sample rank correlations. Statistics of the second type are U-statistics that unbiasedly estimate the expected sum of squared rank correlations. In the asymptotic regime where the ratio $m/n$ converges to a positive constant, a martingale central limit theorem is applied to show that the null distributions of these statistics converge to Gaussian limits. Using the framework of U-statistics, our result covers a variety of rank correlations including Kendall's tau and a dominating term of Spearman's rank correlation coefficient (rho), but also degenerate U-statistics such as Hoeffding's $D$, or the $\tau^*$ of Bergsma and Dassios (2014). For degenerate statistics, the asymptotic variance of the test statistics involves a fourth moment of the kernel that does not appear in classical U-statistic theory. The power of the considered tests is explored in rate-optimality theory under a Gaussian equicorrelation alternative as well as in numerical experiments for specific cases of more general alternatives.},
  eprint =         {1501.01732},
  file =           {:Leung2015Testing.pdf:PDF},
  oai2identifier = {1501.01732}
}

@Article{Li2012Sparse,
  author =    {Bing Li and Hyonho Chun and Hongyu Zhao},
  title =     {Sparse Estimation of Conditional Graphical Models With Application to Gene Networks},
  journal =   jasa_s,
  year =      {2012},
  volume =    {107},
  number =    {497},
  pages =     {152-167},
  month =     {Mar},
  doi =       {10.1080/01621459.2011.644498},
  file =      {Li2012Sparse.pdf:Li2012Sparse.pdf:PDF},
  owner =     {mkolar},
  publisher = {Informa UK Limited},
  timestamp = {2014.02.13},
  url =       {http://dx.doi.org/10.1080/01621459.2011.644498}
}

@Article{Li2014additive,
  author   = {Li, Bing and Chun, Hyonho and Zhao, Hongyu},
  title    = {On an additive semigraphoid model for statistical networks with application to pathway analysis},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2014},
  volume   = {109},
  number   = {507},
  pages    = {1188--1204},
  issn     = {0162-1459},
  doi      = {10.1080/01621459.2014.882842},
  file     = {:Li2014additive.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62H99 (62G05)},
  mrnumber = {3265690},
  url      = {http://dx.doi.org/10.1080/01621459.2014.882842},
}

@Article{Li2015Joint,
  author =         {Danning Li and Lingzhou Xue},
  title =          {Joint limiting laws for high-dimensional independence tests},
  journal =        {ArXiv e-prints, arXiv:1512.08819},
  year =           {2015},
  month =          dec,
  abstract =       {Testing independence is of significant interest in many important areas of large-scale inference. Using extreme-value form statistics to test against sparse alternatives and using quadratic form statistics to test against dense alternatives are two important testing procedures for high-dimensional independence. However, quadratic form statistics suffer from low power against sparse alternatives, and extreme-value form statistics suffer from low power against dense alternatives with small disturbances and may have size distortions due to its slow convergence. For real-world applications, it is important to derive powerful testing procedures against more general alternatives. Based on intermediate limiting distributions, we derive (model-free) joint limiting laws of extreme-value form and quadratic form statistics, and surprisingly, we prove that they are asymptotically independent. Given such asymptotic independencies, we propose (model-free) testing procedures to boost the power against general alternatives and also retain the correct asymptotic size. Under the high-dimensional setting, we derive the closed-form limiting null distributions, and obtain their explicit rates of uniform convergence. We prove their consistent statistical powers against general alternatives. We demonstrate the performance of our proposed test statistics in simulation studies. Our work provides very helpful insights to high-dimensional independence tests, and fills an important gap.},
  comments =       {31 pages},
  eprint =         {1512.08819},
  file =           {:Li2015Joint.pdf:PDF},
  oai2identifier = {1512.08819}
}

@Article{Li2006Gradient,
  Title                    = {Gradient Directed Regularization For Sparse Gaussian Concentration Graphs, With Applications To Inference Of Genetic Networks},
  Author                   = {H. Li and J. Gui},
  Journal                  = {Biostatistics},
  Year                     = {2006},
  Number                   = {2},
  Pages                    = {302--317},
  Volume                   = {7},

  Newspaper                = {Biostatistics},
  Owner                    = {mkolar},
  Publisher                = {Biometrika Trust},
  Timestamp                = {2014.02.18}
}

@Article{Li2012Two,
  author =     {Li, Jun and Chen, Song Xi},
  title =      {Two sample tests for high-dimensional covariance matrices},
  journal =    aos_s,
  year =       {2012},
  volume =     {40},
  number =     {2},
  pages =      {908--940},
  doi =        {10.1214/12-AOS993},
  file =       {Li2012Two.pdf:Li2012Two.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62H15 (62G10 62G20)},
  mrnumber =   {2985938},
  mrreviewer = {Alessio Farcomeni},
  url =        {http://dx.doi.org/10.1214/12-AOS993}
}

@Article{Li2015Large,
  author =    {Mu Li and Wei Bi and James T. Kwok and Bao-Liang Lu},
  title =     {Large-Scale Nystr\"om Kernel Matrix Approximation Using Randomized {SVD}},
  journal =   {{IEEE} Trans. Neural Netw. Learning Syst.},
  year =      {2015},
  volume =    {26},
  number =    {1},
  pages =     {152--164},
  month =     {jan},
  doi =       {10.1109/tnnls.2014.2359798},
  file =      {Li2015Large.pdf:Li2015Large.pdf:PDF},
  publisher = {Institute of Electrical {\&} Electronics Engineers ({IEEE})},
  url =       {http://dx.doi.org/10.1109/TNNLS.2014.2359798}
}

@Book{Li2006Nonparametric,
  Title                    = {Nonparametric Econometrics: Theory and Practice},
  Author                   = {Qi Li and Jeffrey Scott Racine},
  Publisher                = {Princeton University Press},
  Year                     = {2006},

  ISBN                     = {0691121613},
  Owner                    = {mkolar},
  Timestamp                = {2014.05.27},
  Url                      = {http://www.amazon.com/Nonparametric-Econometrics-Practice-Qi-Li/dp/0691121613%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0691121613}
}

@Article{li08variable,
  Title                    = {Variable Selection In Semiparametric Regression Modeling},
  Author                   = {R. Li and H. Liang},
  Journal                  = aos_s,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {261--286},
  Volume                   = {36},

  Doi                      = {10.1214/009053607000000604},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2387971 (2009g:62051)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/009053607000000604}
}

@Article{Li2008L,
  Title                    = {{$L\sb 1$}-norm quantile regression},
  Author                   = {Youjuan Li and Ji Zhu},
  Journal                  = jcgs_s,
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {163--185},
  Volume                   = {17},

  Doi                      = {10.1198/106186008X289155},
  Fjournal                 = {Journal of Computational and Graphical Statistics},
  ISSN                     = {1061-8600},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2424800},
  Url                      = {http://dx.doi.org/10.1198/106186008X289155}
}

@Article{Liang2015equivalent,
  author =   {Liang, Faming and Song, Qifan and Qiu, Peihua},
  title =    {An equivalent measure of partial correlation coefficients for high-dimensional {G}aussian graphical models},
  journal =  jasa_s,
  year =     {2015},
  volume =   {110},
  number =   {511},
  pages =    {1248--1265},
  file =     {Liang2015equivalent.pdf:Liang2015equivalent.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {62H99 (62H20)},
  mrnumber = {3420699}
}

@Article{Lin2015High,
  author     = {Lina Lin and } # mdrton #{ and Ali Shojaie},
  title      = {Estimation of high-dimensional graphical models using regularized score matching},
  journal    = {Electron. J. Stat.},
  year       = {2016},
  volume     = {10},
  number     = {1},
  pages      = {806--854},
  issn       = {1935-7524},
  doi        = {10.1214/16-EJS1126},
  file       = {:Lin2015High.pdf:PDF},
  fjournal   = {Electronic Journal of Statistics},
  mrclass    = {62H12 (62F12)},
  mrnumber   = {3486418},
  mrreviewer = {Emanuel H. Ben-David},
  timestamp  = {2018.05.01},
  url        = {https://doi.org/10.1214/16-EJS1126},
}

@Article{Lin2006Component,
  Title                    = {Component selection and smoothing in multivariate nonparametric regression},
  Author                   = {Yi Lin and } # hhzhang,
  Journal                  = aos_s,
  Year                     = {2006},
  Number                   = {5},
  Pages                    = {2272--2297},
  Volume                   = {34},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/009053606000000722},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62G20 62J07)},
  Mrnumber                 = {2291500 (2008f:62055)},
  Mrreviewer               = {Miguel A. Delgado},
  Url                      = {http://dx.doi.org/10.1214/009053606000000722}
}

@Article{Lindskog2003Kendalls,
  Title                    = {Kendall's Tau for Elliptical Distributions},
  Author                   = {Filip Lindskog and Alexander McNeil and Uwe Schmock},
  Journal                  = {Credit Risk},
  Year                     = {2003},
  Pages                    = {149--156},

  Doi                      = {10.1007/978-3-642-59365-9_8},
  ISBN                     = {http://id.crossref.org/isbn/978-3-642-59365-9},
  ISSN                     = {1431-1933},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-59365-9_8}
}

@Article{Linton2008Estimation,
  author =     {Linton, Oliver and Sperlich, Stefan and Van Keilegom, Ingrid},
  title =      {Estimation of a semiparametric transformation model},
  journal =    {Ann. Statist.},
  year =       {2008},
  volume =     {36},
  number =     {2},
  pages =      {686--718},
  coden =      {ASTSC7},
  doi =        {10.1214/009053607000000848},
  file =       {:Linton2008Estimation.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62E20 (62F12 62G05 62G08)},
  mrnumber =   {2396812},
  mrreviewer = {Emanuele Taufer},
  url =        {http://dx.doi.org/10.1214/009053607000000848}
}

@Article{little88test_missing,
  Title                    = {A Test Of Missing Completely At Random For Multivariate Data With Missing Values},
  Author                   = {R.~J.~A. Little},
  Journal                  = jasa_s,
  Year                     = {1988},
  Number                   = {404},
  Pages                    = {1198--1202},
  Volume                   = {83},

  ISSN                     = {0162-1459},
  Mrnumber                 = {997603},
  Newspaper                = {J. Am. Stat. Assoc.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://links.jstor.org/sici?sici=0162-1459(198812)83:404<1198:ATOMCA>2.0.CO;2-A&origin=MSN}
}

@Book{little87statistical,
  Title                    = {Statistical Analysis With Missing Data},
  Author                   = {R.~J.~A. Little and D.~B. Rubin},
  Publisher                = {Wiley-Interscience [John Wiley \& Sons]},
  Year                     = {2002},

  Address                  = {Hoboken, NJ},
  Edition                  = {Second},
  Series                   = {Wiley Series in Probability and Statistics},

  ISBN                     = {0-471-18386-5},
  Mrnumber                 = {1925014 (2003g:62007)},
  Owner                    = {mkolar},
  Pages                    = {xviii+381},
  Timestamp                = {2014.02.18}
}

@Article{Liu2013Asymptotic,
  Title                    = {Asymptotic properties of {L}asso+m{LS} and {L}asso+{R}idge in
 sparse high-dimensional linear regression},
  Author                   = {Hanzhong Liu and } # byu,
  Journal                  = ejs_s,
  Year                     = {2013},
  Pages                    = {3124--3169},
  Volume                   = {7},

  Fjournal                 = {Electronic Journal of Statistics},
  ISSN                     = {1935-7524},
  Mrclass                  = {62J07 (62F12 62F40)},
  Mrnumber                 = {3151764},
  Mrreviewer               = {Winston T. Lin}
}

@Article{liu2013feature,
  Title                    = {Feature Selection for Varying Coefficient Models With Ultrahigh Dimensional Covariates},
  Author                   = {Jingyuan Liu and } # rli # { and Rongling Wu},
  Journal                  = {Technical Report},
  Year                     = {2013},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{liu97segmented,
  Title                    = {On Segmented Multivariate Regression},
  Author                   = {J. Liu and S. Wu and J.~V. Zidek},
  Journal                  = statsin,
  Year                     = {1997},
  Number                   = {2},
  Pages                    = {497--525},
  Volume                   = {7},

  ISSN                     = {1017-0405},
  Mrnumber                 = {1466692 (99b:62063)},
  Newspaper                = {Stat. Sinica},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{Liu2016Kernelized,
  author    = {Qiang Liu and Jason Lee and Michael Jordan},
  title     = {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {276--284},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Steins identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
  file      = {:Liu2016Kernelized.pdf:PDF;liub16.pdf:http\://proceedings.mlr.press/v48/liub16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/liub16.html},
}

@Article{Liu2015Fast,
  author =   {Liu, Weidong and Luo, Xi},
  title =    {Fast and adaptive sparse precision matrix estimation in high dimensions},
  journal =  jma_s,
  year =     {2015},
  volume =   {135},
  pages =    {153--162},
  doi =      {10.1016/j.jmva.2014.11.005},
  fjournal = {Journal of Multivariate Analysis},
  issn =     {0047-259X},
  mrclass =  {62H12 (62F12)},
  mrnumber = {3306432},
  url =      {http://dx.doi.org/10.1016/j.jmva.2014.11.005}
}

@Article{Liu2013Carmer,
  author =     {Liu, Weidong and Shao, Qi-Man},
  title =      {A {C}ram\'er moderate deviation theorem for {H}otelling's {$T\sp 2$}-statistic with applications to global tests},
  journal =    aos_s,
  year =       {2013},
  volume =     {41},
  number =     {1},
  pages =      {296--322},
  doi =        {10.1214/12-AOS1082},
  file =       {:Liu2013Carmer_supp.pdf:PDF;Liu2013Carmer.pdf:Liu2013Carmer.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62E20 (60F10 62H15)},
  mrnumber =   {3059419},
  mrreviewer = {M. Cs{\"o}rg{\H{o}}},
  url =        {http://dx.doi.org/10.1214/12-AOS1082}
}

@Article{Liu2015Efficient,
  author =    {Liu, Xin and Wen, Zaiwen and Zhang, Yin},
  title =     {An Efficient Gauss--Newton Algorithm for Symmetric Low-Rank Product Matrix Approximations},
  journal =   {SIAM Journal on Optimization},
  year =      {2015},
  volume =    {25},
  number =    {3},
  pages =     {1571--1608},
  file =      {:Liu2015Efficient.pdf:PDF},
  publisher = {SIAM}
}

@Article{Lockhart2013significance,
  Title                    = {A significance test for the lasso},
  Author                   = {Richard Lockhart and } # jtaylor # { and } # rtibs # { and } # rtibs,
  Journal                  = aos_s,
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {413--468},
  Volume                   = {42},

  Doi                      = {10.1214/13-AOS1175},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J05 (62F03 62J07)},
  Mrnumber                 = {3210970},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1175}
}

@Article{Loh2014High,
  author =   {Loh, Po-Ling and B{\"u}hlmann, Peter},
  title =    {High-dimensional learning of linear causal networks via inverse covariance estimation},
  journal =  jmlr_s,
  year =     {2014},
  volume =   {15},
  pages =    {3065--3105},
  file =     {:Loh2014High.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  issn =     {1532-4435},
  mrclass =  {62H99 (62H12)},
  mrnumber = {3277162}
}

@InCollection{Lopes2014Residual,
  Title                    = {A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs},
  Author                   = {Miles Lopes},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
  Pages                    = {3239--3247},

  Url                      = {http://papers.nips.cc/paper/5507-a-residual-bootstrap-for-high-dimensional-regression-with-near-low-rank-designs.pdf}
}

@Article{Lou2014Sparse,
  Title                    = {Sparse Partially Linear Additive Models},
  Author                   = {Yin Lou and Jacob Bien and Rich Caruana and Johannes Gehrke},
  Journal                  = {ArXiv e-prints, arXiv:1407.4729},
  Year                     = {2014},

  Month                    = jul,

  Abstract                 = {The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing them to have either a linear or nonlinear effect on the response. However, the assignment of features to the linear and nonlinear groups is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $apriori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which features to treat nonlinearly. We introduce sparse partially linear additive models (SPLAMs), which combine model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the Lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance.},
  Eprint                   = {1407.4729},
  Oai2identifier           = {1407.4729}
}

@Article{Louani1998Large,
  Title                    = {Large deviations limit theorems for the kernel density estimator},
  Author                   = {Djamal Louani},
  Journal                  = sjs_s,
  Year                     = {1998},
  Number                   = {1},
  Pages                    = {243--253},
  Volume                   = {25},

  Doi                      = {10.1111/1467-9469.00101},
  Fjournal                 = {Scandinavian Journal of Statistics. Theory and Applications},
  ISSN                     = {0303-6898},
  Mrclass                  = {62G05 (60F10)},
  Mrnumber                 = {1614292 (99c:62106)},
  Mrreviewer               = {Detlef Plachky},
  Url                      = {http://dx.doi.org/10.1111/1467-9469.00101}
}

@Article{lounici12missing,
  Title                    = {High-dimensional Covariance Matrix Estimation With Missing Observations},
  Author                   = {K. Lounici},
  Journal                  = {ArXiv e-prints, arXiv:1201.2577},
  Year                     = {2012},

  Month                    = {January},

  Keywords                 = {Mathematics - Statistics Theory, 62H12},
  Monthno                  = {1},
  Newspaper                = {ArXiv e-prints, arXiv:1201.2577},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Lounici2011Oracle,
  author =    {K. Lounici and M. Pontil and } # atsybakov #{ and } # svdgeer,
  title =     {Oracle Inequalities And Optimal Inference Under Group Sparsity},
  journal =   aos_s,
  year =      {2011},
  volume =    {39},
  pages =     {2164--204},
  file =      {Lounici2011Oracle.pdf:Lounici2011Oracle.pdf:PDF},
  newspaper = {Ann. Stat.},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@Article{Lozano2009Grouped,
  author =    {A. C. Lozano and N. Abe and Y. Liu and S. Rosset},
  title =     {Grouped graphical Granger modeling for gene expression regulatory networks discovery},
  journal =   {Bioinformatics},
  year =      {2009},
  volume =    {25},
  number =    {12},
  pages =     {i110--i118},
  month =     {may},
  doi =       {10.1093/bioinformatics/btp199},
  file =      {Lozano2009Grouped.pdf:Lozano2009Grouped.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
  url =       {http://dx.doi.org/10.1093/bioinformatics/btp199}
}

@Article{Lu2016Nonparametric,
  author =         {Junwei Lu and Guang Cheng and Han Liu},
  title =          {Nonparametric Heterogeneity Testing For Massive Data},
  journal =        {ArXiv e-prints, arXiv:1601.06212},
  year =           {2016},
  month =          jan,
  abstract =       {A massive dataset often consists of a growing number of (potentially) heterogeneous sub-populations. This paper is concerned about testing various forms of heterogeneity arising from massive data. In a general nonparametric framework, a set of testing procedures are designed to accommodate a growing number of sub-populations, denoted as $s$, with computational feasibility. In theory, their null limit distributions are derived as being nearly Chi-square with diverging degrees of freedom as long as $s$ does not grow too fast. Interestingly, we find that a lower bound on $s$ needs to be set for obtaining a sufficiently powerful testing result, so-called "blessing of aggregation." As a by-produc, a type of homogeneity testing is also proposed with a test statistic being aggregated over all sub-populations. Numerical results are presented to support our theory.},
  eprint =         {1601.06212},
  file =           {:Lu2016Nonparametric.pdf:PDF},
  oai2identifier = {1601.06212}
}

@Article{Lu2015Post,
  author         = {Junwei Lu and Mladen Kolar and Han Liu},
  title          = {Post-Regularization Confidence Bands for High Dimensional Nonparametric Models with Local Sparsity},
  journal        = {arXiv e-prints, arXiv:1512.08298},
  year           = {2015},
  abstract       = {We propose a novel high dimensional nonparametric model named ATLAS which naturally generlizes the sparse additive model. Given a covariate of interest $X_j$, the ATLAS model assumes the mean function can be locally approximated by a sparse additive function whose sparsity pattern may vary from the global perspective. We propose to infer the marginal influence function $f_j^*(z) = \mathbb{E}[f(X_1,\ldots, X_d) \mid X_j = z]$ using a new kernel-sieve approach that combines the local kernel regression with the B-spline basis approximation. We prove the rate of convergence for estimating $f_j^*$ under the supremum norm. We also propose two types of confidence bands for $f_j^*$ and illustrate their statistical-comptuational tradeoffs. Thorough numerical results on both synthetic data and real-world genomic data are provided to demonstrate the efficacy of the theory.},
  eprint         = {1503.02978},
  oai2identifier = {1503.02978},
}

@Article{Lu2015Posta,
  author  = {Junwei Lu and Mladen Kolar and Han Liu},
  title   = {Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {203},
  pages   = {1-78},
  url     = {http://jmlr.org/papers/v18/17-145.html},
}

@Article{Lu2014How,
  Title                    = {How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets},
  Author                   = {Zhiyun Lu and Avner May and Kuan Liu and Alireza Bagheri Garakani and Dong Guo and Aurlien Bellet and Linxi Fan and Michael Collins and Brian Kingsbury and Michael Picheny and Fei Sha},
  Journal                  = {ArXiv e-prints, arXiv:1411.4000},
  Year                     = {2014},

  Month                    = nov,

  Abstract                 = {In this paper, we investigate how to scale up kernel methods to take on large-scale problems, on which deep neural networks have been prevailing. To this end, we leverage existing techniques and develop new ones. These techniques include approximating kernel functions with features derived from random projections, parallel training of kernel models with 100 million parameters or more, and new schemes for combining kernel functions as a way of learning representations. We demonstrate how to muster those ideas skillfully to implement large-scale kernel machines for challenging problems in automatic speech recognition. We valid our approaches with extensive empirical studies on real-world speech datasets on the tasks of acoustic modeling. We show that our kernel models are equally competitive as well-engineered deep neural networks (DNNs). In particular, kernel models either attain similar performance to, or surpass their DNNs counterparts. Our work thus avails more tools to machine learning researchers in addressing large-scale learning problems.},
  Eprint                   = {1411.4000},
  Oai2identifier           = {1411.4000}
}

@Article{LusBabYuEtal2004,
  Title                    = {Genomic Analysis Of Regulatory Network Dynamics Reveals Large Topological Changes},
  Author                   = {N.~M. Luscombe and M.~M. Babu and H. Yu and M. Snyder and S.~A. Teichmann and M. Gerstein},
  Journal                  = {Nature},
  Year                     = {2004},
  Number                   = {7006},
  Pages                    = {308--312},
  Volume                   = {431},

  Newspaper                = {Nature},
  Owner                    = {mkolar},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2014.02.18}
}

@InProceedings{Lyu2009Interpretation,
  Title                    = {Interpretation and Generalization of Score Matching},
  Author                   = {Siwei Lyu},
  Booktitle                = PROC_s # { 25th Conf. Uncert. Artif. Intel.},
  Year                     = {2009},

  Address                  = {Arlington, Virginia, United States},
  Pages                    = {359--366},
  Publisher                = {AUAI Press},
  Series                   = {UAI '09},

  Acmid                    = {1795156},
  ISBN                     = {978-0-9749039-5-8},
  Location                 = {Montreal, Quebec, Canada},
  Numpages                 = {8},
  Url                      = {http://dl.acm.org/citation.cfm?id=1795114.1795156}
}

@Article{Masse1999Conditional,
  Title                    = {Conditional logspline density estimation},
  Author                   = {M{\^a}sse, Beno{\^{\i}}t R. and Truong, Young K.},
  Journal                  = {Canad. J. Statist.},
  Year                     = {1999},
  Number                   = {4},
  Pages                    = {819--832},
  Volume                   = {27},

  Doi                      = {10.2307/3316133},
  Fjournal                 = {The Canadian Journal of Statistics. La Revue Canadienne de
 Statistique},
  ISSN                     = {0319-5724},
  Mrclass                  = {62G07 (65C60)},
  Mrnumber                 = {1767149},
  Url                      = {http://dx.doi.org/10.2307/3316133}
}

@InProceedings{Ma2015Adding,
  author    = {Ma, Chenxin and Smith, Virginia and Jaggi, Martin and Jordan, Michael I. and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
  title     = {Adding vs. Averaging in Distributed Primal-dual Optimization},
  booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
  year      = {2015},
  series    = {ICML'15},
  pages     = {1973--1982},
  publisher = {JMLR.org},
  acmid     = {3045328},
  location  = {Lille, France},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=3045118.3045328},
}

@InProceedings{Ma2015Finding,
  author =    {Zhuang Ma and Yichao Lu and Dean P. Foster},
  title =     {Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  year =      {2015},
  editor =    {Francis R. Bach and David M. Blei},
  volume =    {37},
  series =    {{JMLR} Proceedings},
  pages =     {169--178},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp2.uni-trier.de/rec/bib/conf/icml/MaLF15},
  timestamp = {Sun, 05 Jul 2015 19:10:23 +0200},
  url =       {http://jmlr.org/proceedings/papers/v37/maa15.html}
}

@Article{Madigan1994Model,
  author =    {David Madigan and Adrian E. Raftery},
  title =     {Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam{\textquotesingle}s Window},
  journal =   jasa_s,
  year =      {1994},
  volume =    {89},
  number =    {428},
  pages =     {1535--1546},
  month =     {dec},
  doi =       {10.1080/01621459.1994.10476894},
  file =      {Madigan1994Model.pdf:Madigan1994Model.pdf:PDF},
  owner =     {mkolar},
  publisher = {Informa {UK} Limited},
  timestamp = {2016.02.09},
  url =       {http://dx.doi.org/10.1080/01621459.1994.10476894}
}

@Article{Mai2013Note,
  Title                    = {A Note On the Connection and Equivalence of Three Sparse Linear Discriminant Analysis Methods},
  Author                   = {Qi Mai and } # hzou,
  Journal                  = {Technometrics},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {243--246},
  Volume                   = {55},

  Owner                    = {mkolar},
  Publisher                = {Taylor \& Francis Group},
  Timestamp                = {2013.10.29},
  Url                      = {http://amstat.tandfonline.com/doi/abs/10.1080/00401706.2012.746208}
}

@Article{Mai2012direct,
  Title                    = {A direct approach to sparse discriminant analysis in ultra-high dimensions},
  Author                   = {Mai, Qing and } # hzou # { and } # myuan,
  Journal                  = {Biometrika},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {29--42},
  Volume                   = {99},

  Coden                    = {BIOKAX},
  Doi                      = {10.1093/biomet/asr066},
  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {62H30 (62F15 62J07)},
  Mrnumber                 = {2899661},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1093/biomet/asr066}
}

@TechReport{Mai2014Multiclass,
  Title                    = {Multiclass Sparse Discriminant Analysis},
  Author                   = {Qing Mai and Yi Yang and } # hzou,
  Institution              = {University of Minnesota},
  Year                     = {2014}
}

@Article{mairal2010online,
  author =    {J. Mairal and } # fbach #{ and J. Ponce and G. Sapiro},
  title =     {Online Learning For Matrix Factorization And Sparse Coding},
  journal =   jmlr_s,
  year =      {2010},
  volume =    {11},
  pages =     {19--60},
  issn =      {1532-4435},
  mrnumber =  {2591620 (2011i:62121)},
  newspaper = {J. Mach. Learn. Res.},
  owner =     {mkolar},
  timestamp = {2014.02.18}
}

@InCollection{Mairal2014Convolutional,
  author =    {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  title =     {Convolutional Kernel Networks},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year =      {2014},
  editor =    {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages =     {2627--2635},
  file =      {:Mairal2014Convolutional.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf}
}

@Article{Major2006estimate,
  Title                    = {An estimate on the supremum of a nice class of stochastic integrals and U-statistics},
  Author                   = {P\'{e}ter Major},
  Journal                  = {Probab. Theory Related Fields},
  Year                     = {2006},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {489?537},
  Volume                   = {134},

  Doi                      = {10.1007/s00440-005-0440-9},
  ISSN                     = {1432-2064},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s00440-005-0440-9}
}

@InProceedings{Malloy2012Near,
  Title                    = {Near-optimal adaptive compressed sensing},
  Author                   = {Matthew L Malloy and } # rnowak,
  Booktitle                = {Proc. 46th Asilomar Conf. Signals, Systems and Computers},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {1935--1939},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6489376}
}

@Article{Mammen1992Bootstrap,
  author =     {Mammen, Enno},
  title =      {Bootstrap, wild bootstrap, and asymptotic normality},
  journal =    {Probab. Theory Related Fields},
  year =       {1992},
  volume =     {93},
  number =     {4},
  pages =      {439--455},
  coden =      {PTRFEU},
  doi =        {10.1007/BF01192716},
  file =       {Mammen1992Bootstrap.pdf:Mammen1992Bootstrap.pdf:PDF},
  fjournal =   {Probability Theory and Related Fields},
  issn =       {0178-8051},
  mrclass =    {62G09 (60F05 60G50 62E20 62G05)},
  mrnumber =   {1183886},
  mrreviewer = {Thomas Mikosch},
  url =        {http://dx.doi.org/10.1007/BF01192716}
}

@Article{mammen97locally,
  Title                    = {Locally Adaptive Regression Splines},
  Author                   = {E. Mammen and } # svdgeer,
  Journal                  = aos_s,
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {387--413},
  Volume                   = {25},

  Doi                      = {10.1214/aos/1034276635},
  ISSN                     = {0090-5364},
  Mrnumber                 = {1429931 (98d:62064)},
  Newspaper                = {Ann. Stat.},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/aos/1034276635}
}

@Article{Mandozzi2013Hierarchical,
  Title                    = {Hierarchical Testing in the High-Dimensional Setting with Correlated Variables},
  Author                   = {Jacopo Mandozzi and } # pbuhl,
  Journal                  = {arXiv preprint arXiv:1312.5556},
  Year                     = {2013},

  Month                    = dec,

  Abstract                 = {We propose a method for testing whether hierarchically ordered groups of potentially correlated variables are significant for explaining a response in a high-dimensional linear model. In presence of highly correlated variables, as is very common in high-dimensional data, it seems indispensable to go beyond an approach of inferring individual regression coefficients. Thanks to the hierarchy among the groups of variables, powerful multiple testing adjustment is possible which leads to a data-driven choice of the resolution level for the groups. Our procedure, based on repeated sample splitting, is shown to asymptotically control the familywise error rate and we provide empirical results for simulated and real data which complement the theoretical analysis.},
  Eprint                   = {1312.5556},
  Oai2identifier           = {1312.5556}
}

@Book{mardia1980multivariate,
  Title                    = {Multivariate analysis},
  Author                   = {Kantilal Varichand Mardia and John T. Kent and John M. Bibby},
  Publisher                = {Academic Press [Harcourt Brace Jovanovich Publishers]},
  Year                     = {1979},

  Address                  = {London},
  Note                     = {Probability and Mathematical Statistics: A Series of Monographs and Textbooks},

  ISBN                     = {0-12-471250-9},
  Mrclass                  = {62-01 (62Hxx)},
  Mrnumber                 = {560319 (81h:62003)},
  Mrreviewer               = {A. W. Davis},
  Owner                    = {mkolar},
  Pages                    = {xv+521},
  Timestamp                = {2014.02.18}
}

@Article{Markowitz1952Portfolio,
  Title                    = {Portfolio Selection},
  Author                   = {H. Markowitz},
  Journal                  = {J. Finance},
  Year                     = {1952},
  Number                   = {1},
  Pages                    = {77--91},
  Volume                   = {7},

  Newspaper                = {J. Finance},
  Publisher                = {Wiley Online Library}
}

@Article{marlin2012asymptotic,
  Title                    = {Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood},
  Author                   = {Benjamin Marlin and Nando {de Freitas}},
  Journal                  = {ArXiv e-prints, arXiv:1202.3746},
  Year                     = {2012}
}

@InCollection{Mason2001Notes,
  author =    {Mason, David M.},
  title =     {Notes on the {KMT} {B}rownian bridge approximation to the uniform empirical process},
  booktitle = {Asymptotic methods in probability and statistics with applications ({S}t.\ {P}etersburg, 1998)},
  publisher = {Birkh\"auser Boston, Boston, MA},
  year =      {2001},
  series =    {Stat. Ind. Technol.},
  pages =     {351--369},
  file =      {Mason2001Notes.pdf:Mason2001Notes.pdf:PDF},
  mrclass =   {62G30},
  mrnumber =  {1890338}
}

@Article{Mason1987refinement,
  author =     {Mason, David M. and van Zwet, Willem R.},
  title =      {A refinement of the {KMT} inequality for the uniform empirical process},
  journal =    {Ann. Probab.},
  year =       {1987},
  volume =     {15},
  number =     {3},
  pages =      {871--884},
  coden =      {APBYAE},
  file =       {:Mason1987refinement.pdf:PDF},
  fjournal =   {The Annals of Probability},
  issn =       {0091-1798},
  mrclass =    {60F99 (60F17)},
  mrnumber =   {893903},
  mrreviewer = {M. Cs{\"o}rg{\H{o}}},
  url =        {http://links.jstor.org/sici?sici=0091-1798(198707)15:3<871:AROTKI>2.0.CO;2-L&origin=MSN}
}

@Article{Massart1989Strong,
  author =     {Massart, Pascal},
  title =      {Strong approximation for multivariate empirical and related processes, via {KMT} constructions},
  journal =    {Ann. Probab.},
  year =       {1989},
  volume =     {17},
  number =     {1},
  pages =      {266--291},
  coden =      {APBYAE},
  file =       {Massart1989Strong.pdf:Massart1989Strong.pdf:PDF},
  fjournal =   {The Annals of Probability},
  issn =       {0091-1798},
  mrclass =    {60F17 (60G50 62E20)},
  mrnumber =   {972785},
  mrreviewer = {M. Cs{\"o}rg{\H{o}}},
  url =        {http://links.jstor.org/sici?sici=0091-1798(198901)17:1<266:SAFMEA>2.0.CO;2-J&origin=MSN}
}

@Book{Massart2007Concentration,
  Title                    = {Concentration inequalities and model selection},
  Author                   = {Pascal Massart},
  Publisher                = {Springer, Berlin},
  Year                     = {2007},
  Note                     = {Lectures from the 33rd Summer School on Probability Theory
 held in Saint-Flour, July 6--23, 2003,
 With a foreword by Jean Picard},
  Series                   = {Lecture Notes in Mathematics},
  Volume                   = {1896},

  ISBN                     = {978-3-540-48497-4; 3-540-48497-3},
  Mrclass                  = {62-02 (60E15 62B10 62G05 62G07 62G30 94A17)},
  Mrnumber                 = {2319879 (2010a:62008)},
  Pages                    = {xiv+337}
}

@Article{Mastrandrea2015Contact,
  author =    {Rossana Mastrandrea and Julie Fournet and Alain Barrat},
  title =     {Contact Patterns in a High School: A Comparison between Data Collected Using Wearable Sensors, Contact Diaries and Friendship Surveys},
  journal =   {{PLOS} {ONE}},
  year =      {2015},
  volume =    {10},
  number =    {9},
  pages =     {e0136497},
  month =     {sep},
  doi =       {10.1371/journal.pone.0136497},
  editor =    {Cecile Viboud},
  file =      {:Mastrandrea2015Contact.pdf:PDF},
  publisher = {Public Library of Science ({PLoS})},
  url =       {http://dx.doi.org/10.1371/journal.pone.0136497}
}

@Book{MATLAB2014,
  Title                    = {version 8.4.0 (R2014b)},
  Author                   = {MATLAB},
  Publisher                = {The MathWorks Inc.},
  Year                     = {2014},

  Address                  = {Natick, Massachusetts}
}

@Article{Maurer2012Excess,
  author =    {Maurer, Andreas and Pontil, Massimiliano},
  title =     {Excess risk bounds for multitask learning with trace norm regularization},
  year =      {2013},
  pages =     {55--76},
  booktitle = {Conference on Learning Theory},
  file =      {Maurer2012Excess.pdf:Maurer2012Excess.pdf:PDF}
}

@Article{Mazumder2011SparseNet:,
  author    = {Rahul Mazumder and } # jfried #{ and } # thastie,
  title     = {SparseNet: Coordinate descent with nonconvex penalties},
  journal   = jasa_s,
  year      = {2011},
  volume    = {106},
  number    = {495},
  pages     = {1125--1138},
  file      = {Mazumder2011SparseNet\:.pdf:Mazumder2011SparseNet\:.pdf:PDF},
  timestamp = {2013.10.16},
  url       = {http://amstat.tandfonline.com/doi/full/10.1198/jasa.2011.tm09738},
}

@Article{Mazumder2012Exact,
  Title                    = {Exact Covariance Thresholding Into Connected Components For Large-scale Graphical Lasso},
  Author                   = {R. Mazumder and } # thastie,
  Journal                  = jmlr_s,
  Year                     = {2012},
  Pages                    = {781--794},
  Volume                   = {13},

  Newspaper                = {J. Mach. Learn. Res.}
}

@TechReport{Mazumder2011Flexible,
  Title                    = {A Flexible, Scalable And Efficient Algorithmic Framework For Primal Graphical Lasso},
  Author                   = {R. Mazumder and D.~K. Agarwal},
  Institution              = {Stanford University},
  Year                     = {2011}
}

@Article{McCarter2015Large,
  author =         {Calvin McCarter and Seyoung Kim},
  title =          {Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models},
  journal =        {ArXiv e-prints, arXiv:1509.04681},
  year =           {2015},
  month =          sep,
  abstract =       {This paper addresses the problem of scalable optimization for L1-regularized conditional Gaussian graphical models. Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables. While highly scalable optimization methods exist for sparse Gaussian graphical model estimation, state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly, fail due to memory constraints for very large problems. In this paper, we propose a new optimization procedure based on a Newton method that efficiently iterates over two sub-problems, leading to drastic improvement in computation time compared to the previous methods. We then extend our method to scale to large problems under memory constraints, using block coordinate descent to limit memory usage while achieving fast convergence. Using synthetic and genomic data, we show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine.},
  comments =       {11 pages, 7 figures. Appearing in Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume 41},
  eprint =         {1509.04681},
  file =           {:McCarter2015Large.pdf:PDF},
  oai2identifier = {1509.04681}
}

@Book{McCullagh1989Generalized,
  title =     {Generalized linear models},
  publisher = {Chapman \& Hall, London},
  year =      {1989},
  author =    {McCullagh, P. and Nelder, J. A.},
  series =    {Monographs on Statistics and Applied Probability},
  note =      {Second edition [of MR0727836]},
  doi =       {10.1007/978-1-4899-3242-6},
  isbn =      {0-412-31760-5},
  mrclass =   {62J12 (62H17)},
  mrnumber =  {3223057},
  pages =     {xix+511},
  url =       {http://dx.doi.org/10.1007/978-1-4899-3242-6}
}

@InCollection{Mcdonald2009Efficient,
  author =    {Ryan Mcdonald and Mohri, Mehryar and Nathan Silberman and Dan Walker and Gideon S. Mann},
  title =     {Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models},
  booktitle = {Advances in Neural Information Processing Systems 22},
  publisher = {Curran Associates, Inc.},
  year =      {2009},
  editor =    {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
  pages =     {1231--1239},
  url =       {http://papers.nips.cc/paper/3881-efficient-large-scale-distributed-training-of-conditional-maximum-entropy-models.pdf}
}

@Article{Meier2009High,
  author =     {Lukas Meier and } # svdgeer #{ and } # pbuhl,
  title =      {High-dimensional additive modeling},
  journal =    aos_s,
  year =       {2009},
  volume =     {37},
  number =     {6B},
  pages =      {3779--3821},
  coden =      {ASTSC7},
  doi =        {10.1214/09-AOS692},
  file =       {Meier2009High.pdf:Meier2009High.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G08 (62F12 62J07)},
  mrnumber =   {2572443 (2010m:62127)},
  mrreviewer = {Yuehua Wu},
  url =        {http://dx.doi.org/10.1214/09-AOS692}
}

@Article{Minnier2011perturbation,
  Title                    = {A perturbation method for inference on regularized regression estimates},
  Author                   = {Jessica Minnier and Lu Tian and Tianxi Cai},
  Journal                  = jasa_s,
  Year                     = {2011},
  Number                   = {496},
  Volume                   = {106},

  Timestamp                = {2013.10.25},
  Url                      = {http://amstat.tandfonline.com/doi/full/10.1198/jasa.2011.tm10382}
}

@Article{Mitra2014Multivariate,
  author =         {Ritwik Mitra and } # chzhang,
  title =          {Multivariate Analysis of Nonparametric Estimates of Large Correlation Matrices},
  journal =        {ArXiv e-prints, arXiv:1403.6195},
  year =           {2014},
  month =          mar,
  abstract =       {We study concentration in spectral norm of nonparametric estimates of correlation matrices. We work within the confine of a Gaussian copula model. Two nonparametric estimators of the correlation matrix, the sine transformations of the Kendall's tau and Spearman's rho correlation coefficient, are studied. Expected spectrum error bound is obtained for both the estimators. A general large deviation bound for the maximum spectral error of a collection of submatrices of a given dimension is also established. These results prove that when both the number of variables and sample size are large, the spectral error of the nonparametric estimators is of no greater order than that of the latent sample covariance matrix, at least when compared with some of the sharpest known error bounds for the later. As an application, we establish the minimax optimal convergence rate in the estimation of high-dimensional bandable correlation matrices via tapering off of these nonparametric estimators. An optimal convergence rate for sparse principal component analysis is also established as another example of possible applications of the main results.},
  comments =       {26 pages},
  eprint =         {1403.6195},
  file =           {:Mitra2014Multivariate.pdf:PDF},
  oai2identifier = {1403.6195}
}

@Article{Miyamura2006Robust,
  Title                    = {Robust Gaussian graphical modeling},
  Author                   = {Masashi Miyamura and Yutaka Kano},
  Journal                  = jma_s,
  Year                     = {2006},

  Month                    = {Aug},
  Number                   = {7},
  Pages                    = {1525--1550},
  Volume                   = {97},

  Doi                      = {10.1016/j.jmva.2006.02.006},
  ISSN                     = {0047-259X},
  Owner                    = {mkolar},
  Publisher                = {Elsevier BV},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1016/j.jmva.2006.02.006}
}

@InCollection{Mohan2012Structured,
  Title                    = {Structured Learning of Gaussian Graphical Models},
  Author                   = {Karthik Mohan and Mike Chung and Seungyeop Han and } # dwitten # { and Su-In Lee and Maryam Fazel},
  Booktitle                = PROC_s # { } # NIPS_s # { 25},
  Year                     = {2012},
  Editor                   = {P. Bartlett and F.c.n. Pereira and C.j.c. Burges and L. Bottou and K.q. Weinberger},
  Pages                    = {629--637},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://books.nips.cc/papers/files/nips25/NIPS2012_0291.pdf}
}

@Article{Mohan2014Node,
  Title                    = {Node-based learning of multiple gaussian graphical models},
  Author                   = {Karthik Mohan and Palma London and Maryam Fazel and } # dwitten # { and Su-In Lee},
  Journal                  = jmlr_s,
  Year                     = {2014},
  Pages                    = {445--488},
  Volume                   = {15},

  Url                      = {http://arxiv.org/abs/1303.5145}
}

@Article{Molenberghs2007Likelihood,
  Title                    = {Likelihood ratio, score, and {W}ald tests in a constrained
 parameter space},
  Author                   = {Molenberghs, Geert and Verbeke, Geert},
  Journal                  = {Amer. Statist.},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {22--27},
  Volume                   = {61},

  Coden                    = {ASTAAJ},
  Doi                      = {10.1198/000313007X171322},
  Fjournal                 = {The American Statistician},
  ISSN                     = {0003-1305},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2339143},
  Url                      = {http://dx.doi.org/10.1198/000313007X171322}
}

@Article{Moussouris1974Gibbs,
  author =    {Moussouris, John},
  title =     {Gibbs and Markov random systems with constraints},
  journal =   {Journal of statistical physics},
  year =      {1974},
  volume =    {10},
  number =    {1},
  pages =     {11--33},
  publisher = {Springer}
}

@Book{muirhead1982aspects,
  Title                    = {Aspects of multivariate statistical theory},
  Author                   = {Robb J. Muirhead},
  Publisher                = {John Wiley \& Sons Inc.},
  Year                     = {1982},

  Address                  = {New York},
  Note                     = {Wiley Series in Probability and Mathematical Statistics},

  ISBN                     = {0-471-09442-0},
  Mrclass                  = {62Hxx},
  Mrnumber                 = {652932 (84c:62073)},
  Mrreviewer               = {Nariaki Sugiura},
  Pages                    = {xix+673}
}

@Article{Murphy2002Estimation,
  author =   {Murphy, Kevin M. and Topel, Robert H.},
  title =    {Estimation and inference in two-step econometric models},
  journal =  {J. Bus. Econom. Statist.},
  year =     {2002},
  volume =   {20},
  number =   {1},
  pages =    {88--97},
  note =     {Twentieth anniversary commemorative issue},
  doi =      {10.1198/073500102753410417},
  file =     {Murphy2002Estimation.pdf:Murphy2002Estimation.pdf:PDF},
  fjournal = {Journal of Business \& Economic Statistics},
  issn =     {0735-0015},
  mrclass =  {62F10 (62J05)},
  mrnumber = {1940632},
  url =      {http://dx.doi.org/10.1198/073500102753410417}
}

@Article{Nagaev1979Large,
  Title                    = {Large Deviations of Sums of Independent Random Variables},
  Author                   = {S. V.~Nagaev},
  Journal                  = aop_s,
  Year                     = {1979},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {745--789},
  Volume                   = {7},

  Doi                      = {10.1214/aop/1176994938},
  ISSN                     = {0091-1798},
  Publisher                = {Institute of Mathematical Statistics},
  Url                      = {http://dx.doi.org/10.1214/aop/1176994938}
}

@Article{Nakajima2013Bayesian,
  author =   {Nakajima, Jouchi and West, Mike},
  title =    {Bayesian analysis of latent threshold dynamic models},
  journal =  jbes_s,
  year =     {2013},
  volume =   {31},
  number =   {2},
  pages =    {151--164},
  doi =      {10.1080/07350015.2012.747847},
  file =     {Nakajima2013Bayesian.pdf:Nakajima2013Bayesian.pdf:PDF},
  fjournal = {Journal of Business \& Economic Statistics},
  issn =     {0735-0015},
  mrclass =  {91B84 (91B64)},
  mrnumber = {3055329},
  url =      {http://dx.doi.org/10.1080/07350015.2012.747847}
}

@Article{Nardi2008asymptotic,
  Title                    = {On the asymptotic properties of the group lasso estimator for
 linear models},
  Author                   = {Yuval Nardi and Alessandro Rinaldo},
  Journal                  = ejs_s,
  Year                     = {2008},
  Pages                    = {605--633},
  Volume                   = {2},

  Doi                      = {10.1214/08-EJS200},
  Fjournal                 = {Electronic Journal of Statistics},
  ISSN                     = {1935-7524},
  Mrclass                  = {62J05 (62F12)},
  Mrnumber                 = {2426104 (2009k:62175)},
  Mrreviewer               = {Junling Ma},
  Url                      = {http://dx.doi.org/10.1214/08-EJS200}
}

@Article{negahban09Phase,
  author =    {S.~N. Negahban and } # mwainw,
  title =     {Simultaneous Support Recovery In High Dimensions: Benefits And Perils Of Block $\ell _{1}/\ell _{\infty}$-regularization},
  journal =   IEEEit_s,
  year =      {2011},
  volume =    {57},
  number =    {6},
  pages =     {3841--3863},
  month =     {June},
  address =   {Piscataway, NJ, USA},
  doi =       {10.1109/TIT.2011.2144150},
  file =      {:negahban09Phase.pdf:PDF},
  issn =      {0018-9448},
  monthno =   {6},
  newspaper = {IEEE Trans. Inf. Theory},
  publisher = {IEEE Press},
  url =       {http://dx.doi.org/10.1109/TIT.2011.2144150}
}

@Article{negahban2010unified,
  author =    {Sahand N Negahban and } # pravik #{ and } # mwainw #{ and } # byu,
  title =     {A unified framework for high-dimensional analysis of $ M $-estimators with decomposable regularizers},
  journal =   statsci_s,
  year =      {2012},
  volume =    {27},
  number =    {4},
  pages =     {538--557},
  file =      {negahban2010unified.pdf:negahban2010unified.pdf:PDF},
  publisher = {Institute of Mathematical Statistics}
}

@InProceedings{nesterov1983method,
  Title                    = {A method of solving a convex programming problem with convergence rate ${\cal O}(1/k^2)$},
  Author                   = {Yurii Nesterov},
  Booktitle                = {Soviet Mathematics Doklady},
  Year                     = {1983},
  Number                   = {2},
  Pages                    = {372--376},
  Volume                   = {27}
}

@Article{nesterov05smooth,
  Title                    = {Smooth Minimization Of Non-smooth Functions},
  Author                   = {Y. Nesterov},
  Journal                  = {Math. Program.},
  Year                     = {2005},
  Number                   = {1, Ser. A},
  Pages                    = {127--152},
  Volume                   = {103},

  Doi                      = {10.1007/s10107-004-0552-5},
  ISSN                     = {0025-5610},
  Mrnumber                 = {2166537 (2006g:90174)},
  Newspaper                = {Math. Program.},
  Url                      = {http://dx.doi.org/10.1007/s10107-004-0552-5}
}

@TechReport{nesterov07gradient,
  Title                    = {Gradient Methods For Minimizing Composite Objective Function},
  Author                   = {Y. Nesterov},
  Institution              = {Center for Operations Research and Econometrics {(CORE),} Catholic University of Louvain},
  Year                     = {2007},
  Number                   = {76:2007}
}

@Article{Neym1959Optimal,
  Title                    = {Optimal asymptotic tests of composite statistical hypotheses},
  Author                   = {Jerzy Neyman},
  Journal                  = {Probability and statistics},
  Year                     = {1959},
  Pages                    = {213},
  Volume                   = {57},

  Publisher                = {New York, John Wiley}
}

@Article{Nicholson2015VARX,
  author =         {William Nicholson and David Matteson and Jacob Bien},
  title =          {VARX-L: Structured Regularization for Large Vector Autoregressions with Exogenous Variables},
  journal =        {ArXiv e-prints, arXiv:1508.07497},
  year =           {2015},
  month =          aug,
  abstract =       {The vector autoregression (VAR) has long proven to be an effective method for modeling the joint dynamics of macroe- conomic time series as well as forecasting. A major shortcomings of the VAR that has hindered its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. Consequently, forecasting using VARs is intractable for low-frequency, high-dimensional macroeconomic data. However, empirical evidence suggests that VARs that incorporate more component series tend to result in more accurate forecasts. Conventional methods that allow for the estimation of large VARs either tend to require ad hoc subjective specifications or are computationally infeasible. Moreover, as global economies become more intricately intertwined, there has been substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. Vector autoregression with exogenous variables (VARX) extends the VAR to allow for the inclusion of unmodeled variables, but it similarly faces dimensionality challenges. We introduce the VARX-L framework, a structured family of VARX models, and provide methodology which allows for both efficient estimation and accurate forecasting in high-dimensional analysis. VARX-L adapts several prominent scalar regression regularization techniques to a vector time series context to greatly reduce the parameter space of VAR and VARX models. We formulate convex optimization procedures that are amenable to efficient solutions for the time-ordered, high-dimensional problems we aim to solve. We also highlight a compelling extension that allows for shrinking toward reference models. We demonstrate the efficacy of VARX-L in both low- and high-dimensional macroeconomic applications and simulated data examples.},
  eprint =         {1508.07497},
  file =           {:Nicholson2015VARX.pdf:PDF},
  oai2identifier = {1508.07497},
  owner =          {mkolar},
  timestamp =      {2016.03.16}
}

@Article{Nicholson2014Hierarchical,
  author =         {William B. Nicholson and Jacob Bien and David S. Matteson},
  title =          {Hierarchical Vector Autoregression},
  journal =        {ArXiv e-prints, arXiv:1412.5250},
  year =           {2014},
  month =          dec,
  abstract =       {Vector autoregression (VAR) is a fundamental tool for modeling the joint dynamics of multivariate time series. However, as the number of component series is increased, the VAR model quickly becomes overparameterized, making reliable estimation difficult and impeding its adoption as a forecasting tool in high dimensional settings. A number of authors have sought to address this issue by incorporating regularized approaches, such as the lasso, that impose sparse or low-rank structures on the estimated coefficient parameters of the VAR. More traditional approaches attempt to address overparameterization by selecting a low lag order, based on the assumption that dynamic dependence among components is short-range. However, these methods typically assume a single, universal lag order that applies across all components, unnecessarily constraining the dynamic relationship between the components and impeding forecast performance. The lasso-based approaches are more flexible but do not incorporate the notion of lag order selection. We propose a new class of regularized VAR models, called hierarchical vector autoregression (HVAR), that embed the notion of lag selection into a convex regularizer. The key convex modeling tool is a group lasso with nested groups which ensure the sparsity pattern of autoregressive lag coefficients honors the ordered structure inherent to VAR. We provide computationally efficient algorithms for solving HVAR problems that can be parallelized across the components. A simulation study shows the improved performance in forecasting and lag order selection over previous approaches, and a macroeconomic application further highlights forecasting improvements as well as the convenient, interpretable output of a HVAR model.},
  eprint =         {1412.5250},
  file =           {:Nicholson2014Hierarchical.pdf:PDF},
  oai2identifier = {1412.5250},
  owner =          {mkolar},
  timestamp =      {2016.03.16}
}

@Article{Nickl2013Confidence,
  Title                    = {Confidence sets in sparse regression},
  Author                   = {Richard Nickl and } # svdgeer,
  Journal                  = aos_s,
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {2852-2876},
  Volume                   = {41},

  Doi                      = {10.1214/13-AOS1170},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Url                      = {http://dx.doi.org/10.1214/13-AOS1170}
}

@Article{Ning2014General,
  author     = {Ning, Yang and Liu, Han},
  title      = {A general theory of hypothesis tests and confidence regions for sparse high dimensional models},
  journal    = {Ann. Statist.},
  year       = {2017},
  volume     = {45},
  number     = {1},
  pages      = {158--195},
  issn       = {0090-5364},
  doi        = {10.1214/16-AOS1448},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62E20 (62F03 62F25)},
  mrnumber   = {3611489},
  mrreviewer = {Zaixing Li},
  url        = {http://dx.doi.org/10.1214/16-AOS1448},
}

@Article{Ning2014Likelihood,
  author =         {Yang Ning and Tianqi Zhao and Han Liu},
  title =          {A Likelihood Ratio Framework for High Dimensional Semiparametric Regression},
  journal =        {ArXiv e-prints, arXiv:1412.2295},
  year =           {2014},
  month =          dec,
  abstract =       {We propose a likelihood ratio based inferential framework for high dimensional semiparametric generalized linear models. This framework addresses a variety of challenging problems in high dimensional data analysis, including incomplete data, selection bias, and heterogeneous multitask learning. Our work has three main contributions. (i) We develop a regularized statistical chromatography approach to infer the parameter of interest under the proposed semiparametric generalized linear model without the need of estimating the unknown base measure function. (ii) We propose a new framework to construct post-regularization confidence regions and tests for the low dimensional components of high dimensional parameters. Unlike existing post-regularization inferential methods, our approach is based on a novel directional likelihood. In particular, the framework naturally handles generic regularized estimators with nonconvex penalty functions and it can be used to infer least false parameters under misspecified models. (iii) We develop new concentration inequalities and normal approximation results for U-statistics with unbounded kernels, which are of independent interest. We demonstrate the consequences of the general theory by using an example of missing data problem. Extensive simulation studies and real data analysis are provided to illustrate our proposed approach.},
  comments =       {51 pages, 1 figure, 2 tables},
  eprint =         {1412.2295},
  file =           {:Ning2014Likelihood.pdf:PDF},
  oai2identifier = {1412.2295}
}

@Article{Noh2012Efficient,
  Title                    = {Efficient model selection in semivarying coefficient models},
  Author                   = {Hohsuk Noh and Ingrid {Van Keilegom}},
  Journal                  = ejs_s,
  Year                     = {2012},
  Number                   = {0},
  Pages                    = {2519-2534},
  Volume                   = {6},

  Doi                      = {10.1214/12-EJS762},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.02.13},
  Url                      = {http://dx.doi.org/10.1214/12-EJS762}
}

@Article{Norvaisa1991Rate,
  author =     {Norvai{\v{s}}a, R. and Paulauskas, V.},
  title =      {Rate of convergence in the central limit theorem for empirical processes},
  journal =    {J. Theoret. Probab.},
  year =       {1991},
  volume =     {4},
  number =     {3},
  pages =      {511--534},
  coden =      {JTPREO},
  doi =        {10.1007/BF01210322},
  file =       {Norvaisa1991Rate.pdf:Norvaisa1991Rate.pdf:PDF},
  fjournal =   {Journal of Theoretical Probability},
  issn =       {0894-9840},
  mrclass =    {60F05 (60B10 60F17 60J65)},
  mrnumber =   {1115160},
  mrreviewer = {P. R{\'e}v{\'e}sz},
  url =        {http://dx.doi.org/10.1007/BF01210322}
}

@Article{nussbaum96asymptotic,
  Title                    = {Asymptotic Equivalence Of Density Estimation And Gaussian White Noise},
  Author                   = {M. Nussbaum},
  Journal                  = aos_s,
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {2399--2430},
  Volume                   = {24},

  Newspaper                = {Ann. Stat.}
}

@Article{OSullivan1988Fast,
  Title                    = {Fast computation of fully automated log-density and log-hazard
 estimators},
  Author                   = {O'Sullivan, Finbarr},
  Journal                  = {SIAM J. Sci. Statist. Comput.},
  Year                     = {1988},
  Number                   = {2},
  Pages                    = {363--379},
  Volume                   = {9},

  Coden                    = {SIJCD4},
  Doi                      = {10.1137/0909024},
  Fjournal                 = {Society for Industrial and Applied Mathematics. Journal on
 Scientific and Statistical Computing},
  ISSN                     = {0196-5204},
  Mrclass                  = {62G05 (65U05)},
  Mrnumber                 = {930052 (89g:62062)},
  Mrreviewer               = {Ulrich Stadtm{\"u}ller},
  Url                      = {http://dx.doi.org/10.1137/0909024}
}

@Article{obozinski10support,
  author =    {G. Obozinski and } # mwainw #{ and } # mjordan,
  title =     {Support Union Recovery In High-dimensional Multivariate Regression},
  journal =   aos_s,
  year =      {2011},
  volume =    {39},
  number =    {1},
  pages =     {1--47},
  file =      {obozinski10support.pdf:obozinski10support.pdf:PDF},
  newspaper = {Ann. Stat.}
}

@Article{Obozinski2010Joint,
  author =    {Guillaume Obozinski and Ben Taskar and } # mjordan,
  title =     {Joint covariate selection and joint subspace selection for multiple classification problems},
  journal =   statcomp_s,
  year =      {2010},
  volume =    {20},
  number =    {2},
  pages =     {231--252},
  file =      {:Obozinski2010Joint.pdf:PDF},
  publisher = {Springer}
}

@Article{Oliveir2013lower,
  Title                    = {The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties},
  Author                   = {Roberto Imbuzeiro Oliveira},
  Journal                  = {ArXiv e-prints, arXiv:1312.2903},
  Year                     = {2013},

  Month                    = dec,

  Abstract                 = {Finite sample properties of random covariance-type matrices have been the subject of much research. In this paper we focus on the "lower tail" of such a matrix, and prove that it is subgaussian under a simple fourth moment assumption on the one-dimensional marginals of the random vectors. A similar result holds for more general sums of random positive semidefinite matrices, and the (relatively simple) proof uses a variant of the so-called PAC-Bayesian method for bounding empirical processes. We give two applications of the main result. In the first one we obtain a new finite-sample bound for ordinary least squares estimator in linear regression with random design. Our result is model-free, requires fairly weak moment assumptions and is almost optimal. Our second application is to bounding restricted eigenvalue constants of certain random ensembles with "heavy tails". These constants are important in the analysis of problems in Compressed Sensing and High Dimensional Statistics, where one recovers a sparse vector from a small umber of linear measurements. Our result implies that heavy tails still allow for the fast recovery rates found in efficient methods such as the LASSO and the Dantzig selector. Along the way we strengthen, with a fairly short argument, a recent result of Rudelson and Zhou on the restricted eigenvalue property.},
  Comments                 = {36 pages},
  Eprint                   = {1312.2903},
  Oai2identifier           = {1312.2903}
}

@Article{Opsomer1997Fitting,
  Title                    = {Fitting a bivariate additive model by local polynomial regression},
  Author                   = {Jean D. Opsomer and David Ruppert},
  Journal                  = aos_s,
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {186--211},
  Volume                   = {25},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/aos/1034276626},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05},
  Mrnumber                 = {1429922 (98b:62074)},
  Mrreviewer               = {Theo Gasser},
  Url                      = {http://dx.doi.org/10.1214/aos/1034276626}
}

@Article{Orabona2012PRISMA,
  Title                    = {{PRISMA}: {PR}oximal Iterative {SM}oothing Algorithm},
  Author                   = {Francesco Orabona and Andreas Argyriou and } # nsrebro,
  Journal                  = {ArXiv e-prints, arXiv:1206.2372},
  Year                     = {2012},

  Month                    = jun,

  Abstract                 = {Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and sparse inverse covariance selection, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. We use a time variant smoothing strategy that allows us to obtain a guarantee that does not depend on knowing in advance the total number of iterations nor a bound on the domain.},
  Eprint                   = {1206.2372},
  Oai2identifier           = {1206.2372}
}

@Book{Fishburn1989Studies,
  title =     {Studies in the Economics of Uncertainty: In Honor of Josef Hadar},
  publisher = {Springer-Verlag New York},
  year =      {1989},
  author =    {P. C. Fishburn (auth.), Thomas B. Fomby, Tae Kun Seo (eds.)},
  edition =   {1},
  isbn =      {978-1-4613-8924-8,978-1-4613-8922-4},
  url =       {http://gen.lib.rus.ec/book/index.php?md5=261194d19fe3dbda44f70c98f0246c5f}
}

@Book{Pagan1999Nonparametric,
  Title                    = {Nonparametric Econometrics (Themes in Modern Econometrics)},
  Author                   = {Adrian Pagan and Aman Ullah},
  Publisher                = {Cambridge University Press},
  Year                     = {1999},

  ISBN                     = {0521586119},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.19},
  Url                      = {http://www.amazon.com/Nonparametric-Econometrics-Themes-Modern/dp/0521586119%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0521586119}
}

@Article{Park2013Varying,
  Title                    = {Varying Coefficient Regression Models: A Review and New Developments},
  Author                   = {Byeong U. Park and Enno Mammen and Young K. Lee and Eun Ryung Lee},
  Journal                  = {International Statistical Review},
  Year                     = {2013},

  Month                    = {Dec},
  Pages                    = {n/a-n/a},

  Doi                      = {10.1111/insr.12029},
  Publisher                = {Wiley Blackwell (Blackwell Publishing)},
  Url                      = {http://dx.doi.org/10.1111/insr.12029}
}

@Article{Parry2012Proper,
  author =    {Matthew Parry and } # adawid #{ and } # slauritzen,
  title =     {Proper local scoring rules},
  journal =   aos_s,
  year =      {2012},
  volume =    {40},
  number =    {1},
  pages =     {561--592},
  month =     {Feb},
  doi =       {10.1214/12-aos971},
  file =      {:Parry2012Proper.pdf:PDF},
  issn =      {0090-5364},
  publisher = {Institute of Mathematical Statistics - care of Project Euclid},
  url =       {http://dx.doi.org/10.1214/12-AOS971}
}

@Article{patterson1971recovery,
  Title                    = {Recovery Of Inter-block Information When Block Sizes Are Unequal},
  Author                   = {H.~D. Patterson and R. Thompson},
  Journal                  = {Biometrika},
  Year                     = {1971},
  Pages                    = {545--554},
  Volume                   = {58},

  ISSN                     = {0006-3444},
  Mrnumber                 = {0319325 (47 \#7869)},
  Newspaper                = {Biometrika}
}

@InCollection{Peel2010Empirical,
  Title                    = {Empirical Bernstein Inequalities for U-Statistics},
  Author                   = {Thomas Peel and Sandrine Anthoine and Liva Ralaivola},
  Booktitle                = NIPS_s # { 23},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2010},
  Editor                   = {J.D. Lafferty and C.K.I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta},
  Pages                    = {1903--1911},

  Url                      = {http://papers.nips.cc/paper/4081-empirical-bernstein-inequalities-for-u-statistics.pdf}
}

@Article{Peng2009Partial,
  author    = {J. Peng and P. Wang and N. Zhou and J. Zhu},
  title     = {Partial Correlation Estimation By Joint Sparse Regression Models},
  journal   = jasa_s,
  year      = {2009},
  volume    = {104},
  number    = {486},
  pages     = {735--746},
  file      = {:Peng2009Partial.pdf:PDF},
  newspaper = {J. Am. Stat. Assoc.},
  timestamp = {2018.04.30},
}

@Article{peng08regularized,
  author =    {J. Peng and J. Zhu and A. Bergamaschi and W. Han and D.-Y. Noh and J.~R. Pollack and P. Wang},
  title =     {Regularized Multivariate Regression For Identifying Master Predictors With Application To Integrative Genomics Study Of Breast Cancer},
  journal =   aoas_s,
  year =      {2010},
  volume =    {4},
  number =    {1},
  pages =     {53--77},
  doi =       {10.1214/09-AOAS271},
  file =      {peng08regularized.pdf:peng08regularized.pdf:PDF},
  issn =      {1932-6157},
  mrnumber =  {2758084},
  newspaper = {Ann. Appl. Stat.},
  url =       {http://dx.doi.org/10.1214/09-AOAS271}
}

@Article{Peters2014Identifiability,
  author =   {Peters, J. and B{\"u}hlmann, P.},
  title =    {Identifiability of {G}aussian structural equation models with equal error variances},
  journal =  {Biometrika},
  year =     {2014},
  volume =   {101},
  number =   {1},
  pages =    {219--228},
  doi =      {10.1093/biomet/ast043},
  file =     {Peters2014Identifiability.pdf:Peters2014Identifiability.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62G05 (62H99)},
  mrnumber = {3180667},
  url =      {http://dx.doi.org/10.1093/biomet/ast043}
}

@Article{Petersen2014Fused,
  Title                    = {Fused Lasso Additive Model},
  Author                   = {Ashley Petersen and } # dwitten # { and } # nsimon,
  Journal                  = {ArXiv e-prints, arXiv:1409.5391},
  Year                     = {2014},

  Url                      = {http://arxiv.org/abs/1409.5391}
}

@Article{verbyla1990conditional,
  Title                    = {A Conditional Derivation Of Residual Maximum Likelihood},
  Author                   = {V.~A. Petras},
  Journal                  = {Aust. J. Stat.},
  Year                     = {1990},
  Number                   = {2},
  Pages                    = {227--230},
  Volume                   = {32},

  Newspaper                = {Australian Journal of Statistics},
  Publisher                = {Wiley Online Library}
}

@Article{Pfister2016Kernel,
  author =         {Niklas Pfister and Peter Bhlmann and Bernhard Schlkopf and Jonas Peters},
  title =          {Kernel-based Tests for Joint Independence},
  journal =        {ArXiv e-prints, arXiv:1603.00285},
  year =           {2016},
  month =          mar,
  abstract =       {We investigate the problem of testing whether $d$ random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the $d$-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the $d$ variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small $d$, it performs well in practice. Finally, we apply the test to a problem in causal discovery.},
  comments =       {57 pages},
  eprint =         {1603.00285},
  file =           {:Pfister2016Kernel.pdf:PDF},
  oai2identifier = {1603.00285}
}

@Article{Pilanci2014Iterative,
  author =         {Mert Pilanci and } # mwainw,
  title =          {Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares},
  journal =        {ArXiv e-prints, arXiv:1411.0347},
  year =           {2014},
  month =          nov,
  abstract =       {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including $\ell_1$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
  eprint =         {1411.0347},
  file =           {:Pilanci2014Iterative.pdf:PDF},
  oai2identifier = {1411.0347},
  owner =          {mkolar},
  timestamp =      {2016.04.21}
}

@Article{Pilanci2014Randomized,
  author =         {Mert Pilanci and } # mwainw,
  title =          {Randomized Sketches of Convex Programs with Sharp Guarantees},
  journal =        {ArXiv e-prints, arXiv:1404.7203},
  year =           {2014},
  month =          apr,
  abstract =       {Random projection (RP) is a classical technique for reducing storage and computational costs. We analyze RP-based approximations of convex programs, in which the original optimization problem is approximated by the solution of a lower-dimensional problem. Such dimensionality reduction is essential in computation-limited settings, since the complexity of general convex programming can be quite high (e.g., cubic for quadratic programs, and substantially higher for semidefinite programs). In addition to computational savings, random projection is also useful for reducing memory usage, and has useful properties for privacy-sensitive optimization. We prove that the approximation ratio of this procedure can be bounded in terms of the geometry of constraint set. For a broad class of random projections, including those based on various sub-Gaussian distributions as well as randomized Hadamard and Fourier transforms, the data matrix defining the cost function can be projected down to the statistical dimension of the tangent cone of the constraints at the original solution, which is often substantially smaller than the original dimension. We illustrate consequences of our theory for various cases, including unconstrained and $\ell_1$-constrained least squares, support vector machines, low-rank matrix estimation, and discuss implications on privacy-sensitive optimization and some connections with de-noising and compressed sensing.},
  eprint =         {1404.7203},
  file =           {:Pilanci2014Randomized.pdf:PDF},
  oai2identifier = {1404.7203},
  owner =          {mkolar},
  timestamp =      {2016.04.21}
}

@Article{Pilanci2015Newton,
  author =         {Mert Pilanci and } # mwainw,
  title =          {Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence},
  journal =        {ArXiv e-prints, arXiv:1505.02250},
  year =           {2015},
  month =          may,
  abstract =       {We propose a randomized second-order method for optimization known as the Newton Sketch: it is based on performing an approximate Newton step using a randomly projected or sub-sampled Hessian. For self-concordant functions, we prove that the algorithm has super-linear convergence with exponentially high probability, with convergence and complexity guarantees that are independent of condition numbers and related problem-dependent quantities. Given a suitable initialization, similar guarantees also hold for strongly convex and smooth objectives without self-concordance. When implemented using randomized projections based on a sub-sampled Hadamard basis, the algorithm typically has substantially lower complexity than Newton's method. We also describe extensions of our methods to programs involving convex constraints that are equipped with self-concordant barriers. We discuss and illustrate applications to linear programs, quadratic programs with convex constraints, logistic regression and other generalized linear models, as well as semidefinite programs.},
  eprint =         {1505.02250},
  file =           {:Pilanci2015Newton.pdf:PDF},
  oai2identifier = {1505.02250},
  owner =          {mkolar},
  timestamp =      {2016.04.21}
}

@InCollection{Portnoy1984Tightness,
  author =     {Portnoy, Stephen},
  title =      {Tightness of the sequence of empiric c.d.f. processes defined from regression fractiles},
  booktitle =  {Robust and nonlinear time series analysis ({H}eidelberg, 1983)},
  publisher =  {Springer, New York},
  year =       {1984},
  volume =     {26},
  series =     {Lecture Notes in Statist.},
  pages =      {231--246},
  doi =        {10.1007/978-1-4615-7821-5_13},
  file =       {Portnoy1984Tightness.pdf:Portnoy1984Tightness.pdf:PDF},
  mrclass =    {62J05 (62F12 62G05)},
  mrnumber =   {786311},
  mrreviewer = {M. Hu{\v{s}}kov{\'a}},
  url =        {http://dx.doi.org/10.1007/978-1-4615-7821-5_13}
}

@Article{Pourahmadi2011Covariance,
  Title                    = {Covariance Estimation: The GLM and Regularization Perspectives},
  Author                   = {Mohsen Pourahmadi},
  Journal                  = statsci_s,
  Year                     = {2011},

  Month                    = {08},
  Number                   = {3},
  Pages                    = {369--387},
  Volume                   = {26},

  Doi                      = {10.1214/11-STS358},
  Long_journal             = {Statistical Science},
  Publisher                = {The Institute of Mathematical Statistics},
  Url                      = {http://dx.doi.org/10.1214/11-STS358}
}

@Book{Pourahmadi2013High,
  Title                    = {High-Dimensional Covariance Estimation: With High-Dimensional Data (Wiley Series in Probability and Statistics)},
  Author                   = {Mohsen Pourahmadi},
  Publisher                = {Wiley},
  Year                     = {2013},

  ISBN                     = {1118034295},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.17},
  Url                      = {http://www.amazon.com/High-Dimensional-Covariance-Estimation-Probability-Statistics/dp/1118034295%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D1118034295}
}

@Article{punskaya2002bayesian,
  Title                    = {Bayesian Curve Fitting Using Mcmc With Applications To Signal Segmentation},
  Author                   = {E. Punskaya and C. Andrieu and A. Doucet and W.~J. Fitzgerald},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2002},
  Number                   = {3},
  Pages                    = {747--758},
  Volume                   = {50},

  Newspaper                = {IEEE Trans. Signal Proces.},
  Publisher                = {IEEE}
}

@Article{Qiu2013Joint,
  author     = {Qiu, Huitong and Han, Fang and Liu, Han and Caffo, Brian},
  title      = {Joint estimation of multiple graphical models from high dimensional time series},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2016},
  volume     = {78},
  number     = {2},
  pages      = {487--504},
  issn       = {1369-7412},
  doi        = {10.1111/rssb.12123},
  file       = {Qiu2013Joint.pdf:Qiu2013Joint.pdf:PDF},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62M10 (62H12 62H99 62M99)},
  mrnumber   = {3454206},
  mrreviewer = {Yoshihiro Yajima},
  url        = {http://dx.doi.org/10.1111/rssb.12123},
}

@Manual{RLanguage,
  Title                    = {R: A Language and Environment for Statistical Computing},

  Address                  = {Vienna, Austria},
  Author                   = {{R Core Team}},
  Note                     = {{ISBN} 3-900051-07-0},
  Organization             = {R Foundation for Statistical Computing},
  Year                     = {2012},

  Url                      = {http://www.R-project.org/}
}

@Article{rubinstein08efficient,
  Title                    = {Efficient Implementation Of The K-svd Algorithm Using Batch Orthogonal Matching Pursuit},
  Author                   = {R. R. and Z. M. and E. M.},
  Journal                  = {CS Technion, Tech. Rep.},
  Year                     = {2008},

  Newspaper                = {CS Technion, Tech. Rep.}
}

@Article{Ruetimann2009High,
  author =   {Philipp R{\"u}timann and } # pbuhl,
  title =    {High dimensional sparse covariance estimation via directed acyclic graphs},
  journal =  {Electron. J. Stat.},
  year =     {2009},
  volume =   {3},
  pages =    {1133--1160},
  doi =      {10.1214/09-EJS534},
  file =     {Ruetimann2009High.pdf:Ruetimann2009High.pdf:PDF},
  fjournal = {Electronic Journal of Statistics},
  issn =     {1935-7524},
  mrclass =  {62H12 (62-09 62F12)},
  mrnumber = {2566184},
  url =      {http://dx.doi.org/10.1214/09-EJS534}
}

@Article{Ram2010Distributed,
  author =    {Ram, S Sundhar and Nedi{\'c}, A and Veeravalli, Venugopal V},
  title =     {Distributed stochastic subgradient projection algorithms for convex optimization},
  journal =   {Journal of optimization theory and applications},
  year =      {2010},
  volume =    {147},
  number =    {3},
  pages =     {516--545},
  file =      {:Ram2010Distributed.pdf:PDF},
  publisher = {Springer}
}

@InProceedings{Ranganath2014Black,
  Title                    = {Black Box Variational Inference},
  Author                   = {Rajesh Ranganath and Sean Gerrish and David Blei},
  Booktitle                = PROC_s # { 17th Int. Conf, Artif. Intel. Stat.},
  Year                     = {2014},
  Pages                    = {814--822}
}

@Article{rao2007inferring,
  Title                    = {Inferring Time-varying Network Topologies From Gene Expression Data},
  Author                   = {A. Rao and A.~O. Hero, III and D.~J. States and J.~D. Engel},
  Journal                  = {EURASIP J. Bioinformatics Syst. Bio.},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {51947},
  Volume                   = {2007},

  Doi                      = {10.1155/2007/51947},
  ISSN                     = {1687-4153},
  Newspaper                = {EURASIP J. Bioinformatics Syst. Bio.},
  Url                      = {http://bsb.eurasipjournals.com/content/2007/1/51947}
}

@Article{Rao1969Partial,
  Title                    = {Partial Canonical Correlations},
  Author                   = {B. Rao},
  Journal                  = {Trabajos de Estadstica y de Investigacin Operativa},
  Year                     = {1969},
  Number                   = {2},
  Pages                    = {211--219},
  Volume                   = {20},

  Doi                      = {10.1007/BF03028532},
  ISSN                     = {0041-0241},
  Keywords                 = {Business and Economics},
  Newspaper                = {Trabajos de Estadstica y de Investigacin Operativa}
}

@Article{rao1970estimation,
  Title                    = {Estimation Of Heteroscedastic Variances In Linear Models},
  Author                   = {C.~R. Rao},
  Journal                  = jasa_s,
  Year                     = {1970},
  Number                   = {329},
  Pages                    = {161--172},
  Volume                   = {65},

  Newspaper                = {J. Am. Stat. Assoc.}
}

@Article{Raskutti2012Minimax,
  author     = {Garvesh Raskutti and } # mwainw #{ and } # byu,
  title      = {Minimax-optimal rates for sparse additive models over kernel classes via convex programming},
  journal    = jmlr_s,
  year       = {2012},
  volume     = {13},
  pages      = {389--427},
  issn       = {1532-4435},
  file       = {:Raskutti2012Minimax.pdf:PDF},
  fjournal   = {Journal of Machine Learning Research (JMLR)},
  mrclass    = {62G05 (62B10 62G08 62J07)},
  mrnumber   = {2913704},
  mrreviewer = {Hung Hung},
}

@Article{Raskutti2013Learning,
  author =         {Garvesh Raskutti and Caroline Uhler},
  title =          {Learning directed acyclic graphs based on sparsest permutations},
  journal =        {ArXiv e-prints, arXiv:1307.0366},
  year =           {2013},
  month =          jul,
  abstract =       {We consider the problem of learning a Bayesian network or directed acyclic graph (DAG) model from observational data. A number of constraint-based, score-based and hybrid algorithms have been developed for this purpose. For constraint-based methods, statistical consistency guarantees typically rely on the faithfulness assumption, which has been show to be restrictive especially for graphs with cycles in the skeleton. However, there is only limited work on consistency guarantees for score-based and hybrid algorithms and it has been unclear whether consistency guarantees can be proven under weaker conditions than the faithfulness assumption. In this paper, we propose the sparsest permutation (SP) algorithm. This algorithm is based on finding the causal ordering of the variables that yields the sparsest DAG. We prove that this new score-based method is consistent under strictly weaker conditions than the faithfulness assumption. We also demonstrate through simulations on small DAGs that the SP algorithm compares favorably to the constraint-based PC and SGS algorithms as well as the score-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method. In the Gaussian setting, we prove that our algorithm boils down to finding the permutation of the variables with sparsest Cholesky decomposition for the inverse covariance matrix. Using this connection, we show that in the oracle setting, where the true covariance matrix is known, the SP algorithm is in fact equivalent to $\ell_0$-penalized maximum likelihood estimation.},
  comments =       {22 pages, 5 figures},
  eprint =         {1307.0366},
  file =           {:Raskutti2013Learning.pdf:PDF},
  oai2identifier = {1307.0366}
}

@Article{Ravikumar2011High,
  author    = {P. Ravikumar and } # mwainw #{ and G. Raskutti and B. Yu},
  title     = {High-dimensional Covariance Estimation By Minimizing $\ell_1$-penalized Log-determinant Divergence},
  journal   = ejs_s,
  year      = {2011},
  volume    = {5},
  pages     = {935-980},
  file      = {:Ravikumar2011High.pdf:PDF},
  newspaper = {Electron. J. Stat.},
  timestamp = {2019.04.23},
}

@Article{Ray2015Improved,
  author =   {Ray, Avik and Sanghavi, Sujay and Shakkottai, Sanjay},
  title =    {Improved greedy algorithms for learning graphical models},
  journal =  {IEEE Trans. Inform. Theory},
  year =     {2015},
  volume =   {61},
  number =   {6},
  pages =    {3457--3468},
  doi =      {10.1109/TIT.2015.2427354},
  file =     {Ray2015Improved.pdf:Ray2015Improved.pdf:PDF},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  issn =     {0018-9448},
  mrclass =  {62B10},
  mrnumber = {3352509},
  url =      {http://dx.doi.org/10.1109/TIT.2015.2427354}
}

@Article{Reeves2012sampling,
  Title                    = {The sampling rate-distortion tradeoff for sparsity pattern recovery in compressed sensing},
  Author                   = {Galen Reeves and Michael Gastpar},
  Journal                  = IEEEit_s,
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {3065--3092},
  Volume                   = {58},

  Owner                    = {mkolar},
  Publisher                = {IEEE},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6142088}
}

@Article{Ren2013Asymptotic,
  author =   {Zhao Ren and Tingni Sun and } # chzhang #{ and } # hzhou,
  title =    {Asymptotic normality and optimalities in estimation of large {G}aussian graphical models},
  journal =  aos_s,
  year =     {2015},
  volume =   {43},
  number =   {3},
  pages =    {991--1026},
  doi =      {10.1214/14-AOS1286},
  file =     {:Ren2013Asymptotic_supp.pdf:PDF;:Ren2013Asymptotic.pdf:PDF},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62H12 (62F12 62H99)},
  mrnumber = {3346695},
  url =      {http://dx.doi.org/10.1214/14-AOS1286}
}

@Article{Richtarik2014Iteration,
  Title                    = {Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  Author                   = {Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}},
  Journal                  = {Math. Program.},
  Year                     = {2014},
  Number                   = {1-2, Ser. A},
  Pages                    = {1--38},
  Volume                   = {144},

  Doi                      = {10.1007/s10107-012-0614-z},
  Fjournal                 = {Mathematical Programming. A Publication of the Mathematical
 Programming Society},
  ISSN                     = {0025-5610},
  Mrclass                  = {90C06 (65K10 65Y20 90C25 90C60)},
  Mrnumber                 = {3179953},
  Url                      = {http://dx.doi.org/10.1007/s10107-012-0614-z}
}

@Article{rigby1996semi,
  Title                    = {A Semi-parametric Additive Model For Variance Heterogeneity},
  Author                   = {R.~A. Rigby and D.~M. Stasinopoulos},
  Journal                  = statcomp_s,
  Year                     = {1996},
  Number                   = {1},
  Pages                    = {57--65},
  Volume                   = {6},

  Newspaper                = {Statistics and Computing},
  Publisher                = {Springer}
}

@Article{Rinaldo08properties,
  Title                    = {Properties And Refinements Of The Fused Lasso},
  Author                   = {A. Rinaldo},
  Journal                  = aos_s,
  Year                     = {2009},
  Number                   = {5B},
  Pages                    = {2922--2952},
  Volume                   = {37},

  Doi                      = {10.1214/08-AOS665},
  ISSN                     = {0090-5364},
  Mrnumber                 = {2541451 (2011b:62113)},
  Newspaper                = {Ann. Stat.},
  Url                      = {http://dx.doi.org/10.1214/08-AOS665}
}

@Article{Robins1995Semiparametric,
  author =   {Robins, James M. and Rotnitzky, Andrea},
  title =    {Semiparametric efficiency in multivariate regression models with missing data},
  journal =  jasa_s,
  year =     {1995},
  volume =   {90},
  number =   {429},
  pages =    {122--129},
  coden =    {JSTNAL},
  file =     {:Robins1995Semiparametric.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {62G20 (62G05)},
  mrnumber = {1325119 (96d:62084)},
  url =      {http://links.jstor.org/sici?sici=0162-1459(199503)90:429<122:SEIMRM>2.0.CO;2-R&origin=MSN}
}

@Article{robins94estimation,
  Title                    = {Estimation Of Regression Coefficients When Some Regressors Are Not Always Observed},
  Author                   = {J.~M. Robins and A. Rotnitzky and L.~P. Zhao},
  Journal                  = jasa_s,
  Year                     = {1994},
  Number                   = {427},
  Pages                    = {846--866},
  Volume                   = {89},

  Newspaper                = {J. Am. Stat. Assoc.},
  Publisher                = {Taylor \& Francis Group}
}

@Article{robins03uniform,
  Title                    = {Uniform Consistency In Causal Inference},
  Author                   = {J.~M. Robins and R. Scheines and P. Spirtes and } # lwasser,
  Journal                  = {Biometrika},
  Year                     = {2003},
  Number                   = {3},
  Pages                    = {491--515},
  Volume                   = {90},

  Doi                      = {10.1093/biomet/90.3.491},
  ISSN                     = {0006-3444},
  Mrnumber                 = {2006831 (2004g:62014)},
  Newspaper                = {Biometrika},
  Url                      = {http://dx.doi.org/10.1093/biomet/90.3.491}
}

@Article{robinson2010learning,
  Title                    = {Learning Non-stationary Dynamic {b}ayesian Networks},
  Author                   = {J.~W. Robinson and A.~J. Hartemink},
  Journal                  = jmlr_s,
  Year                     = {2010},
  Pages                    = {3647--3680},
  Volume                   = {11},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2756196 (2011i:62012)},
  Newspaper                = {J. Mach. Learn. Res.}
}

@Article{Rocha2008path,
  Title                    = {A path following algorithm for Sparse Pseudo-Likelihood Inverse Covariance Estimation (SPLICE)},
  Author                   = {Guilherme V. Rocha and Peng Zhao and } # byu,
  Journal                  = {arXiv preprint arXiv:0807.3734},
  Year                     = {2008},

  Month                    = jul,

  Abstract                 = {Given n observations of a p-dimensional random vector, the covariance matrix and its inverse (precision matrix) are needed in a wide range of applications. Sample covariance (e.g. its eigenstructure) can misbehave when p is comparable to the sample size n. Regularization is often used to mitigate the problem. In this paper, we proposed an l1-norm penalized pseudo-likelihood estimate for the inverse covariance matrix. This estimate is sparse due to the l1-norm penalty, and we term this method SPLICE. Its regularization path can be computed via an algorithm based on the homotopy/LARS-Lasso algorithm. Simulation studies are carried out for various inverse covariance structures for p=15 and n=20, 1000. We compare SPLICE with the l1-norm penalized likelihood estimate and a l1-norm penalized Cholesky decomposition based method. SPLICE gives the best overall performance in terms of three metrics on the precision matrix and ROC curve for model selection. Moreover, our simulation results demonstrate that the SPLICE estimates are positive-definite for most of the regularization path even though the restriction is not enforced.},
  Comments                 = {33 pages, 11 Figures},
  Eprint                   = {0807.3734},
  Oai2identifier           = {0807.3734},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.13}
}

@Article{Roosta-Khorasani2016Sub-I,
  author         = {Farbod Roosta-Khorasani and Michael W. Mahoney},
  title          = {Sub-Sampled Newton Methods I: Globally Convergent Algorithms},
  journal        = {arxiv:1601.04737},
  year           = {2016},
  month          = jan,
  abstract       = {Large scale optimization problems are ubiquitous in machine learning and data analysis and there is a plethora of algorithms for solving such problems. Many of these algorithms employ sub-sampling, as a way to either speed up the computations and/or to implicitly implement a form of statistical regularization. In this paper, we consider second-order iterative optimization algorithms and we provide bounds on the convergence of the variants of Newton's method that incorporate uniform sub-sampling as a means to estimate the gradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our algorithms are global and are guaranteed to converge from any initial iterate. Using random matrix concentration inequalities, one can sub-sample the Hessian to preserve the curvature information. Our first algorithm incorporates Hessian sub-sampling while using the full gradient. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or ridge-type regularization. Next, in addition to Hessian sub-sampling, we also consider sub-sampling the gradient as a way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra to obtain the proper sampling strategy. In all these algorithms, computing the update boils down to solving a large scale linear system, which can be computationally expensive. As a remedy, for all of our algorithms, we also give global convergence results for the case of inexact updates where such linear system is solved only approximately. This paper has a more advanced companion paper, [42], in which we demonstrate that, by doing a finer-grained analysis, we can get problem-independent bounds for local convergence of these algorithms and explore trade-offs to improve upon the basic results of the present paper.},
  eprint         = {1601.04737},
  file           = {:Roosta-Khorasani2016Sub-I.pdf:PDF;:Roosta-Khorasani2016Sub-II.pdf:PDF},
  oai2identifier = {1601.04737},
  owner          = {mkolar},
  timestamp      = {2016.02.23},
}

@Article{Roosta-Khorasani2016Sub-II,
  author =         {Farbod Roosta-Khorasani and Michael W. Mahoney},
  title =          {Sub-Sampled Newton Methods II: Local Convergence Rates},
  journal =        {ArXiv e-prints, arXiv:1601.04738},
  year =           {2016},
  month =          jan,
  abstract =       {Many data-fitting applications require the solution of an optimization problem involving a sum of large number of functions of high dimensional parameter. Here, we consider the problem of minimizing a sum of $n$ functions over a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both $n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$ can offer great amount of computational efficiency, while maintaining their original convergence properties. Within the context of second order methods, we first give quantitative local convergence results for variants of Newton's method where the Hessian is uniformly sub-sampled. Using random matrix concentration inequalities, one can sub-sample in a way that the curvature information is preserved. Using such sub-sampling strategy, we establish locally Q-linear and Q-superlinear convergence rates. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type regularization. Finally, in addition to Hessian sub-sampling, we consider sub-sampling the gradient as way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra (RandNLA) to obtain the proper sampling strategy and we establish locally R-linear convergence rates. In such a setting, we also show that a very aggressive sample size increase results in a R-superlinearly convergent algorithm. While the sample size depends on the condition number of the problem, our convergence rates are problem-independent, i.e., they do not depend on the quantities related to the problem. Hence, our analysis here can be used to complement the results of our basic framework from the companion paper, [38], by exploring algorithmic trade-offs that are important in practice.},
  eprint =         {1601.04738},
  file =           {:Roosta-Khorasani2016Sub-II.pdf:PDF},
  oai2identifier = {1601.04738},
  owner =          {mkolar},
  timestamp =      {2016.02.23}
}

@InCollection{Rosasco2015Learning,
  author =    {Rosasco, Lorenzo and Villa, Silvia},
  title =     {Learning with Incremental Iterative Regularization},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {1621--1629},
  file =      {:Rosasco2015Learning.pdf:PDF},
  url =       {http://papers.nips.cc/paper/6015-learning-with-incremental-iterative-regularization.pdf}
}

@Article{Rosasco2013Nonparametric,
  Title                    = {Nonparametric sparsity and regularization},
  Author                   = {Lorenzo Rosasco and Silvia Villa and Sofia Mosci and Matteo Santoro and Alessandro Verri},
  Journal                  = jmlr_s,
  Year                     = {2013},
  Pages                    = {1665--1714},
  Volume                   = {14},

  Fjournal                 = {Journal of Machine Learning Research (JMLR)},
  ISSN                     = {1532-4435},
  Mrclass                  = {62G08 (62J02 90C90)},
  Mrnumber                 = {3104492}
}

@Article{Rosenblatt2014Optimality,
  author =         {Jonathan Rosenblatt and Boaz Nadler},
  title =          {On the Optimality of Averaging in Distributed Statistical Learning},
  journal =        {ArXiv e-prints, arXiv:1407.2724},
  year =           {2014},
  month =          jul,
  abstract =       {A common approach to statistical learning with big-data is to randomly split it among $m$ machines and learn the parameter of interest by averaging the $m$ individual estimates. In this paper, focusing on empirical risk minimization, or equivalently M-estimation, we study the statistical error incurred by this strategy. We consider two large-sample settings: First, a classical setting where the number of parameters $p$ is fixed, and the number of samples per machine $n\to\infty$. Second, a high-dimensional regime where both $p,n\to\infty$ with $p/n \to \kappa \in (0,1)$. For both regimes and under suitable assumptions, we present asymptotically exact expressions for this estimation error. In the fixed-$p$ setting, under suitable assumptions, we prove that to leading order averaging is as accurate as the centralized solution. We also derive the second order error terms, and show that these can be non-negligible, notably for non-linear models. The high-dimensional setting, in contrast, exhibits a qualitatively different behavior: data splitting incurs a first-order accuracy loss, which to leading order increases linearly with the number of machines. The dependence of our error approximations on the number of machines traces an interesting accuracy-complexity tradeoff, allowing the practitioner an informed choice on the number of machines to deploy. Finally, we confirm our theoretical analysis with several simulations.},
  comments =       {Major changes from previous version. Particularly on the second order error approximation and implications},
  eprint =         {1407.2724},
  file =           {:Rosenblatt2014Optimality.pdf:PDF},
  oai2identifier = {1407.2724}
}

@Article{royall1986model,
  Title                    = {Model Robust Confidence Intervals Using Maximum Likelihood Estimators},
  Author                   = {R.~M. Royall},
  Journal                  = {International Statistical Review/Revue Internationale de Statistique},
  Year                     = {1986},
  Pages                    = {221--226},

  Newspaper                = {International Statistical Review/Revue Internationale de Statistique},
  Publisher                = {JSTOR}
}

@Article{rubin1976inference,
  Title                    = {Inference And Missing Data},
  Author                   = {D.~B. Rubin},
  Journal                  = {Biometrika},
  Year                     = {1976},
  Note                     = {With comments by R. J. A. Little and a reply by the author},
  Number                   = {3},
  Pages                    = {581--592},
  Volume                   = {63},

  ISSN                     = {0006-3444},
  Mrnumber                 = {0455196 (56 \#13435)},
  Newspaper                = {Biometrika}
}

@Article{Rudelson2008sparse,
  author =     {Rudelson, Mark and Vershynin, Roman},
  title =      {On sparse reconstruction from {F}ourier and {G}aussian measurements},
  journal =    {Comm. Pure Appl. Math.},
  year =       {2008},
  volume =     {61},
  number =     {8},
  pages =      {1025--1045},
  coden =      {CPAMA},
  doi =        {10.1002/cpa.20227},
  file =       {:Rudelson2008sparse.pdf:PDF},
  fjournal =   {Communications on Pure and Applied Mathematics},
  issn =       {0010-3640},
  mrclass =    {94A20 (46B07 46B09 60B11 90C90)},
  mrnumber =   {2417886},
  mrreviewer = {Alberto Portal},
  url =        {http://dx.doi.org/10.1002/cpa.20227}
}

@Article{Rudelson2010Non,
  Title                    = {Non-asymptotic theory of random matrices: extreme singular values},
  Author                   = {Mark Rudelson and Roman Vershynin},
  Journal                  = {arXiv preprint arXiv:1003.2990},
  Year                     = {2010},

  Month                    = mar,

  Abstract                 = {The classical random matrix theory is mostly focused on asymptotic spectral properties of random matrices as their dimensions grow to infinity. At the same time many recent applications from convex geometry to functional analysis to information theory operate with random matrices in fixed dimensions. This survey addresses the non-asymptotic theory of extreme singular values of random matrices with independent entries. We focus on recently developed geometric methods for estimating the hard edge of random matrices (the smallest singular value).},
  Comments                 = {Submission for ICM 2010. Some typographic corrections made},
  Eprint                   = {1003.2990},
  Oai2identifier           = {1003.2990},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Article{Rudelson2013Hanson,
  Title                    = {Hanson-Wright inequality and sub-gaussian concentration},
  Author                   = {Rudelson, Mark and Vershynin, Roman},
  Journal                  = ecp_s,
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {0},
  Volume                   = {18},

  Doi                      = {10.1214/ECP.v18-2865},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics - care of Project Euclid},
  Timestamp                = {2014.02.18},
  Url                      = {http://dx.doi.org/10.1214/ECP.v18-2865}
}

@Article{rudelson2011reconstruction,
  author    = {Rudelson, Mark and Zhou, Shuheng},
  title     = {Reconstruction from anisotropic random measurements},
  journal   = {IEEE Trans. Inform. Theory},
  year      = {2013},
  volume    = {59},
  number    = {6},
  pages     = {3434--3447},
  issn      = {0018-9448},
  doi       = {10.1109/TIT.2013.2243201},
  fjournal  = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass   = {94A12},
  mrnumber  = {3061256},
  timestamp = {2019.05.02},
  url       = {https://doi.org/10.1109/TIT.2013.2243201},
}

@InCollection{Rudi2015Lessa,
  author =    {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  title =     {Less is More: Nystr\"{o}m Computational Regularization},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {1648--1656},
  file =      {:Rudi2015Lessa.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5936-less-is-more-nystrom-computational-regularization.pdf}
}

@Article{Rudi2016Generalization,
  author =         {Alessandro Rudi and Raffaello Camoriano and Lorenzo Rosasco},
  title =          {Generalization Properties of Learning with Random Features},
  year =           {2016},
  month =          feb,
  abstract =       {We study the generalization properties of regularized learning with random features in the statistical learning theory framework. We show that optimal learning errors can be achieved with a number of features smaller than the number of examples. As a byproduct, we also show that learning with random features can be seen as a form of regularization, rather than only a way to speed up computations.},
  eprint =         {1602.04474},
  file =           {:Rudi2016Generalization.pdf:PDF},
  oai2identifier = {1602.04474}
}

@Article{ruppert97local,
  Title                    = {Local Polynomial Variance-function Estimation},
  Author                   = {D. Ruppert and M.~P. Wand and U. Holst and O. H{\"o}ssjer},
  Journal                  = {Technometrics},
  Year                     = {1997},
  Number                   = {3},
  Pages                    = {262--273},
  Volume                   = {39},

  Doi                      = {10.2307/1271131},
  ISSN                     = {0040-1706},
  Mrnumber                 = {1462587 (98c:62073)},
  Newspaper                = {Technometrics},
  Url                      = {http://dx.doi.org/10.2307/1271131}
}

@Article{rutemiller1968estimation,
  Title                    = {Estimation In A Heteroscedastic Regression Model},
  Author                   = {H.~C. Rutemiller and D.~A. Bowers},
  Journal                  = jasa_s,
  Year                     = {1968},
  Pages                    = {552--557},
  Volume                   = {63},

  ISSN                     = {0162-1459},
  Mrnumber                 = {0232506 (38 \#831)},
  Newspaper                = {J. Am. Stat. Assoc.}
}

@Article{Sachs2005Causal,
  author    = {Sachs, Karen and Perez, Omar and Pe{\textquoteright}er, Dana and Lauffenburger, Douglas A. and Nolan, Garry P.},
  title     = {Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data},
  journal   = {Science},
  year      = {2005},
  volume    = {308},
  number    = {5721},
  pages     = {523--529},
  issn      = {0036-8075},
  abstract  = {Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.},
  doi       = {10.1126/science.1105809},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019.05.02},
  url       = {http://science.sciencemag.org/content/308/5721/523},
}

@Article{Sander1991Database,
  author =    {Sander, Chris and Schneider, Reinhard},
  title =     {Database of homology-derived protein structures and the structural meaning of sequence alignment},
  journal =   {Proteins: Structure, Function, and Bioinformatics},
  year =      {1991},
  volume =    {9},
  number =    {1},
  pages =     {56--68},
  file =      {:Sander1991Database.pdf:PDF},
  publisher = {Wiley Online Library}
}

@Article{Sardy2004AMlet,
  Title                    = {A{M}let, {RAM}let, and {GAM}let: automatic nonlinear fitting
 of additive models, robust and generalized, with wavelets},
  Author                   = {Sylvain Sardy and Paul Tseng},
  Journal                  = jcgs_s,
  Year                     = {2004},
  Number                   = {2},
  Pages                    = {283--309},
  Volume                   = {13},

  Doi                      = {10.1198/1061860043434},
  Fjournal                 = {Journal of Computational and Graphical Statistics},
  ISSN                     = {1061-8600},
  Mrclass                  = {62G05 (42C40)},
  Mrnumber                 = {2063986 (2005d:62060)},
  Mrreviewer               = {J. A. Melamed},
  Url                      = {http://dx.doi.org/10.1198/1061860043434}
}

@Article{sarkarmoore2006,
  Title                    = {Dynamic Social Network Analysis Using Latent Space Models},
  Author                   = {P. Sarkar and A.~W. Moore},
  Journal                  = {ACM SIGKDD Explor. Newsl.},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {31--40},
  Volume                   = {7},

  Newspaper                = {ACM SIGKDD Explor. Newsl.},
  Publisher                = {ACM}
}

@Book{Schumaker2007Spline,
  Title                    = {Spline functions: basic theory},
  Author                   = {Larry L. Schumaker},
  Publisher                = {Cambridge University Press, Cambridge},
  Year                     = {2007},
  Edition                  = {Third},
  Series                   = {Cambridge Mathematical Library},

  Doi                      = {10.1017/CBO9780511618994},
  ISBN                     = {978-0-521-70512-7},
  Mrclass                  = {41-02 (41A15 65D07)},
  Mrnumber                 = {2348176 (2008i:41002)},
  Pages                    = {xvi+582},
  Url                      = {http://dx.doi.org/10.1017/CBO9780511618994}
}

@Article{Schwarz1978Estimating,
  Title                    = {Estimating the Dimension of a Model},
  Author                   = {Gideon Schwarz},
  Journal                  = aos_s,
  Year                     = {1978},

  Month                    = {03},
  Number                   = {2},
  Pages                    = {461--464},
  Volume                   = {6},

  Ajournal                 = {Ann. Statist.},
  Doi                      = {10.1214/aos/1176344136},
  Publisher                = {The Institute of Mathematical Statistics},
  Url                      = {http://dx.doi.org/10.1214/aos/1176344136}
}

@InProceedings{Seltzer2013Multi,
  author =       {Seltzer, Michael L and Droppo, Jasha},
  title =        {Multi-task learning in deep neural networks for improved phoneme recognition},
  booktitle =    {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013},
  year =         {2013},
  pages =        {6965--6969},
  organization = {IEEE},
  file =         {Seltzer2013Multi.pdf:Seltzer2013Multi.pdf:PDF}
}

@Book{Serfling2001Approximation,
  Title                    = {Approximation Theorems of Mathematical Statistics},
  Author                   = {Robert J. Serfling},
  Publisher                = {Wiley-Interscience},
  Year                     = {2001},

  ISBN                     = {0471219274},
  Url                      = {http://www.amazon.com/Approximation-Theorems-Mathematical-Statistics-Serfling/dp/0471219274%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0471219274}
}

@Article{Settati2009Gaussian,
  author =     {Settati, Adel},
  title =      {Gaussian approximation of the empirical process under random entropy conditions},
  journal =    {Stochastic Process. Appl.},
  year =       {2009},
  volume =     {119},
  number =     {5},
  pages =      {1541--1560},
  coden =      {STOPB7},
  doi =        {10.1016/j.spa.2008.08.001},
  file =       {Settati2009Gaussian.pdf:Settati2009Gaussian.pdf:PDF},
  fjournal =   {Stochastic Processes and their Applications},
  issn =       {0304-4149},
  mrclass =    {60F15 (60F05 62B10)},
  mrnumber =   {2513118},
  mrreviewer = {Christophe Vignat},
  url =        {http://dx.doi.org/10.1016/j.spa.2008.08.001}
}

@Article{Shah2013Variable,
  Title                    = {Variable selection with error control: another look at stability selection},
  Author                   = {Rajen D. Shah and } # rsamworth,
  Journal                  = jrssb_s,
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {55--80},
  Volume                   = {75},

  Publisher                = {Wiley Online Library},
  Timestamp                = {2013.10.25},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.01034.x/full}
}

@Article{Shalev-Shwartz2010Trading,
  author =    {Shai Shalev-Shwartz and } # nsrebro #{ and } # tzhang,
  title =     {Trading accuracy for sparsity in optimization problems with sparsity constraints},
  journal =   {SIAM Journal on Optimization},
  year =      {2010},
  volume =    {20},
  number =    {6},
  pages =     {2807--2832},
  publisher = {SIAM}
}

@Book{Shalev-Shwartz2014Understanding,
  title =     {Understanding machine learning: From theory to algorithms},
  publisher = {Cambridge University Press},
  year =      {2014},
  author =    {Shalev-Shwartz, Shai and Ben-David, Shai}
}

@InProceedings{Shalev-Shwartz2011Large,
  author =    {Shai Shalev-Shwartz and Alon Gonen and Ohad Shamir},
  title =     {Large-Scale Convex Minimization with a Low-Rank Constraint},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  series =    {ICML '11},
  pages =     {329--336},
  address =   {New York, NY, USA},
  month =     {June},
  publisher = {ACM},
  file =      {:Shalev-Shwartz2011Large.pdf:PDF},
  isbn =      {978-1-4503-0619-5},
  location =  {Bellevue, Washington, USA}
}

@InProceedings{Shamir2015Stochastic,
  author =    {Ohad Shamir},
  title =     {A Stochastic {PCA} and {SVD} Algorithm with an Exponential Convergence Rate},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  year =      {2015},
  editor =    {Francis R. Bach and David M. Blei},
  volume =    {37},
  series =    {{JMLR} Proceedings},
  pages =     {144--152},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp2.uni-trier.de/rec/bib/conf/icml/Shamir15},
  timestamp = {Sun, 05 Jul 2015 19:10:23 +0200},
  url =       {http://jmlr.org/proceedings/papers/v37/shamir15.html}
}

@InProceedings{Shamir2014Distributed,
  author =       {Ohad Shamir and } # nsrebro,
  title =        {Distributed stochastic optimization and learning},
  booktitle =    {52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2014},
  year =         {2014},
  pages =        {850--857},
  organization = {IEEE},
  file =         {:Shamir2014Distributed.pdf:PDF}
}

@InProceedings{Shamir2013Communication,
  author =    {Ohad Shamir and } # nsrebro #{ and } # tzhang,
  title =     {Communication efficient distributed optimization using an approximate newton-type method},
  booktitle = {Proceedings of The 31st International Conference on Machine Learning},
  year =      {2014},
  pages =     {1000--1008},
  file =      {:Shamir2013Communication.pdf:PDF}
}

@Article{Shang2013Local,
  author =     {Shang, Zuofeng and Cheng, Guang},
  title =      {Local and global asymptotic inference in smoothing spline models},
  journal =    aos_s,
  year =       {2013},
  volume =     {41},
  number =     {5},
  pages =      {2608--2638},
  doi =        {10.1214/13-AOS1164},
  file =       {:Shang2013Local_supp.pdf:PDF;Shang2013Local.pdf:Shang2013Local.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62G20 (62F12 62F15 62F25)},
  mrnumber =   {3161439},
  mrreviewer = {Debdeep Pati},
  url =        {http://dx.doi.org/10.1214/13-AOS1164}
}

@Article{shao2011lda,
  Title                    = {Sparse linear discriminant analysis by thresholding for high dimensional data},
  Author                   = {Jun Shao and Yazhen Wang and Xinwei Deng and Sijian Wang},
  Journal                  = aos_s,
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {1241--1265},
  Volume                   = {39},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/10-AOS870},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62H30 (62F12)},
  Mrnumber                 = {2816353 (2012j:62244)},
  Url                      = {http://dx.doi.org/10.1214/10-AOS870}
}

@Article{Shao2014Cramer,
  author =         {Qi-Man Shao and Wen-Xin Zhou},
  title =          {Cram\'er type moderate deviation theorems for self-normalized processes},
  journal =        {ArXiv e-prints, arXiv:1405.1218},
  year =           {2014},
  month =          may,
  abstract =       {Cram\'er type moderate deviation theorems quantify the accuracy of the relative error of the normal approximation and provide theoretical justifications for many commonly used methods in statistics. In this paper, we develop a new randomized concentration inequality and establish a Cram\'er type moderate deviation theorem for general self-normalized processes which include many well-known Studentized nonlinear statistics. In particular, a sharp moderate deviation theorem under optimal moment conditions is established for Studentized $U$-statistics.},
  comments =       {50 pages, Old title: Cram\'er type moderate deviation theorems for Studentized non-linear statistics},
  eprint =         {1405.1218},
  file =           {:Shao2014Cramer.pdf:PDF},
  oai2identifier = {1405.1218}
}

@Article{Sharpnack2014Mean,
  Title                    = {Mean and variance estimation in high-dimensional heteroscedastic models with non-convex penalties},
  Author                   = {James Sharpnack and } # mkolar,
  Journal                  = {ArXiv e-prints, arXiv:1410.7874},
  Year                     = {2014},

  Month                    = oct,

  Abstract                 = {Despite its prevalence in statistical datasets, heteroscedasticity (non-constant sample variances) has been largely ignored in the high-dimensional statistics literature. Recently, studies have shown that the Lasso can accommodate heteroscedastic errors, with minor algorithmic modifications (Belloni et al., 2012; Gautier and Tsybakov, 2013). In this work, we study heteroscedastic regression with linear mean model and log-linear variances model with sparse high-dimensional parameters. In this work, we propose estimating variances in a post-Lasso fashion, which is followed by weighted-least squares mean estimation. These steps employ non-convex penalties as in Fan and Li (2001), which allows us to prove oracle properties for both post-Lasso variance and mean parameter estimates. We reinforce our theoretical findings with experiments.},
  Eprint                   = {1410.7874},
  Oai2identifier           = {1410.7874},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@Article{Shen2012Likelihood,
  Title                    = {Likelihood-based Selection And Sharp Parameter Estimation},
  Author                   = {X. Shen and W. Pan and Y. Zhu},
  Journal                  = jasa_s,
  Year                     = {2012},
  Pages                    = {223-232},
  Volume                   = {107},

  Newspaper                = {J. Am. Stat. Assoc.}
}

@Article{Shimizu2006linear,
  author =   {Shimizu, Shohei and Hoyer, Patrik O. and Hyv{\"a}rinen, Aapo and Kerminen, Antti},
  title =    {A linear non-{G}aussian acyclic model for causal discovery},
  journal =  jmlr_s,
  year =     {2006},
  volume =   {7},
  pages =    {2003--2030},
  file =     {:Shimizu2006linear.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  issn =     {1532-4435},
  mrclass =  {68T05 (62H25)},
  mrnumber = {2274431}
}

@Article{Shojaie2010Discovering,
  author =    {A. Shojaie and G. Michailidis},
  title =     {Discovering graphical Granger causality using the truncating lasso penalty},
  journal =   {Bioinformatics},
  year =      {2010},
  volume =    {26},
  number =    {18},
  pages =     {i517--i523},
  month =     {sep},
  doi =       {10.1093/bioinformatics/btq377},
  file =      {Shojaie2010Discovering.pdf:Shojaie2010Discovering.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
  url =       {http://dx.doi.org/10.1093/bioinformatics/btq377}
}

@Article{Shojaie2010Penalized,
  author     = {Shojaie, Ali and Michailidis, George},
  title      = {Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs},
  journal    = {Biometrika},
  year       = {2010},
  volume     = {97},
  number     = {3},
  pages      = {519--538},
  issn       = {0006-3444},
  coden      = {BIOKAX},
  doi        = {10.1093/biomet/asq038},
  file       = {:Shojaie2010Penalized.pdf:PDF},
  fjournal   = {Biometrika},
  mrclass    = {62J07 (05C20 62H12 68T30)},
  mrnumber   = {2672481},
  mrreviewer = {Marcello Sanguineti},
  url        = {http://dx.doi.org/10.1093/biomet/asq038},
}

@Article{Shorack1979weighted,
  author =     {Shorack, Galen R.},
  title =      {The weighted empirical process of row independent random variables with arbitrary distribution functions},
  journal =    {Statist. Neerlandica},
  year =       {1979},
  volume =     {33},
  number =     {4},
  pages =      {169--189},
  coden =      {SNERAM},
  doi =        {10.1111/j.1467-9574.1979.tb00673.x},
  file =       {Shorack1979weighted.pdf:Shorack1979weighted.pdf:PDF},
  fjournal =   {Statistica Neerlandica. Orgaan van de Vereniging voor Statistiek},
  issn =       {0039-0402},
  mrclass =    {60F05 (62G30)},
  mrnumber =   {558170},
  mrreviewer = {M. D. Burke},
  url =        {http://dx.doi.org/10.1111/j.1467-9574.1979.tb00673.x}
}

@Article{Silvapulle1995score,
  Title                    = {A score test against one-sided alternatives},
  Author                   = {Silvapulle, Mervyn J. and Silvapulle, Paramsothy},
  Journal                  = jasa_s,
  Year                     = {1995},
  Number                   = {429},
  Pages                    = {342--349},
  Volume                   = {90},

  Coden                    = {JSTNAL},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62F03 (62M10)},
  Mrnumber                 = {1325141 (96e:62022)},
  Url                      = {http://links.jstor.org/sici?sici=0162-1459(199503)90:429<342:ASTAOA>2.0.CO;2-M&origin=MSN}
}

@Article{Silverman1978Weak,
  Title                    = {Weak and Strong Uniform Consistency of the Kernel Estimate of a Density and its Derivatives},
  Author                   = {Bernard W. Silverman},
  Journal                  = aos_s,
  Year                     = {1978},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {177-184},
  Volume                   = {6},

  Doi                      = {10.1214/aos/1176344076},
  ISSN                     = {0090-5364},
  Owner                    = {mkolar},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2014.06.04},
  Url                      = {http://dx.doi.org/10.1214/aos/1176344076}
}

@Article{Silverman1982estimation,
  Title                    = {On the estimation of a probability density function by the
 maximum penalized likelihood method},
  Author                   = {Silverman, B. W.},
  Journal                  = aos_s,
  Year                     = {1982},
  Number                   = {3},
  Pages                    = {795--810},
  Volume                   = {10},

  Coden                    = {ASTSC7},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05},
  Mrnumber                 = {663433 (84c:62059)},
  Mrreviewer               = {Gilbert Walter},
  Url                      = {http://links.jstor.org/sici?sici=0090-5364(198209)10:3<795:OTEOAP>2.0.CO;2-S&origin=MSN}
}

@Article{Simon2011Discriminant,
  Title                    = {Discriminant Analysis with Adaptively Pooled Covariance},
  Author                   = {Noah Simon and } # rtibs,
  Journal                  = {arXiv preprint arXiv:1111.1687},
  Year                     = {2011},

  Month                    = nov,

  Abstract                 = {Linear and Quadratic Discriminant analysis (LDA/QDA) are common tools for classification problems. For these methods we assume observations are normally distributed within group. We estimate a mean and covariance matrix for each group and classify using Bayes theorem. With LDA, we estimate a single, pooled covariance matrix, while for QDA we estimate a separate covariance matrix for each group. Rarely do we believe in a homogeneous covariance structure between groups, but often there is insufficient data to separately estimate covariance matrices. We propose L1- PDA, a regularized model which adaptively pools elements of the precision matrices. Adaptively pooling these matrices decreases the variance of our estimates (as in LDA), without overly biasing them. In this paper, we propose and discuss this method, give an efficient algorithm to fit it for moderate sized problems, and show its efficacy on real and simulated datasets.},
  Eprint                   = {1111.1687},
  Oai2identifier           = {1111.1687},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@InProceedings{siracusa2009tractable,
  Title                    = {Tractable Bayesian Inference Of Time-series Dependence Structure},
  Author                   = {M.~R. Siracusa and J.~W. Fisher},
  Booktitle                = {Proc. of AISTATS},
  Year                     = {2009}
}

@Article{Sjoebeck2001Alzheimers,
  Title                    = {Alzheimer's Disease and the Cerebellum: A Morphologic Study on Neuronal and Glial Changes},
  Author                   = {Martin Sj\"obeck and Elisabet Englund},
  Journal                  = {Dementia and Geriatric Cognitive Disorders},
  Year                     = {2001},
  Number                   = {3},
  Pages                    = {211-218},
  Volume                   = {12},

  Doi                      = {10.1159/000051260},
  Publisher                = {S. Karger AG},
  Url                      = {http://dx.doi.org/10.1159/000051260}
}

@Article{Slawski2013Non,
  author =   {Slawski, Martin and Hein, Matthias},
  title =    {Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization},
  journal =  ejs_s,
  year =     {2013},
  volume =   {7},
  pages =    {3004--3056},
  doi =      {10.1214/13-EJS868},
  file =     {:Slawski2013Non_supp.pdf:PDF;Slawski2013Non.pdf:Slawski2013Non.pdf:PDF},
  fjournal = {Electronic Journal of Statistics},
  issn =     {1935-7524},
  mrclass =  {62J05 (52B99 60B20 62F30)},
  mrnumber = {3151760},
  url =      {http://dx.doi.org/10.1214/13-EJS868}
}

@Article{Smale2007Learning,
  author =   {Smale, Steve and Zhou, Ding-Xuan},
  title =    {Learning theory estimates via integral operators and their approximations},
  journal =  {Constr. Approx.},
  year =     {2007},
  volume =   {26},
  number =   {2},
  pages =    {153--172},
  doi =      {10.1007/s00365-006-0659-y},
  file =     {:Smale2007Learning.pdf:PDF},
  fjournal = {Constructive Approximation. An International Journal for Approximations and Expansions},
  issn =     {0176-4276},
  mrclass =  {68T05 (42B10 68Q32 94A20)},
  mrnumber = {2327597},
  url =      {http://dx.doi.org/10.1007/s00365-006-0659-y}
}

@InProceedings{Sohn2012Joint,
  author =    {{Kyung-ah} Sohn and Seyoung Kim},
  title =     {Joint Estimation of Structured Sparsity and Output Structure in Multiple-Output Regression via Inverse-Covariance Regularization},
  booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  year =      {2012},
  editor =    {Neil D. Lawrence and Mark A. Girolami},
  volume =    {22},
  pages =     {1081-1089},
  file =      {:Sohn2012Joint.pdf:PDF},
  journal =   {Journal of Machine Learning Research - Workshop and Conference Proceedings},
  url =       {http://jmlr.csail.mit.edu/proceedings/papers/v22/sohn12/sohn12.pdf}
}

@Article{Song2011Large,
  author =         {Song Song and Peter J. Bickel},
  title =          {Large Vector Auto Regressions},
  journal =        {ArXiv e-prints, arXiv:1106.3915},
  year =           {2011},
  month =          jun,
  abstract =       {One popular approach for nonstructural economic and financial forecasting is to include a large number of economic and financial variables, which has been shown to lead to significant improvements for forecasting, for example, by the dynamic factor models. A challenging issue is to determine which variables and (their) lags are relevant, especially when there is a mixture of serial correlation (temporal dynamics), high dimensional (spatial) dependence structure and moderate sample size (relative to dimensionality and lags). To this end, an \textit{integrated} solution that addresses these three challenges simultaneously is appealing. We study the large vector auto regressions here with three types of estimates. We treat each variable's own lags different from other variables' lags, distinguish various lags over time, and is able to select the variables and lags simultaneously. We first show the consequences of using Lasso type estimate directly for time series without considering the temporal dependence. In contrast, our proposed method can still produce an estimate as efficient as an \textit{oracle} under such scenarios. The tuning parameters are chosen via a data driven "rolling scheme" method to optimize the forecasting performance. A macroeconomic and financial forecasting problem is considered to illustrate its superiority over existing estimators.},
  eprint =         {1106.3915},
  file =           {:Song2011Large.pdf:PDF},
  oai2identifier = {1106.3915},
  owner =          {mkolar},
  timestamp =      {2016.03.16}
}

@InProceedings{Soni2011Efficient,
  Title                    = {Efficient adaptive compressive sensing using sparse hierarchical learned dictionaries},
  Author                   = {Akshay Soni and } # jhaupt,
  Booktitle                = {Proc. 45th Conf. Signals, Systems and Computers},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {1250--1254},

  Owner                    = {mkolar},
  Timestamp                = {2014.02.18},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6190216}
}

@Article{Soni2013Fundamental,
  Title                    = {On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy Linear Measurements},
  Author                   = {Akshay Soni and } # jhaupt,
  Journal                  = {arXiv preprint arXiv:1306.4391},
  Year                     = {2013},

  Month                    = jun,

  Abstract                 = {Recent breakthrough results in compressive sensing (CS) have established that many high dimensional signals can be accurately recovered from a relatively small number of non-adaptive linear observations, provided that the signals possess a sparse representation in some basis. Subsequent efforts have shown that the performance of CS can be improved by exploiting additional structure in the locations of the nonzero signal coefficients during inference, or by utilizing some form of data-dependent adaptive measurement focusing during the sensing process. To our knowledge, our own previous work was the first to establish the potential benefits that can be achieved when fusing the notions of adaptive sensing and structured sparsity -- that work examined the task of support recovery from noisy linear measurements, and established that an adaptive sensing strategy specifically tailored to signals that are tree-sparse can significantly outperform adaptive and non-adaptive sensing strategies that are agnostic to the underlying structure. In this work we establish fundamental performance limits for the task of support recovery of tree-sparse signals from noisy measurements, in settings where measurements may be obtained either non-adaptively (using a randomized Gaussian measurement strategy motivated by initial CS investigations) or by any adaptive sensing strategy. Our main results here imply that the adaptive tree sensing procedure analyzed in our previous work is nearly optimal, in the sense that no other sensing and estimation strategy can perform fundamentally better for identifying the support of tree-sparse signals.},
  Comments                 = {33 pages, 5 figures, IEEE Transactions on Information Theory (accepted for publication)},
  Eprint                   = {1306.4391},
  Oai2identifier           = {1306.4391},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{spirtes00causation,
  Title                    = {Causation, Prediction, And Search},
  Author                   = {P. Spirtes and C. Glymour and R. Scheines},
  Publisher                = {MIT Press},
  Year                     = {2000},

  Address                  = {Cambridge, MA},
  Edition                  = {Second},
  Note                     = {With additional material by David Heckerman, Christopher Meek, Gregory F. Cooper and Thomas Richardson, A Bradford Book},
  Series                   = {Adaptive Computation and Machine Learning},

  ISBN                     = {0-262-19440-6},
  Mrnumber                 = {1815675 (2001j:62009)},
  Pages                    = {xxii+543}
}

@Article{Spyromitros-Xioufis2012Multi,
  author =         {Eleftherios Spyromitros-Xioufis and Grigorios Tsoumakas and William Groves and Ioannis Vlahavas},
  title =          {Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs},
  journal =        {ArXiv e-prints, arXiv:1211.6581},
  year =           {2012},
  month =          nov,
  abstract =       {In many practical applications of supervised learning the task involves the prediction of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are continuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classification domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called Stacked Single-Target and Ensemble of Regressor Chains, by adapting two popular multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods - a discrepancy of the values of the additional input variables between training and prediction - and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent improvements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods including regularization-based multi-task learning methods and a multi-objective random forest approach.},
  comments =       {Accepted for publication in Machine Learning journal. This replacement contains major improvements compared to the previous version, including a deeper theoretical and experimental analysis and an extended discussion of related work},
  doi =            {10.1007/s10994-016-5546-z},
  eprint =         {1211.6581},
  file =           {:Spyromitros-Xioufis2012Multi.pdf:PDF},
  oai2identifier = {1211.6581}
}

@InProceedings{srebro01ml,
  Title                    = {Maximum Likelihood Bounded Tree-width Markov Networks},
  Author                   = {N. Srebro},
  Booktitle                = {Proc. of UAI},
  Year                     = {2001},

  Address                  = {San Francisco, CA, USA},
  Pages                    = {504--511},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Series                   = {UAI'01},

  ISBN                     = {1-55860-800-1},
  Location                 = {Seattle, Washington},
  Url                      = {http://dl.acm.org/citation.cfm?id=2074022.2074084}
}

@Article{Sriperumbudur2013Density,
  author    = {Sriperumbudur, Bharath and Fukumizu, Kenji and Gretton, Arthur and Hyv\"{a}rinen, Aapo and Kumar, Revant},
  title     = {Density estimation in infinite dimensional exponential families},
  journal   = {J. Mach. Learn. Res.},
  year      = {2017},
  volume    = {18},
  pages     = {Paper No. 57, 59},
  issn      = {1532-4435},
  file      = {:Sriperumbudur2013Density_jmlr.pdf:PDF;:Sriperumbudur2013Density.pdf:PDF},
  fjournal  = {Journal of Machine Learning Research (JMLR)},
  mrclass   = {62G07 (62G20)},
  mrnumber  = {3687600},
  timestamp = {2019.05.02},
}

@Article{stadler2009missing,
  Title                    = {Missing Values: Sparse Inverse Covariance Estimation And An Extension To Sparse Regression},
  Author                   = {N. St{\"a}dler and } # pbuhl,
  Journal                  = {Stat. Comput.},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {219--235},
  Volume                   = {22},

  Doi                      = {10.1007/s11222-010-9219-7},
  ISSN                     = {0960-3174},
  Mrnumber                 = {2865066},
  Newspaper                = {Stat. Comput.},
  Url                      = {http://dx.doi.org/10.1007/s11222-010-9219-7}
}

@InProceedings{Steinwart2009Optimal,
  author =    {Steinwart, Ingo and Hush, Don R and Scovel, Clint},
  title =     {Optimal Rates for Regularized Least Squares Regression.},
  booktitle = {COLT},
  year =      {2009},
  file =      {:Steinwart2009Optimal.pdf:PDF},
  owner =     {mkolar},
  timestamp = {2016.03.10}
}

@Article{Steinwart2012Mercers,
  author =    {Ingo Steinwart and Clint Scovel},
  title =     {Mercer's Theorem on General Domains: On the Interaction between Measures, Kernels, and {RKHSs}},
  journal =   {Constr Approx},
  year =      {2012},
  volume =    {35},
  number =    {3},
  pages =     {363--417},
  month =     {feb},
  doi =       {10.1007/s00365-012-9153-3},
  file =      {:Steinwart2012Mercers.pdf:PDF},
  publisher = {Springer Science $\mathplus$ Business Media},
  url =       {http://dx.doi.org/10.1007/s00365-012-9153-3}
}

@Article{Stern2006Cognitive,
  Title                    = {Cognitive Reserve And Alzheimer Disease},
  Author                   = {Yaakov Stern},
  Journal                  = {Alzheimer Disease \& Associated Disorders},
  Year                     = {2006},
  Number                   = {2},
  Pages                    = {112--117},
  Volume                   = {20},

  Newspaper                = {Alzheimer Disease \& Associated Disorders},
  Publisher                = {LWW}
}

@Book{stewart90matrix,
  Title                    = {Matrix Perturbation Theory},
  Author                   = {G.~W. Stewart and J.~G. Sun},
  Publisher                = {Academic Press Inc.},
  Year                     = {1990},

  Address                  = {Boston, MA},
  Series                   = {Computer Science and Scientific Computing},

  ISBN                     = {0-12-670230-6},
  Mrnumber                 = {1061154 (92a:65017)},
  Pages                    = {xvi+365}
}

@Article{Stone1985Additive,
  Title                    = {Additive regression and other nonparametric models},
  Author                   = {Charles J. Stone},
  Journal                  = aos_s,
  Year                     = {1985},
  Number                   = {2},
  Pages                    = {689--705},
  Volume                   = {13},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/aos/1176349548},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62J02 (62G05)},
  Mrnumber                 = {790566 (87i:62111)},
  Mrreviewer               = {K. C. Chanda},
  Url                      = {http://dx.doi.org/10.1214/aos/1176349548}
}

@Article{Stone1997Polynomial,
  Title                    = {Polynomial splines and their tensor products in extended
 linear modeling},
  Author                   = {Stone, Charles J. and Hansen, Mark H. and Kooperberg, Charles
 and Truong, Young K.},
  Journal                  = aos_s,
  Year                     = {1997},
  Note                     = {With discussion and a rejoinder by the authors and Jianhua Z.
 Huang},
  Number                   = {4},
  Pages                    = {1371--1470},
  Volume                   = {25},

  Coden                    = {ASTSC7},
  Doi                      = {10.1214/aos/1031594728},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62G07 62J12)},
  Mrnumber                 = {1463561 (99e:62060)},
  Url                      = {http://dx.doi.org/10.1214/aos/1031594728}
}

@Article{Storey2004Strong,
  author =     {Storey, John D. and Taylor, Jonathan E. and Siegmund, David},
  title =      {Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: a unified approach},
  journal =    JRSSB_s,
  year =       {2004},
  volume =     {66},
  number =     {1},
  pages =      {187--205},
  doi =        {10.1111/j.1467-9868.2004.00439.x},
  fjournal =   {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  issn =       {1369-7412},
  mrclass =    {62F03 (62F10)},
  mrnumber =   {2035766},
  mrreviewer = {Goetz Trenkler},
  url =        {http://dx.doi.org/10.1111/j.1467-9868.2004.00439.x}
}

@Article{Sun2012Robust,
  Title                    = {Robust Gaussian Graphical Modeling Via $\ell_1$ Penalization},
  Author                   = {Hokeun Sun and Hongzhe Li},
  Journal                  = {Biometrics},
  Year                     = {2012},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {1197--1206},
  Volume                   = {68},

  Doi                      = {10.1111/j.1541-0420.2012.01785.x},
  ISSN                     = {0006-341X},
  Owner                    = {mkolar},
  Publisher                = {Wiley-Blackwell},
  Timestamp                = {2014.05.05},
  Url                      = {http://dx.doi.org/10.1111/j.1541-0420.2012.01785.x}
}

@InCollection{Sun2015Learning,
  author =    {Siqi Sun and Mladen Kolar and Jinbo Xu},
  title =     {Learning structured densities via infinite dimensional exponential families},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year =      {2015},
  editor =    {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages =     {2287--2295},
  url =       {http://papers.nips.cc/paper/6006-learning-structured-densities-via-infinite-dimensional-exponential-families.pdf}
}

@Article{Sun2012Scaled,
  author    = {Tingni Sun and } # chzhang,
  title     = {Scaled sparse linear regression},
  journal   = {Biometrika},
  year      = {2012},
  volume    = {99},
  number    = {4},
  pages     = {879-898},
  month     = {Nov},
  doi       = {10.1093/biomet/ass043},
  file      = {:Sun2012Scaled.pdf:PDF},
  publisher = {Oxford University Press (OUP)},
  timestamp = {2014.02.13},
  url       = {http://dx.doi.org/10.1093/biomet/ass043},
}

@Article{Sun2012Sparse,
  author     = {Sun, Tingni and Zhang, Cun-Hui},
  title      = {Sparse matrix inversion with scaled lasso},
  journal    = {J. Mach. Learn. Res.},
  year       = {2013},
  volume     = {14},
  pages      = {3385--3418},
  issn       = {1532-4435},
  file       = {:Sun2012Sparse.pdf:PDF},
  fjournal   = {Journal of Machine Learning Research (JMLR)},
  mrclass    = {62H12 (62F12 62J07)},
  mrnumber   = {3144466},
  mrreviewer = {Irina Irincheeva},
}

@Article{Sun2013maximal,
  Title                    = {On the maximal size of Large-Average and {ANOVA}-fit Submatrices in a Gaussian Random Matrix},
  Author                   = {Xing Sun and } # anobel,
  Journal                  = {Bernoulli},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {275--294},
  Volume                   = {19},

  Owner                    = {mkolar},
  Publisher                = {NIH Public Access},
  Timestamp                = {2014.02.18}
}

@Article{Tanczos2013Adaptive,
  Title                    = {Adaptive Sensing for Estimation of Structured Sparse Signals},
  Author                   = {Ervin T\'{a}nczos and } # rcastro,
  Journal                  = {arXiv preprint arXiv:1311.7118},
  Year                     = {2013},

  Month                    = nov,

  Abstract                 = {In many practical settings one can sequentially and adaptively guide the collection of future data, based on information extracted from data collected previously. These sequential data collection procedures are known by different names, such as sequential experimental design, active learning or adaptive sensing/sampling. The intricate relation between data analysis and acquisition in adaptive sensing paradigms can be extremely powerful, and often allows for reliable signal estimation and detection in situations where non-adaptive sensing would fail dramatically. In this work we investigate the problem of estimating the support of a structured sparse signal from coordinate-wise observations under the adaptive sensing paradigm. We present a general procedure for support set estimation that is optimal in a variety of cases and shows that through the use of adaptive sensing one can: (i) mitigate the effect of observation noise when compared to non-adaptive sensing and, (ii) capitalize on structural information to a much larger extent than possible with non-adaptive sensing. In addition to a general procedure to perform adaptive sensing in structured settings we present both performance upper bounds, and corresponding lower bounds for both sensing paradigms.},
  Eprint                   = {1311.7118},
  Oai2identifier           = {1311.7118},
  Owner                    = {mkolar},
  Timestamp                = {2014.02.18}
}

@Book{Talagr2005Generic,
  Title                    = {The Generic Chaining: Upper and Lower Bounds of Stochastic Processes},
  Author                   = {Michel Talagrand},
  Publisher                = {Springer},
  Year                     = {2005},

  ISBN                     = {3540245189},
  Url                      = {http://www.amazon.com/The-Generic-Chaining-Stochastic-Mathematics/dp/3540245189%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D3540245189}
}

@Article{talih2005structural,
  Title                    = {Structural Learning With Time-varying Components: Tracking The Cross-section Of The Financial Time Series},
  Author                   = {M. Talih and N. Hengartner},
  Journal                  = jrssb_s,
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {321--341},
  Volume                   = {67},

  Doi                      = {10.1111/j.1467-9868.2005.00504.x},
  ISSN                     = {1369-7412},
  Mrnumber                 = {2155341},
  Newspaper                = {J. R. Stat. Soc. B},
  Url                      = {http://dx.doi.org/10.1111/j.1467-9868.2005.00504.x}
}

@Article{Tan2010Bounded,
  author =   {Tan, Zhiqiang},
  title =    {Bounded, efficient and doubly robust estimation with inverse weighting},
  journal =  {Biometrika},
  year =     {2010},
  volume =   {97},
  number =   {3},
  pages =    {661--682},
  coden =    {BIOKAX},
  doi =      {10.1093/biomet/asq035},
  file =     {:Tan2010Bounded.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62N01 (62G05 62G35)},
  mrnumber = {2672490 (2012d:62332)},
  url =      {http://dx.doi.org/10.1093/biomet/asq035}
}

@Article{Tang2012unified,
  Title                    = {A unified variable selection approach for varying coefficient models},
  Author                   = {Yanlin Tang and Huixia Judy Wang and Zhongyi Zhu and Xinyuan Song},
  Journal                  = statsin_s,
  Year                     = {2012},

  Month                    = {Apr},
  Number                   = {2},
  Volume                   = {22},

  Doi                      = {10.5705/ss.2010.121},
  Owner                    = {mkolar},
  Publisher                = {Institute of Statistical Science, Academia Sinica},
  Timestamp                = {2014.02.13},
  Url                      = {http://dx.doi.org/10.5705/ss.2010.121}
}

@InProceedings{thrun1996discovering,
  Title                    = {Discovering Structure In Multiple Learning Tasks: The Tc Algorithm},
  Author                   = {S. Thrun and J. O'Sullivan},
  Booktitle                = {Proc. of ICML},
  Year                     = {1996},
  Organization             = {MORGAN KAUFMANN PUBLISHERS, INC.},
  Pages                    = {489--497}
}

@Article{tropp2004greed,
  author =    {J.~A. Tropp},
  title =     {Greed Is Good: Algorithmic Results For Sparse Approximation},
  journal =   {IEEEit},
  year =      {2004},
  volume =    {50},
  number =    {10},
  pages =     {2231--2242},
  doi =       {10.1109/TIT.2004.834793},
  file =      {:tropp2004greed.pdf:PDF},
  issn =      {0018-9448},
  mrnumber =  {2097044 (2005e:94036)},
  newspaper = {IEEEit},
  url =       {http://dx.doi.org/10.1109/TIT.2004.834793}
}

@Article{tropp06algorithms_part_1,
  Title                    = {Algorithms For Simultaneous Sparse Approximation. Part I: Greedy Pursuit},
  Author                   = {J.~A. Tropp and A.~C. Gilbert and M.~J. Strauss},
  Journal                  = {Signal Proces.},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {572--588},
  Volume                   = {86},

  Newspaper                = {Signal Proces.},
  Publisher                = {Elsevier}
}

@Article{Tseng2001Convergence,
  Title                    = {Convergence Of A Block Coordinate Descent Method For Nondifferentiable Minimization},
  Author                   = {P. Tseng},
  Journal                  = {J. Optim. Theory Appl.},
  Year                     = {2001},
  Number                   = {3},
  Pages                    = {475--494},
  Volume                   = {109},

  Address                  = {New York, NY, USA},
  ISSN                     = {0022-3239},
  Newspaper                = {J. Optim. Theory Appl.},
  Publisher                = {Plenum Press}
}

@Book{Tsiatis2006Semiparametric,
  title =      {Semiparametric theory and missing data},
  publisher =  {Springer, New York},
  year =       {2006},
  author =     {Tsiatis, Anastasios A.},
  series =     {Springer Series in Statistics},
  isbn =       {0-387-32448-8; 978-0387-32448-7},
  mrclass =    {62-02 (62F10 62G05 62N01)},
  mrnumber =   {2233926 (2007g:62009)},
  mrreviewer = {Jean-Fran{\c{c}}ois Dupuy},
  pages =      {xvi+383}
}

@Article{Tsukahara2005Semiparametric,
  author =    {Tsukahara, Hideatsu},
  title =     {Semiparametric estimation in copula models},
  journal =   {Canadian Journal of Statistics},
  year =      {2005},
  volume =    {33},
  number =    {3},
  pages =     {357--375},
  publisher = {Wiley Online Library}
}

@Article{Tur2014Mapping,
  Title                    = {Mapping eQTL networks with mixed graphical models},
  Author                   = {Inma Tur and Alberto Roverato and Robert Castelo},
  Journal                  = {ArXiv e-prints, arXiv:1402.4547},
  Year                     = {2014},

  Month                    = feb,

  Abstract                 = {Expression quantitative trait loci (eQTL) mapping constitutes a challenging problem due to, among other reasons, the high-dimensional multivariate nature of gene expression traits. Next to the expression heterogeneity produced by confounding factors and other sources of unwanted variation, indirect effects spread throughout genes as a result of genetic, molecular and environmental perturbations. Disentangling direct from indirect effects while adjusting for unwanted variability should help us moving from current parts list of molecular components to understanding how these components work together. In this paper we approach this challenge with mixed graphical Markov models and higher-order conditional independences. To unlock this methodological framework we derive the parameters for an exact likelihood ratio test and demonstrate its fundamental relevance for higher-order conditioning on continuous expression and discrete genotypes. These models show that additive genetic effects propagate through the network as function of gene-gene correlations. The estimation of the eQTL network underlying a well-studied yeast data set using our methodology leads to a sparse structure with more direct genetic and regulatory associations that enable a straightforward comparison of the genetic control of gene expression across chromosomes. More importantly, it reveals that the larger genetic effects are trans-acting on genes located in a different chromosome and with a high number of connections to other genes in the network.},
  Comments                 = {48 pages, 8 figures, 2 supplementary figures; fixed problems with embedded fonts; figure 7 sideways for improving display; minor fixes},
  Eprint                   = {1402.4547},
  Oai2identifier           = {1402.4547},
  Owner                    = {mkolar},
  Timestamp                = {2014.05.05}
}

@Article{turlach2005simultaneous,
  author =    {B.~A. Turlach and W.~N. Venables and S.~J. Wright},
  title =     {Simultaneous Variable Selection},
  journal =   {Technometrics},
  year =      {2005},
  volume =    {47},
  number =    {3},
  pages =     {349--363},
  doi =       {10.1198/004017005000000139},
  file =      {:turlach2005simultaneous:PDF},
  issn =      {0040-1706},
  mrnumber =  {2164706},
  newspaper = {Technometrics},
  url =       {http://dx.doi.org/10.1198/004017005000000139}
}

@Article{Turnbull2008Semantic,
  author =    {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
  title =     {Semantic annotation and retrieval of music and sound effects},
  journal =   {IEEE Transactions on Acoustics, Speech and Signal Processing},
  year =      {2008},
  volume =    {16},
  number =    {2},
  pages =     {467--476},
  file =      {:Turnbull2008Semantic.pdf:PDF},
  publisher = {IEEE}
}

@Article{vanDuijn09framework,
  Title                    = {A Framework For The Comparison Of Maximum Pseudo-likelihood And Maximum Likelihood Estimation Of Exponential Family Random Graph Models},
  Author                   = {M.~A.~J. {van Duijn} and K.~J. Gile and M.~S. Handcock},
  Journal                  = {Social Networks},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {52--62},
  Volume                   = {31},

  Newspaper                = {Social Networks},
  Publisher                = {Elsevier}
}

@Misc{vanHandel2014Probability,
  Title                    = {Probability in High Dimensions: Lecture Notes},

  Author                   = {Ramon {van Handel}},
  Year                     = {2014},

  Journal                  = {Lecture Notes}
}

@Article{Varin2011overview,
  Title                    = {An overview of composite likelihood methods},
  Author                   = {Varin, Cristiano and Reid, Nancy and Firth, David},
  Journal                  = statsin_s,
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {5--42},
  Volume                   = {21},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62-02 (62F10 62M40)},
  Mrnumber                 = {2796852 (2012b:62007)}
}

@Article{Varin2005note,
  Title                    = {A note on composite likelihood inference and model selection},
  Author                   = {Varin, Cristiano and Vidoni, Paolo},
  Journal                  = {Biometrika},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {519--528},
  Volume                   = {92},

  Coden                    = {BIOKAX},
  Doi                      = {10.1093/biomet/92.3.519},
  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {62F10 (62B10 62F15)},
  Mrnumber                 = {2202643},
  Url                      = {http://dx.doi.org/10.1093/biomet/92.3.519}
}

@InProceedings{Varoquaux2010Brain,
  Title                    = {Brain Covariance Selection: Better Individual Functional Connectivity Models Using Population Prior},
  Author                   = {G. Varoquaux and A. Gramfort and J.-B. Poline and B. Thirion},
  Booktitle                = {Proc. of NIPS},
  Year                     = {2010},
  Editor                   = {John D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.~S. Zemel and A. Culotta},
  Pages                    = {2334--2342}
}

@InCollection{Vershynin2012Introduction,
  author    = {Vershynin, Roman},
  title     = {Introduction to the non-asymptotic analysis of random matrices},
  booktitle = {Compressed sensing},
  publisher = {Cambridge Univ. Press, Cambridge},
  year      = {2012},
  pages     = {210--268},
  file      = {:Vershynin2012Introduction.pdf:PDF},
  mrclass   = {60B20 (15B52 62H12)},
  mrnumber  = {2963170},
}

@InProceedings{vogel09on,
  Title                    = {On Robust Gaussian Graphical Modelling},
  Author                   = {D. Vogel and R. Fried},
  Booktitle                = {Recent Developments in Applied Probability and Statistics},
  Year                     = {2010},
  Editor                   = {L. Devroye et al. (Eds.)},
  Pages                    = {155-182},
  Publisher                = {Berlin, Heidelberg: Springer-Verlag}
}

@Article{Vogel2011Elliptical,
  Title                    = {Elliptical graphical modelling},
  Author                   = {Vogel, D. and Fried, R.},
  Journal                  = {Biometrika},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {935--951},
  Volume                   = {98},

  Coden                    = {BIOKAX},
  Doi                      = {10.1093/biomet/asr037},
  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {62H12 (62F12 62H15)},
  Mrnumber                 = {2860334},
  Mrreviewer               = {Sanjay Chaudhuri},
  Url                      = {http://dx.doi.org/10.1093/biomet/asr037}
}

@Article{Voorman2014Graph,
  author    = {Arend Voorman and Ali Shojaie and } # dwitten,
  title     = {Graph estimation with joint additive models},
  journal   = {Biometrika},
  year      = {2014},
  volume    = {101},
  number    = {1},
  pages     = {85--101},
  month     = {Mar},
  issn      = {1464-3510},
  doi       = {10.1093/biomet/ast053},
  file      = {Voorman2014Graph.pdf:Voorman2014Graph.pdf:PDF},
  owner     = {mkolar},
  publisher = {Oxford University Press (OUP)},
  timestamp = {2014.05.05},
  url       = {http://dx.doi.org/10.1093/biomet/ast053},
}

@Article{Vu2011Singular,
  author =    {Van Vu},
  title =     {Singular vectors under random perturbation},
  journal =   {Random Struct. Alg.},
  year =      {2011},
  volume =    {39},
  number =    {4},
  pages =     {526--538},
  month =     {may},
  doi =       {10.1002/rsa.20367},
  file =      {Vu2011Singular.pdf:Vu2011Singular.pdf:PDF},
  owner =     {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.03.01},
  url =       {http://dx.doi.org/10.1002/rsa.20367}
}

@Article{Wahl2014Optimal,
  Title                    = {Optimal estimation of components in structured nonparametric models},
  Author                   = {Martin Wahl},
  Journal                  = {ArXiv e-prints, arXiv:1403.1088},
  Year                     = {2014},

  Month                    = mar,

  Abstract                 = {We consider the nonparametric random regression model $Y=f_1(X_1)+f_2(X_2)+\epsilon$ in which the function $f_1$ is the parameter of interest and the function $f_2$ is a nuisance parameter. We present a theory for estimating $f_1$ in settings where $f_2(X_2)$ is more complex than $f_1(X_1)$. The proposed estimation procedure combines two least squares criteria and can be written as an alternating projection procedure. We derive nonasymptotic risk bounds which reveal connections between the performance of our estimators of $f_1$ and the notions of minimal angles and Hilbert-Schmidt operators in the theory of Hilbert spaces. Under additional regularity conditions on the design densities, these bounds can be further improved. Our results establish general assumptions under which the estimators of $f_1$ have up to first order the same sharp upper bound as the corresponding estimators in the model $Y=f_1(X_1)+\epsilon$. As an example we apply the results to an additive model where the number of covariates is large or the nuisance components are considerably less smooth than $f_1$.},
  Comments                 = {33 pages},
  Eprint                   = {1403.1088},
  Oai2identifier           = {1403.1088}
}

@Article{Wahl2014Variable,
  Title                    = {Variable selection in high-dimensional additive models based on norms of projections},
  Author                   = {Martin Wahl},
  Journal                  = {ArXiv e-prints, arXiv:1406.0052},
  Year                     = {2014},

  Month                    = jun,

  Abstract                 = {We consider the problem of variable selection in high-dimensional sparse additive models. The proposed method is motivated by geometric considerations in Hilbert spaces, and consists in comparing the norms of the projections of the data on various additive subspaces. Our main results are concentration inequalities which lead to conditions making variable selection possible. In special cases these conditions are known to be optimal. As an application we consider the problem of estimating single components. We show that, up to first order, one can estimate a single component as well as if the other components were known.},
  Comments                 = {24 pages},
  Eprint                   = {1406.0052},
  Oai2identifier           = {1406.0052}
}

@Article{Wahlberg2012ADMM,
  Title                    = {An {ADMM} algorithm for a class of total variation regularized estimation problems},
  Author                   = {Bo Wahlberg and } # sboyd # { and Mariette Annergren and Yang Wang},
  Journal                  = {arXiv preprint arXiv:1203.1828},
  Year                     = {2012}
}

@Article{Wang2007Regression,
  author =     {Wang, Hansheng and Li, Guodong and Tsai, Chih-Ling},
  title =      {Regression coefficient and autoregressive order shrinkage and selection via the lasso},
  journal =    JRSSB_s,
  year =       {2007},
  volume =     {69},
  number =     {1},
  pages =      {63--78},
  doi =        {10.1111/j.1467-9868.2007.00577.x},
  file =       {Wang2007Regression.pdf:Wang2007Regression.pdf:PDF},
  fjournal =   {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  issn =       {1369-7412},
  mrclass =    {62J07 (62M10)},
  mrnumber =   {2301500},
  mrreviewer = {Abdolrahman Rasekh},
  url =        {http://dx.doi.org/10.1111/j.1467-9868.2007.00577.x}
}

@Article{Wang2013Joint,
  author =         {Junhui Wang},
  title =          {Joint estimation of sparse multivariate regression and conditional graphical models},
  journal =        {ArXiv e-prints, arXiv:1306.4410},
  year =           {2013},
  month =          jun,
  abstract =       {Multivariate regression model is a natural generalization of the classical univari- ate regression model for ?tting multiple responses. In this paper, we propose a high- dimensional multivariate conditional regression model for constructing sparse estimates of the multivariate regression coe?cient matrix that accounts for the dependency struc- ture among the multiple responses. The proposed method decomposes the multivariate regression problem into a series of penalized conditional log-likelihood of each response conditioned on the covariates and other responses. It allows simultaneous estimation of the sparse regression coe?cient matrix and the sparse inverse covariance matrix. The asymptotic selection consistency and normality are established for the diverging dimension of the covariates and number of responses. The e?ectiveness of the pro- posed method is also demonstrated in a variety of simulated examples as well as an application to the Glioblastoma multiforme cancer data.},
  comments =       {29 pages, 1 figure},
  eprint =         {1306.4410},
  file =           {:Wang2013Joint.pdf:PDF},
  oai2identifier = {1306.4410}
}

@InProceedings{Wang2016Inference,
  author    = {Jialei Wang and Mladen Kolar},
  title     = {Inference for High-dimensional Exponential Family Graphical Models},
  booktitle = {Proc. of AISTATS},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  pages     = {751--760},
}

@Article{Wang2014Inference,
  Title                    = {Inference for Sparse Conditional Precision Matrices},
  Author                   = {Jialei Wang and } # mkolar,
  Journal                  = {ArXiv e-prints, arXiv:1412.7638},
  Year                     = {2014},

  Month                    = dec,

  Abstract                 = {Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is a high-dimensional vector and $Z$ is a low-dimensional index variable, we study the problem of estimating the conditional inverse covariance matrix $\Omega(z) = (E[(X-E[X \mid Z])(X-E[X \mid Z])^T \mid Z=z])^{-1}$ under the assumption that the set of non-zero elements is small and does not depend on the index variable. We develop a novel procedure that combines the ideas of the local constant smoothing and the group Lasso for estimating the conditional inverse covariance matrix. A proximal iterative smoothing algorithm is used to solve the corresponding convex optimization problems. We prove that our procedure recovers the conditional independence assumptions of the distribution $X \mid Z$ with high probability. This result is established by developing a uniform deviation bound for the high-dimensional conditional covariance matrix from its population counterpart, which may be of independent interest. Furthermore, we develop point-wise confidence intervals for individual elements of the conditional inverse covariance matrix. We perform extensive simulation studies, in which we demonstrate that our proposed procedure outperforms sensible competitors. We illustrate our proposal on a S&P 500 stock price data set.},
  Eprint                   = {1412.7638},
  Oai2identifier           = {1412.7638},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@InProceedings{Wang2015Distributed,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srerbo},
  title     = {Distributed Multi-Task Learning},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  series    = {Proceedings of Machine Learning Research},
  pages     = {751--760},
  address   = {Cadiz, Spain},
  month     = {09--11 May},
  publisher = {PMLR},
  abstract  = {We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space, where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.},
  file      = {:Wang2015Distributed.pdf:PDF;wang16d.pdf:http\://proceedings.mlr.press/v51/wang16d.pdf:PDF},
  url       = {http://proceedings.mlr.press/v51/wang16d.html},
}

@Article{Wang2016Distributed,
  author         = {Jialei Wang and Mladen Kolar and Nathan Srebro},
  title          = {Distributed Multi-Task Learning with Shared Representation},
  journal        = {Technical report},
  year           = {2016},
  month          = mar,
  abstract       = {We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.},
  eprint         = {1603.02185},
  oai2identifier = {1603.02185},
}

@InProceedings{Wang2016Efficient,
  author    = {Jialei Wang and Mladen Kolar and Nathan Srebro and Tong Zhang},
  title     = {Efficient Distributed Learning with Sparsity},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3636--3645},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
  abstract  = {We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted $\ell_1$ regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.},
  file      = {wang17f.pdf:http\://proceedings.mlr.press/v70/wang17f/wang17f.pdf:PDF},
  url       = {http://proceedings.mlr.press/v70/wang17f.html},
}

@InProceedings{Wang2016Sketching,
  author    = {Jialei Wang and Jason Lee and Mehrdad Mahdavi and Mladen Kolar and Nati Srebro},
  title     = {{Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data}},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year      = {2017},
  editor    = {Aarti Singh and Jerry Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1150--1158},
  address   = {Fort Lauderdale, FL, USA},
  month     = {20--22 Apr},
  publisher = {PMLR},
  abstract  = {Sketching techniques scale up machine learning algorithms by reducing the sample size or dimensionality of massive data sets, without sacrificing their statistical properties. In this paper, we study sketching from an optimization point of view. We first show that the iterative Hessian sketch is an optimization process with \emphpreconditioning and develop an \emphaccelerated version using this insight together with conjugate gradient descent. Next, we establish a primal-dual connection between the Hessian sketch and dual random projection, which allows us to develop an \emphaccelerated iterative dual random projection method by applying the preconditioned conjugate gradient descent on the dual problem. Finally, we tackle the problems of large sample size and high-dimensionality in massive data sets by developing the \emphprimal-dual sketch.  The primal-dual sketch iteratively sketches the primal and dual formulations and requires only a logarithmic number of calls to solvers of small sub-problems to recover the optimum of the original problem up to \empharbitrary precision. Extensive experiments on synthetic and real data sets complement our theoretical results.},
  file      = {wang17d.pdf:http\://proceedings.mlr.press/v54/wang17d/wang17d.pdf:PDF},
  url       = {http://proceedings.mlr.press/v54/wang17d.html},
}

@Article{Wang2013penalized,
  author =    {Lie Wang},
  title =     {The penalized LAD estimator for high dimensional linear regression},
  journal =   jma_s,
  year =      {2013},
  volume =    {120},
  pages =     {135--151},
  doi =       {10.1016/j.jmva.2013.04.001},
  file =      {Wang2013penalized.pdf:Wang2013penalized.pdf:PDF},
  issn =      {0047-259X},
  publisher = {Elsevier BV},
  url =       {http://dx.doi.org/10.1016/j.jmva.2013.04.001}
}

@Article{Wang2013Calibrating,
  author =   {Lan Wang and Yongdai Kim and } # rli,
  title =    {Calibrating nonconvex penalized regression in ultra-high dimension},
  journal =  aos_s,
  year =     {2013},
  volume =   {41},
  number =   {5},
  pages =    {2505--2536},
  doi =      {10.1214/13-AOS1159},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62J05 (62J07)},
  mrnumber = {3127873},
  url =      {http://dx.doi.org/10.1214/13-AOS1159}
}

@Article{Wang2008Variable,
  Title                    = {Variable Selection in Nonparametric Varying-Coefficient Models for Analysis of Repeated Measurements},
  Author                   = {Lifeng Wang and Hongzhe Li and Jianhua Z. Huang},
  Journal                  = jasa_s,
  Year                     = {2008},

  Month                    = {Dec},
  Number                   = {484},
  Pages                    = {1556-1569},
  Volume                   = {103},

  Doi                      = {10.1198/016214508000000788},
  Publisher                = {Informa UK (American Statistical Association)},
  Url                      = {http://dx.doi.org/10.1198/016214508000000788}
}

@Article{Wang2012Quantile,
  Title                    = {Quantile regression for analyzing heterogeneity in ultra-high dimension},
  Author                   = {Lan Wang and Yichao Wu and } # rli,
  Journal                  = jasa_s,
  Year                     = {2012},
  Number                   = {497},
  Pages                    = {214--222},
  Volume                   = {107},

  Coden                    = {JSTNAL},
  Doi                      = {10.1080/01621459.2012.656014},
  Fjournal                 = {Journal of the American Statistical Association},
  ISSN                     = {0162-1459},
  Mrclass                  = {62J07 (62G08 62G20)},
  Mrnumber                 = {2949353},
  Mrreviewer               = {Keming Yu},
  Url                      = {http://dx.doi.org/10.1080/01621459.2012.656014}
}

@Article{wang09learning,
  Title                    = {Learning Networks From High Dimensional Binary Data: An Application To Genomic Instability Data},
  Author                   = {P. Wang and D.~L. Chao and L. Hsu},
  Journal                  = {ArXiv e-prints, arXiv:0908.3882},
  Year                     = {2009},

  Newspaper                = {ArXiv e-prints, arXiv:0908.3882}
}

@Article{Wang2004Weighted,
  author =     {Wang, Qiying and Jing, Bing-Yi},
  title =      {Weighted bootstrap for {$U$}-statistics},
  journal =    {J. Multivariate Anal.},
  year =       {2004},
  volume =     {91},
  number =     {2},
  pages =      {177--198},
  doi =        {10.1016/j.jmva.2004.01.002},
  file =       {Wang2004Weighted.pdf:Wang2004Weighted.pdf:PDF},
  fjournal =   {Journal of Multivariate Analysis},
  issn =       {0047-259X},
  mrclass =    {62E20 (60F05)},
  mrnumber =   {2087842},
  mrreviewer = {Vydas {\v{C}}ekanavi{\v{c}}ius},
  url =        {http://dx.doi.org/10.1016/j.jmva.2004.01.002}
}

@Article{wang2007improved,
  Title                    = {Improved centroids estimation for the nearest shrunken centroid classifier},
  Author                   = {S. Wang and J. Zhu},
  Journal                  = {Bioinformatics},
  Year                     = {2007},
  Number                   = {8},
  Pages                    = {972},
  Volume                   = {23},

  Publisher                = {Oxford Univ Press}
}

@Article{Wang2008Conditionally,
  Title                    = {Conditionally specified continuous distributions},
  Author                   = {Wang, Yuchung J. and Ip, Edward H.},
  Journal                  = {Biometrika},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {735--746},
  Volume                   = {95},

  Coden                    = {BIOKAX},
  Doi                      = {10.1093/biomet/asn029},
  Fjournal                 = {Biometrika},
  ISSN                     = {0006-3444},
  Mrclass                  = {Database Expansion Item},
  Mrnumber                 = {2443187},
  Url                      = {http://dx.doi.org/10.1093/biomet/asn029}
}

@Article{Wang2013Optimal,
  author =   {Zhaoran Wang and } # hliu #{ and } # tzhang,
  title =    {Optimal computational and statistical rates of convergence for sparse nonconvex learning problems},
  journal =  aos_s,
  year =     {2014},
  volume =   {42},
  number =   {6},
  pages =    {2164--2201},
  doi =      {10.1214/14-AOS1238},
  fjournal = {The Annals of Statistics},
  issn =     {0090-5364},
  mrclass =  {62F30 (62J12 90C26 90C52)},
  mrnumber = {3269977},
  url =      {http://dx.doi.org/10.1214/14-AOS1238}
}

@Article{wang2011time,
  Title                    = {Time Varying Dynamic Bayesian Network For Nonstationary Events Modeling And Online Inference},
  Author                   = {Z. Wang and E.~E. Kuruoglu and X. Yang and Y. Xu and T.~S. Huang},
  Journal                  = {IEEE Trans. Signal Proces.},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {1553--1568},
  Volume                   = {59},

  Newspaper                = {IEEE Trans. Signal Proces.},
  Publisher                = {IEEE}
}

@Article{WatStro1998,
  Title                    = {Collective dynamics of 'small-world' networks},
  Author                   = {D.~J. Watts and S.~H. Strogatz},
  Journal                  = {nature},
  Year                     = {1998},
  Number                   = {6684},
  Pages                    = {440--442},
  Volume                   = {393},

  Newspaper                = {nature},
  Publisher                = {Nature Publishing Group}
}

@Article{Wegkamp2013Adaptive,
  Title                    = {Adaptive estimation of the copula correlation matrix for semiparametric elliptical copulas},
  Author                   = {Marten Wegkamp and Yue Zhao},
  Journal                  = {ArXiv e-prints, arXiv:1305.6526},
  Year                     = {2013},

  Month                    = may,

  Abstract                 = {We study the adaptive estimation of copula correlation matrix $\Sigma$ for elliptical copulas. In this context, the correlations are connected to Kendall's tau through a sine function transformation. Hence, a natural estimate for $\Sigma$ is the plug-in estimator $\widehat\Sigma$ with Kendall's tau statistic. We first obtain a sharp bound for the operator norm of $\widehat \Sigma - \Sigma$. Then, we study a factor model for $\Sigma$, for which we propose a refined estimator $\widetilde\Sigma$ by fitting a low-rank matrix plus a diagonal matrix to $\widehat\Sigma$ using least squares with a nuclear norm penalty on the low-rank matrix. The bound for the operator norm of $\widehat \Sigma - \Sigma$ serves to scale the penalty term, and we obtain finite sample oracle inequalities for $\widetilde\Sigma$. We also consider an elementary factor model of $\Sigma$, for which we propose closed-form estimators. We provide data-driven versions for all our estimation procedures and performance bounds.},
  Eprint                   = {1305.6526},
  Oai2identifier           = {1305.6526}
}

@Article{Wei2010Consistent,
  Title                    = {Consistent group selection in high-dimensional linear
 regression},
  Author                   = {Fengrong Wei and } # jhuang,
  Journal                  = {Bernoulli},
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1369--1384},
  Volume                   = {16},

  Doi                      = {10.3150/10-BEJ252},
  Fjournal                 = {Bernoulli. Official Journal of the Bernoulli Society for
 Mathematical Statistics and Probability},
  ISSN                     = {1350-7265},
  Mrclass                  = {62J07 (62J05)},
  Mrnumber                 = {2759183 (2012f:62147)},
  Mrreviewer               = {Ryan S. Gill},
  Url                      = {http://dx.doi.org/10.3150/10-BEJ252}
}

@InProceedings{Weinberger2009Feature,
  author =       {Weinberger, Kilian and Dasgupta, Anirban and Langford, John and Smola, Alex and Attenberg, Josh},
  title =        {Feature hashing for large scale multitask learning},
  booktitle =    {Proceedings of the 26th Annual International Conference on Machine Learning},
  year =         {2009},
  pages =        {1113--1120},
  organization = {ACM},
  file =         {:Weinberger2009Feature.pdf:PDF}
}

@Article{white1980heteroskedasticity,
  Title                    = {A Heteroskedasticity-consistent Covariance Matrix Estimator And A Direct Test For Heteroskedasticity},
  Author                   = {H. White},
  Journal                  = {Econometrica},
  Year                     = {1980},
  Number                   = {4},
  Pages                    = {817--838},
  Volume                   = {48},

  Doi                      = {10.2307/1912934},
  ISSN                     = {0012-9682},
  Mrnumber                 = {575027 (81k:62097)},
  Newspaper                = {Econometrica},
  Url                      = {http://dx.doi.org/10.2307/1912934}
}

@Article{wilks32moments,
  Title                    = {Moments And Distributions Of Estimates Of Population Parameters From Fragmentary Samples},
  Author                   = {S.~S.~Wilks},
  Journal                  = {Ann. Math. Stat.},
  Year                     = {1932},
  Number                   = {3},
  Pages                    = {163--195},
  Volume                   = {3},

  Newspaper                = {Ann. Math. Stat.},
  Publisher                = {JSTOR}
}

@Article{Wood2014Generalized,
  Title                    = {Generalized additive models for large data sets},
  Author                   = {Simon N. Wood and Yannig Goude and Simon Shaw},
  Journal                  = {J. R. Stat. Soc. C},
  Year                     = {2014},

  Month                    = {May},
  Pages                    = {n/a--n/a},

  Doi                      = {10.1111/rssc.12068},
  ISSN                     = {0035-9254},
  Publisher                = {Wiley-Blackwell},
  Url                      = {http://dx.doi.org/10.1111/rssc.12068}
}

@Article{wu2009sparse,
  Title                    = {Sparse linear discriminant analysis for simultaneous testing for the significance of a gene set/pathway and gene selection},
  Author                   = {Michael C. Wu and Lingsong Zhang and Zhaoxi Wang and David C. Christiani and Xihong Lin},
  Journal                  = {Bioinformatics},
  Year                     = {2009},
  Number                   = {9},
  Pages                    = {1145--1151},
  Volume                   = {25},

  Publisher                = {Oxford Univ Press}
}

@Article{Wu2009Genome,
  author =   {Tong Tong Wu and Yi Fang Chen and } # thastie #{ and Eric Sobel and } # klange,
  title =    {Genome-wide association analysis by lasso penalized logistic regression},
  journal =  {Bioinformatics},
  year =     {2009},
  volume =   {25},
  number =   {6},
  pages =    {714-721},
  abstract = {Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.Method: The present article evaluates the performance of lasso penalized logistic regression in casecontrol disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.Contact: klange@ucla.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
  doi =      {10.1093/bioinformatics/btp041},
  eprint =   {http://bioinformatics.oxfordjournals.org/content/25/6/714.full.pdf+html},
  url =      {http://bioinformatics.oxfordjournals.org/content/25/6/714.abstract}
}

@Article{wu2009banding,
  Title                    = {Banding Sample Autocovariance Matrices Of Stationary Processes},
  Author                   = {W.~B. Wu and M.~P. },
  Journal                  = statsin,
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {1755--1768},
  Volume                   = {19},

  ISSN                     = {1017-0405},
  Mrnumber                 = {2589209 (2010m:62165)},
  Newspaper                = {Stat. Sinica}
}

@Article{Wu2016Performance,
  author =   {Wu, Wei-Biao and Wu, Ying Nian},
  title =    {Performance bounds for parameter estimates of high-dimensional linear models with correlated errors},
  journal =  ejs_s,
  year =     {2016},
  volume =   {10},
  number =   {1},
  pages =    {352--379},
  doi =      {10.1214/16-EJS1108},
  file =     {Wu2016Performance.pdf:Wu2016Performance.pdf:PDF},
  fjournal = {Electronic Journal of Statistics},
  issn =     {1935-7524},
  mrclass =  {Preliminary Data},
  mrnumber = {3466186},
  url =      {http://dx.doi.org/10.1214/16-EJS1108}
}

@Article{Wu2011Altered,
  author    = {X. Wu and R. Li and A.~S. Fleisher and E.~M. Reiman and X. Guan and Y. Zhang and K. Chen and L. Yao},
  title     = {Altered Default Mode Network Connectivity In Alzheimer's Disease --- a Resting Functional Mri And Bayesian Network Study},
  journal   = {Human brain mapping},
  year      = {2011},
  volume    = {32},
  number    = {11},
  pages     = {1868--1881},
  newspaper = {Human brain mapping},
  publisher = {Wiley Online Library},
  timestamp = {2018.03.09},
}

@Article{Wu2009Variable,
  Title                    = {Variable selection in quantile regression},
  Author                   = {Yichao Wu and Yufeng Liu},
  Journal                  = statsin_s,
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {801--817},
  Volume                   = {19},

  Fjournal                 = {Statistica Sinica},
  ISSN                     = {1017-0405},
  Mrclass                  = {62G08},
  Mrnumber                 = {2514189 (2010e:62108)}
}

@Article{Wu2011Non,
  author =   {Wu, Yichao and Liu, Yufeng},
  title =    {Non-crossing large-margin probability estimation and its application to robust {SVM} via preconditioning},
  journal =  {Stat. Methodol.},
  year =     {2011},
  volume =   {8},
  number =   {1},
  pages =    {56--67},
  doi =      {10.1016/j.stamet.2009.05.004},
  file =     {Wu2011Non.pdf:Wu2011Non.pdf:PDF},
  fjournal = {Statistical Methodology},
  issn =     {1572-3127},
  mrclass =  {Database Expansion Item},
  mrnumber = {2741509},
  url =      {http://dx.doi.org/10.1016/j.stamet.2009.05.004}
}

@InProceedings{Wytock2013Large,
  author =    {M. Wytock and J. Z. Kolter},
  title =     {Large-scale probabilistic forecasting in energy systems using sparse Gaussian conditional random fields},
  booktitle = {52nd IEEE Conference on Decision and Control},
  year =      {2013},
  pages =     {1019--1024},
  month =     {Dec},
  doi =       {10.1109/CDC.2013.6760016},
  file =      {:Wytock2013Large.pdf:PDF},
  issn =      {0191-2216}
}

@Article{Xu2014Faithful,
  Title                    = {Faithful Variable Screening for High-Dimensional Convex Regression},
  Author                   = {Min Xu and Minhua Chen and } # jlafferty,
  Journal                  = {ArXiv e-prints, arXiv:1411.1805},
  Year                     = {2014},

  Month                    = nov,

  Abstract                 = {We study the problem of variable selection in convex nonparametric regression. Under the assumption that the true regression function is convex and sparse, we develop a screening procedure to select a subset of variables that contains the relevant variables. Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions, followed by one-dimensional concave regression fits on the residuals. In contrast to previous methods for sparse additive models, the optimization is finite dimensional and requires no tuning parameters for smoothness. Under appropriate assumptions, we prove that the procedure is faithful in the population setting, yielding no false negatives. We give a finite sample statistical analysis, and introduce algorithms for efficiently carrying out the required quadratic programs. The approach leads to computational and statistical advantages over fitting a full model, and provides an effective, practical approach to variable screening in convex regression.},
  Eprint                   = {1411.1805},
  Oai2identifier           = {1411.1805}
}

@Article{Xu2011Improved,
  Title                    = {An Improved Iterative Proportional Scaling Procedure for {Gauss}ian Graphical Models},
  Author                   = {Ping-Feng Xu and Jianhua Guo and Xuming He},
  Journal                  = jcgs_s,
  Year                     = {2011},

  Month                    = {Jan},
  Number                   = {2},
  Pages                    = {417-431},
  Volume                   = {20},

  Doi                      = {10.1198/jcgs.2010.09044},
  Owner                    = {mkolar},
  Publisher                = {Informa UK Limited},
  Timestamp                = {2014.02.13},
  Url                      = {http://dx.doi.org/10.1198/jcgs.2010.09044}
}

@InProceedings{Xuan2007modeling,
  Title                    = {Modeling Changing Dependency Structure In Multivariate Time Series},
  Author                   = {X. Xuan and K. Murphy},
  Booktitle                = {Proc. of ICML},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {1055--1062},
  Publisher                = {ACM},
  Series                   = {ICML '07},

  Doi                      = {10.1145/1273496.1273629},
  ISBN                     = {978-1-59593-793-3},
  Location                 = {Corvalis, Oregon}
}

@Article{Xue2012Regularized,
  Title                    = {Regularized rank-based estimation of high-dimensional nonparanormal graphical models},
  Author                   = {Lingzhou Xue and } # hzou,
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {2541--2571},
  Volume                   = {40},

  Doi                      = {10.1214/12-AOS1041},
  Fjournal                 = {The Annals of Statistics},
  ISSN                     = {0090-5364},
  Mrclass                  = {62G05 (62G20 62H12 62J07)},
  Mrnumber                 = {3097612},
  Url                      = {http://dx.doi.org/10.1214/12-AOS1041}
}

@Article{Xue2013Minimax,
  Title                    = {Minimax optimal estimation of general bandable covariance matrices},
  Author                   = {Lingzhou Xue and } # hzou,
  Journal                  = jma_s,
  Year                     = {2013},
  Pages                    = {45--51},
  Volume                   = {116},

  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0047259X1200262X}
}

@Article{Xue2012Nonconcave,
  Title                    = {Nonconcave penalized composite conditional likelihood estimation of sparse Ising models},
  Author                   = {Lingzhou Xue and } # hzou # { and Tianxi Ca},
  Journal                  = aos_s,
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {1403--1429},
  Volume                   = {40},

  Publisher                = {Institute of Mathematical Statistics},
  Url                      = {http://projecteuclid.org/euclid.aos/1344610588}
}

@Article{Xue2012Positive,
  Title                    = {Positive-Definite 1-Penalized Estimation of Large Covariance Matrices},
  Author                   = {Lingzhou Xue and Shiqian Ma and } # hzou,
  Journal                  = jasa_s,
  Year                     = {2012},
  Number                   = {500},
  Pages                    = {1480--1491},
  Volume                   = {107},

  Publisher                = {Taylor \& Francis Group},
  Url                      = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2012.725386}
}

@Article{Xue2007Multi,
  author =    {Xue, Ya and Liao, Xuejun and Carin, Lawrence and Krishnapuram, Balaji},
  title =     {Multi-task learning for classification with Dirichlet process priors},
  journal =   jmlr_s,
  year =      {2007},
  volume =    {8},
  pages =     {35--63},
  file =      {:Xue2007Multi.pdf:PDF},
  publisher = {JMLR. org}
}

@InCollection{Yang2014Elementary,
  Title                    = {Elementary Estimators for Graphical Models},
  Author                   = {Eunho Yang and } # alozano # { and } # pravik,
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
  Pages                    = {2159--2167},

  Url                      = {http://papers.nips.cc/paper/5318-elementary-estimators-for-graphical-models.pdf}
}

@InProceedings{Yang2014Elementarya,
  Title                    = {Elementary Estimators for High-Dimensional Linear Regression},
  Author                   = {Eunho Yang and } # alozano # { and } # pravik,
  Booktitle                = PROC_s # { } # ICML2014_s,
  Year                     = {2014},
  Editor                   = {Tony Jebara and Eric P. Xing},
  Pages                    = {388-396},
  Publisher                = {JMLR Workshop and Conference Proceedings},

  Url                      = {http://jmlr.org/proceedings/papers/v32/yangc14.pdf}
}

@InCollection{Yang2012Graphical,
  Title                    = {Graphical Models via Generalized Linear Models},
  Author                   = {Eunho Yang and } # gallen # { and Zhandong Liu and } # pravik,
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2012},
  Editor                   = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  Pages                    = {1358--1366},

  Url                      = {http://papers.nips.cc/paper/4617-graphical-models-via-generalized-linear-models.pdf}
}

@InCollection{Yang2013Poisson,
  Title                    = {On Poisson Graphical Models},
  Author                   = {Eunho Yang and } # pravik # { and } # gallen # { and Zhandong Liu},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2013},
  Editor                   = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  Pages                    = {1718--1726},

  Url                      = {http://papers.nips.cc/paper/5153-on-poisson-graphical-models.pdf}
}

@Article{Yang2013Graphical,
  author =  {Eunho Yang and } # pravik #{ and } # gallen #{ and Zhandong Liu},
  title =   {On Graphical Models via Univariate Exponential Family Distributions},
  journal = jmlr_s,
  year =    {2015},
  volume =  {16},
  pages =   {3813-3847},
  file =    {:Yang2013Graphical.pdf:PDF},
  url =     {http://jmlr.org/papers/v16/yang15a.html}
}

@InProceedings{Yang2014Mixed,
  Title                    = {Mixed Graphical Models via Exponential Families},
  Author                   = {Eunho Yang and Yulia Baker and } # pravik # { and } # gallen # { and Zhandong Liu},
  Booktitle                = PROC_s # { 17th Int. Conf, Artif. Intel. Stat.},
  Year                     = {2014},
  Pages                    = {1042--1050}
}

@InCollection{Yang2013Trading,
  author =    {Yang, Tianbao},
  title =     {Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year =      {2013},
  editor =    {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages =     {629--637},
  url =       {http://papers.nips.cc/paper/5114-trading-computation-for-communication-distributed-stochastic-dual-coordinate-ascent.pdf}
}

@Article{Yang2015Randomized,
  author =         {Yun Yang and Mert Pilanci and } # mwainw,
  title =          {Randomized sketches for kernels: Fast and optimal non-parametric regression},
  journal =        {ArXiv e-prints, arXiv:1501.06195},
  year =           {2015},
  month =          jan,
  abstract =       {Kernel ridge regression (KRR) is a standard method for performing non-parametric regression over reproducing kernel Hilbert spaces. Given $n$ samples, the time and space complexity of computing the KRR estimate scale as $\mathcal{O}(n^3)$ and $\mathcal{O}(n^2)$ respectively, and so is prohibitive in many cases. We propose approximations of KRR based on $m$-dimensional randomized sketches of the kernel matrix, and study how small the projection dimension $m$ can be chosen while still preserving minimax optimality of the approximate KRR estimate. For various classes of randomized sketches, including those based on Gaussian and randomized Hadamard matrices, we prove that it suffices to choose the sketch dimension $m$ proportional to the statistical dimension (modulo logarithmic factors). Thus, we obtain fast and minimax optimal approximations to the KRR estimate for non-parametric regression.},
  comments =       {27 pages, 3 figures},
  eprint =         {1501.06195},
  file =           {:Yang2015Randomized.pdf:PDF},
  oai2identifier = {1501.06195},
  owner =          {mkolar},
  timestamp =      {2016.04.21}
}

@Article{Yang2014Minimax,
  Title                    = {Minimax-optimal nonparametric regression in high dimensions},
  Author                   = {Yun Yang and Surya T. Tokdar},
  Journal                  = {ArXiv e-prints, arXiv:1401.7278},
  Year                     = {2014},

  Month                    = jan,

  Abstract                 = {Minimax $L_2$ risks for high dimensional nonparametric regression are derived under two sparsity assumptions: 1. the true regression surface is a sparse function that depends only on $d=O(\log n)$ important predictors among a list of $p$ predictors, with $\log p= o(n)$; 2. the true regression surface depends on $O(n)$ predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. Broad range general results are presented to facilitate sharp lower and upper bound calculations on minimax risks in terms of modified packing entropies and covering entropies, and are specialized to spaces of additive functions. For either modeling assumption, a practicable extension of the widely used Bayesian Gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to $\log n$ terms) asymptotically as both $n,p \to \infty$ with $\log p = o(n)$.},
  Comments                 = {We have substantially improved the exposition of our paper by streamlining the presentation of the results with a more transparent split between main theorems and supporting auxiliary results and with better connections to existing results to remove redundancies},
  Eprint                   = {1401.7278},
  Oai2identifier           = {1401.7278}
}

@Article{Yang2014Semiparametric,
  author =         {Zhuoran Yang and Yang Ning and Han Liu},
  title =          {On Semiparametric Exponential Family Graphical Models},
  journal =        {ArXiv e-prints, arXiv:1412.8697},
  year =           {2014},
  month =          dec,
  abstract =       {We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our results.},
  comments =       {51 pages, 2 figures},
  eprint =         {1412.8697},
  file =           {:Yang2014Semiparametric.pdf:PDF},
  oai2identifier = {1412.8697},
  owner =          {mkolar},
  timestamp =      {2016.03.03}
}

@Article{Yi2013SURE,
  Title                    = {{SURE}-tuned tapering estimation of large covariance matrices},
  Author                   = {Fe Yi and } # hzou,
  Journal                  = csda_s,
  Year                     = {2013},
  Pages                    = {339--351},
  Volume                   = {58},

  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947312003362}
}

@Article{yin10nonparametric,
  Title                    = {Nonparametric Covariance Model},
  Author                   = {J. Yin and Z. Geng and } # rli # { and } # hwang,
  Journal                  = statsin_s,
  Year                     = {2010},
  Pages                    = {469--479},
  Volume                   = {20},

  Newspaper                = {Stat. Sinica}
}

@Article{Yin2011sparse,
  author =    {Jianxin Yin and Hongzhe Li},
  title =     {A sparse conditional {Gauss}ian graphical model for analysis of genetical genomics data},
  journal =   aoas_s,
  year =      {2011},
  volume =    {5},
  number =    {4},
  pages =     {2630-2650},
  month =     {Dec},
  doi =       {10.1214/11-AOAS494},
  file =      {Yin2011sparse.pdf:Yin2011sparse.pdf:PDF},
  publisher = {Institute of Mathematical Statistics - care of Project Euclid},
  timestamp = {2014.02.12},
  url =       {http://dx.doi.org/10.1214/11-AOAS494}
}

@Article{Yin2013Adjusting,
  author =   {Yin, Jianxin and Li, Hongzhe},
  title =    {Adjusting for high-dimensional covariates in sparse precision matrix estimation by {$\ell\sb 1$}-penalization},
  journal =  jma_s,
  year =     {2013},
  volume =   {116},
  pages =    {365--381},
  doi =      {10.1016/j.jmva.2013.01.005},
  fjournal = {Journal of Multivariate Analysis},
  issn =     {0047-259X},
  mrclass =  {62H12 (62-09 62H20 62J07 62P10)},
  mrnumber = {3049910},
  url =      {http://dx.doi.org/10.1016/j.jmva.2013.01.005}
}

@Article{Yoon2005Discovering,
  Title                    = {Discovering coherent biclusters from gene expression data using zero-suppressed binary decision diagrams},
  Author                   = {Sungroh Yoon and Christine Nardini and Luca Benini and Giovanni {De Micheli}},
  Journal                  = {IEEE/ACM Trans. Comput. Biol. Bioinf.},
  Year                     = {2005},
  Number                   = {4},
  Pages                    = {339--354},
  Volume                   = {2},

  Owner                    = {mkolar},
  Publisher                = {IEEE Computer Society Press},
  Timestamp                = {2014.02.18},
  Url                      = {http://dl.acm.org/citation.cfm?id=1100952}
}

@InProceedings{Yoshida2005estimating,
  Title                    = {Estimating Time-dependent Gene Networks From Time Series Microarray Data By Dynamic Linear Models With Markov Switching},
  Author                   = {R. Yoshida and S. Imoto and T. Higuchi},
  Booktitle                = {Proc. 2005 IEEE Comp. Comput. Syst. Bioinformatics. Conf.},
  Year                     = {2005},

  Address                  = {Washington, DC, USA},
  Pages                    = {289--298},
  Publisher                = {IEEE Computer Society},

  Doi                      = {10.1109/CSB.2005.32},
  ISBN                     = {0-7695-2344-7}
}

@Article{Yu2016Sparse,
  author =  {Guan Yu and Yufeng Liu},
  title =   {Sparse Regression Incorporating Graphical Structure among Predictors},
  journal = jasa_s,
  year =    {2016},
  volume =  {0},
  number =  {ja},
  pages =   {00-00},
  doi =     {10.1080/01621459.2015.1034319},
  eprint =  { http://dx.doi.org/10.1080/01621459.2015.1034319 },
  file =    {Yu2016Sparse.pdf:Yu2016Sparse.pdf:PDF},
  url =     { 
        http://dx.doi.org/10.1080/01621459.2015.1034319
    
}
}

@Article{yu2004likelihood,
  Title                    = {Likelihood-based Local Linear Estimation Of The Conditional Variance Function},
  Author                   = {K. Yu and M.~C. Jones},
  Journal                  = jasa_s,
  Year                     = {2004},
  Number                   = {465},
  Pages                    = {139--144},
  Volume                   = {99},

  Doi                      = {10.1198/016214504000000133},
  ISSN                     = {0162-1459},
  Mrnumber                 = {2054293},
  Newspaper                = {J. Am. Stat. Assoc.},
  Url                      = {http://dx.doi.org/10.1198/016214504000000133}
}

@InCollection{Yu2016Statistical,
  author =    {Ming Yu and Varun Gupta and Mladen Kolar},
  title =     {Statistical Inference for Pairwise Graphical Models Using Score Matching},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year =      {2016}
}

@Article{Yuan2010High,
  author    = {M. Yuan},
  title     = {High Dimensional Inverse Covariance Matrix Estimation Via Linear Programming},
  journal   = jmlr_s,
  year      = {2010},
  volume    = {11},
  pages     = {2261--2286},
  file      = {Yuan2010High.pdf:Yuan2010High.pdf:PDF},
  newspaper = {J. Mach. Learn. Res.},
}

@Article{Yuan2007Dimension,
  author =    {Yuan, Ming and Ekici, Ali and Lu, Zhaosong and Monteiro, Renato},
  title =     {Dimension reduction and coefficient estimation in multivariate linear regression},
  journal =   JRSSB_s,
  year =      {2007},
  volume =    {69},
  number =    {3},
  pages =     {329--346},
  file =      {:Yuan2007Dimension.pdf:PDF},
  publisher = {Wiley Online Library}
}

@Article{Yuan2006Model,
  author =    {M. Yuan and Y. Lin},
  title =     {Model Selection And Estimation In Regression With Grouped Variables},
  journal =   jrssb_s,
  year =      {2006},
  volume =    {68},
  pages =     {49--67},
  file =      {:Yuan2006Model.pdf:PDF},
  newspaper = {J. R. Stat. Soc. B}
}

@Article{Yuan2007Model,
  Title                    = {Model Selection And Estimation In The Gaussian Graphical Model},
  Author                   = {M. Yuan and Y. Lin},
  Journal                  = {Biometrika},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {19-35},
  Volume                   = {94},

  Newspaper                = {Biometrika}
}

@Article{Yuan2012Alternating,
  Title                    = {Alternating Direction Method for Covariance Selection Models},
  Author                   = {Xiaoming Yuan},
  Journal                  = {J. Sci. Comp.},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {261-273},
  Volume                   = {51},

  Doi                      = {10.1007/s10915-011-9507-1},
  ISSN                     = {0885-7474},
  Keywords                 = {Covariance selection problem; l 1-norm; Log-likelihood; Alternating direction method},
  Language                 = {English},
  Publisher                = {Springer US},
  Url                      = {http://dx.doi.org/10.1007/s10915-011-9507-1}
}

@Article{Yuan2013Learning,
  Title                    = {Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics},
  Author                   = {Xiao-Tong Yuan and Ping Li and Tong Zhang},
  Journal                  = {ArXiv e-prints, arXiv:1311.5479},
  Year                     = {2013},

  Month                    = nov,

  Abstract                 = {We investigate a generic problem of learning pairwise exponential family graphical models with pairwise sufficient statistics defined by a global mapping function, e.g., Mercer kernels. This subclass of pairwise graphical models allow us to flexibly capture complex interactions among variables beyond pairwise product. We propose two $\ell_1$-norm penalized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional estimator which estimates the parameters individually for each node. For both estimators, we show that under proper conditions the extra flexibility gained in our model comes at almost no cost of statistical and computational efficiency. We demonstrate the advantages of our model over state-of-the-art methods on synthetic and real datasets.},
  Eprint                   = {1311.5479},
  Oai2identifier           = {1311.5479}
}

@Article{Yuan2014Partial,
  author =     {Yuan, Xiao-Tong and Zhang, Tong},
  title =      {Partial {G}aussian graphical model estimation},
  journal =    {IEEE Trans. Inform. Theory},
  year =       {2014},
  volume =     {60},
  number =     {3},
  pages =      {1673--1687},
  doi =        {10.1109/TIT.2013.2296784},
  file =       {Yuan2014Partial.pdf:Yuan2014Partial.pdf:PDF},
  fjournal =   {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  issn =       {0018-9448},
  mrclass =    {62H12 (62F10 94A12)},
  mrnumber =   {3168429},
  mrreviewer = {Xiaoqian Sun},
  url =        {http://dx.doi.org/10.1109/TIT.2013.2296784}
}

@InProceedings{zha01spectral,
  Title                    = {Spectral Relaxation For K-means Clustering},
  Author                   = {H. Zha and C. Ding and M. Gu and X. He and H. Simon},
  Booktitle                = {Proc. of NIPS},
  Year                     = {2001},
  Pages                    = {1057--1064}
}

@Article{Zhang2015Gaussian,
  author =         {Danna Zhang and Wei Biao Wu},
  title =          {Gaussian Approximation for High Dimensional Time Series},
  journal =        {ArXiv e-prints, arXiv:1508.07036},
  year =           {2015},
  month =          aug,
  abstract =       {We consider the problem of approximating sums of high-dimensional stationary time series by Gaussian vectors, using the framework of functional dependence measure. The validity of the Gaussian approximation depends on the sample size $n$, the dimension $p$, the moment condition and the dependence of the underlying processes. We also consider an estimator for long-run covariance matrices and study its convergence properties. Our results allow constructing simultaneous confidence intervals for mean vectors of high-dimensional time series with asymptotically correct coverage probabilities. A Gaussian multiplier bootstrap method is proposed. A simulation study indicates the quality of Gaussian approximation with different $n$, $p$ under different moment and dependence conditions.},
  eprint =         {1508.07036},
  file =           {:Zhang2015Gaussian.pdf:PDF},
  oai2identifier = {1508.07036}
}

@PhdThesis{zhang06probabilistic,
  Title                    = {A Probabilistic Framework For Multitask Learning},
  Author                   = {J. Zhang},
  School                   = {Carnegie Mellon University},
  Year                     = {2006}
}

@Article{Zhang2014Learning,
  author =    {Lingxue Zhang and Seyoung Kim},
  title =     {Learning Gene Networks under {SNP} Perturbations Using {eQTL} Datasets},
  journal =   {{PLoS} Comput Biol},
  year =      {2014},
  volume =    {10},
  number =    {2},
  pages =     {e1003420},
  month =     {feb},
  doi =       {10.1371/journal.pcbi.1003420},
  editor =    {Kevin Chen},
  publisher = {Public Library of Science ({PLoS})},
  url =       {http://dx.doi.org/10.1371/journal.pcbi.1003420}
}

@InProceedings{Zhang2009Adaptive,
  author =    {Zhang, Tong},
  title =     {Adaptive forward-backward greedy algorithm for sparse learning with linear models},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2009},
  pages =     {1921--1928}
}

@Article{Zhang2000Variable,
  Title                    = {Variable Bandwidth Selection in Varying-Coefficient Models},
  Author                   = {Wenyang Zhang and Sik-Yum Lee},
  Journal                  = jma_s,
  Year                     = {2000},

  Month                    = {Jul},
  Number                   = {1},
  Pages                    = {116--134},
  Volume                   = {74},

  Doi                      = {10.1006/jmva.1999.1883},
  ISSN                     = {0047-259X},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1006/jmva.1999.1883}
}

@Article{Zhang2010Simultaneous,
  Title                    = {Simultaneous confidence band and hypothesis test in generalised varying-coefficient models},
  Author                   = {Wenyang Zhang and Heng Peng},
  Journal                  = jma_s,
  Year                     = {2010},

  Month                    = {Aug},
  Number                   = {7},
  Pages                    = {1656--1680},
  Volume                   = {101},

  Doi                      = {10.1016/j.jmva.2010.03.003},
  ISSN                     = {0047-259X},
  Owner                    = {mkolar},
  Publisher                = {Elsevier BV},
  Timestamp                = {2014.08.27},
  Url                      = {http://dx.doi.org/10.1016/j.jmva.2010.03.003}
}

@Article{Zhang2014Simultaneous,
  author     = {Zhang, Xianyang and Cheng, Guang},
  title      = {Simultaneous inference for high-dimensional linear models},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {2017},
  volume     = {112},
  number     = {518},
  pages      = {757--768},
  issn       = {0162-1459},
  doi        = {10.1080/01621459.2016.1166114},
  file       = {:Zhang2014Simultaneous.pdf:PDF},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62J05 (62G09 62J07 62J15)},
  mrnumber   = {3671768},
  mrreviewer = {Manoj Kumar},
  timestamp  = {2018.05.01},
  url        = {https://doi.org/10.1080/01621459.2016.1166114},
}

@Article{Zhang2013Time,
  author =   {Zhang, Xiaoke and Park, Byeong U. and Wang, Jane-Ling},
  title =    {Time-varying additive models for longitudinal data},
  journal =  jasa_s,
  year =     {2013},
  volume =   {108},
  number =   {503},
  pages =    {983--998},
  doi =      {10.1080/01621459.2013.778776},
  file =     {Zhang2013Time.pdf:Zhang2013Time.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  issn =     {0162-1459},
  mrclass =  {62G08 (62M10)},
  mrnumber = {3174678},
  url =      {http://dx.doi.org/10.1080/01621459.2013.778776}
}

@Article{Zhang2015Varying,
  author =   {Zhang, Xiaoke and Wang, Jane-Ling},
  title =    {Varying-coefficient additive models for functional data},
  journal =  {Biometrika},
  year =     {2015},
  volume =   {102},
  number =   {1},
  pages =    {15--32},
  doi =      {10.1093/biomet/asu053},
  file =     {Zhang2015Varying.pdf:Zhang2015Varying.pdf:PDF;:Zhang2015Varying_supp.pdf:PDF},
  fjournal = {Biometrika},
  issn =     {0006-3444},
  mrclass =  {62-07 (62J05)},
  mrnumber = {3335093},
  url =      {http://dx.doi.org/10.1093/biomet/asu053}
}

@InProceedings{Zhang2013Information,
  author =    {Yuchen Zhang and } # jduchi #{ and } # mjordan #{ and } # mwainw,
  title =     {Information-theoretic lower bounds for distributed statistical estimation with communication constraints},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2013},
  pages =     {2328--2336},
  file =      {:Zhang2013Information.pdf:PDF}
}

@InProceedings{Zhang2012Communication,
  author =    {Yuchen Zhang and } # mwainw #{ and } # jduchi,
  title =     {Communication-efficient algorithms for statistical optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year =      {2012},
  pages =     {1502--1510}
}

@Article{Zhang2013Communication,
  author =     {Zhang, Yuchen and Duchi, John C. and Wainwright, Martin J.},
  title =      {Communication-efficient algorithms for statistical optimization},
  journal =    jmlr_s,
  year =       {2013},
  volume =     {14},
  pages =      {3321--3363},
  file =       {:Zhang2013Communication.pdf:PDF},
  fjournal =   {Journal of Machine Learning Research (JMLR)},
  issn =       {1532-4435},
  mrclass =    {90C15 (62C05 62F10 62G09 62L20)},
  mrnumber =   {3144464},
  mrreviewer = {Kurt Marti}
}

@Article{Zhang2015Communication,
  author =  {Zhang, Yuchen and Xiao, Lin},
  title =   {Communication-efficient distributed optimization of self-concordant empirical loss},
  journal = {ArXiv e-prints, arXiv:1501.00263},
  year =    {2015},
  file =    {:Zhang2015Communication.pdf:PDF}
}

@Article{zhao06model,
  Title                    = {On Model Selection Consistency Of {l}asso},
  Author                   = {P. Zhao and } # byu,
  Journal                  = jmlr_s,
  Year                     = {2006},

  Month                    = {Nov},
  Pages                    = {2541--2563},
  Volume                   = {7},

  ISSN                     = {1532-4435},
  Mrnumber                 = {2274449},
  Newspaper                = {J. Mach. Learn. Res.}
}

@Article{Zhao2014Direct,
  author    = {Sihai Dave Zhao and } # tcai #{ and Hongzhe Li},
  title     = {Direct estimation of differential networks},
  journal   = {Biometrika},
  year      = {2014},
  volume    = {101},
  number    = {2},
  pages     = {253--268},
  month     = {May},
  issn      = {1464-3510},
  doi       = {10.1093/biomet/asu009},
  file      = {:Zhao2014Direct.pdf:PDF},
  owner     = {mkolar},
  publisher = {Oxford University Press (OUP)},
  timestamp = {2015.02.17},
  url       = {http://dx.doi.org/10.1093/biomet/asu009},
}

@Article{Zhao2014Calibrated,
  author =    {Tuo Zhao and } # hliu,
  title =     {Calibrated Precision Matrix Estimation for High Dimensional Elliptical Distributions},
  journal =   IEEEit_s,
  year =      {2014},
  pages =     {1--1},
  doi =       {10.1109/tit.2014.2360980},
  file =      {Zhao2014Calibrated.pdf:Zhao2014Calibrated.pdf:PDF},
  issn =      {1557-9654},
  publisher = {Institute of Electrical \& Electronics Engineers (IEEE)},
  url =       {http://dx.doi.org/10.1109/TIT.2014.2360980}
}

@Manual{Rhuge,
  Title                    = {huge: High-dimensional Undirected Graph Estimation},
  Author                   = {Tuo Zhao and } # hliu # { and Kathryn Roeder and } # jlafferty # { and } # lwasser,
  Note                     = {R package version 1.2.6},
  Year                     = {2014},

  Url                      = {http://CRAN.R-project.org/package=huge}
}

@Article{Zhao2012Huge,
  Title                    = {The Huge Package For High-dimensional Undirected Graph Estimation In {R}},
  Author                   = {T. Zhao and } # hliu # { and K.~E. Roeder and } # jlafferty # {and } # lwasser,
  Journal                  = jmlr_s,
  Year                     = {2012},
  Pages                    = {1059--1062},
  Volume                   = {13},

  Newspaper                = {J. Mach. Learn. Res.}
}

@Article{Zhao2014General,
  Title                    = {A General Framework for Robust Testing and Confidence Regions in High-Dimensional Quantile Regression},
  Author                   = {Tianqi Zhao and } # mkolar # { and } # hliu,
  Journal                  = {ArXiv e-prints, arXiv:1412.8724},
  Year                     = {2014},

  Month                    = dec,

  Abstract                 = {We propose a robust inference procedure for high-dimensional linear models, where the dimension $p$ could grow exponentially fast with the sample size $n$. Our method applies the de-biasing technique together with the composite quantile loss function to construct an estimator that weakly converges to a Normal distribution. This estimator can be used to construct uniformly valid confidence intervals and conduct hypothesis tests. Our estimator is robust and does not require the existence of first or second moment of the noise distribution. It also preserves efficiency when the second moment does exist, in the sense that the maximum loss in efficiency is less than 30\% compared to the square-loss-based de-biased Lasso estimator. In many cases our estimator is close to or better than the de-biased Lasso estimator, especially when the noise distribution is heavy-tailed. Although the result is a high-dimensional counterpart of the composite quantile regression (CQR) introduced in Zou and Yuan (2008), our procedure does not require solving the $L_1$-penalized CQR in high dimensions. Instead, we allow any first-stage estimator that has a desired convergence rate and empirical sparsity. Furthermore, we consider a high-dimensional simultaneous test for the regression parameter by applying Gaussian approximation and multiplier bootstrap theories. Lastly we study distributed learning and exploit the divide-and-conquer estimator to reduce computation complexity when the sample size is massive. Our de-biasing method does not require the precision matrix corresponding to the design to be sparse, which is commonly required for constructing confidence intervals in high-dimensional linear models. Finally, our theories are empirically tested on synthetic data.},
  Comments                 = {72 pages, 2 figures, 3 tables},
  Eprint                   = {1412.8724},
  Oai2identifier           = {1412.8724},
  Owner                    = {mkolar},
  Timestamp                = {2015.02.02}
}

@Article{Zhao2014Partially,
  author =         {Tianqi Zhao and Guang Cheng and Han Liu},
  title =          {A Partially Linear Framework for Massive Heterogeneous Data},
  journal =        {ArXiv e-prints, arXiv:1410.8570},
  year =           {2014},
  month =          oct,
  abstract =       {We consider a partially linear framework for modelling massive heterogeneous data. The major goal is to extract common features across all sub-populations while exploring heterogeneity of each sub-population. In particular, we propose an aggregation type estimator for the commonality parameter that possesses the (non-asymptotic) minimax optimal bound and asymptotic distribution as if there were no heterogeneity. This oracular result holds when the number of sub-populations does not grow too fast. A plug-in estimator for the heterogeneity parameter is further constructed, and shown to possess the asymptotic distribution as if the commonality information were available. We also test the heterogeneity among a large number of sub-populations. All the above results require to regularize each sub-estimation as though it had the entire sample size. Our general theory applies to the divide-and-conquer approach that is often used to deal with massive homogeneous data. A technical by-product of this paper is the statistical inferences for the general kernel ridge regression. Thorough numerical results are also provided to back up our theory.},
  comments =       {40 pages main text + 40 pages suppl, To appear in Annals of Statistics},
  eprint =         {1410.8570},
  file =           {:Zhao2014Partially.pdf:PDF},
  oai2identifier = {1410.8570}
}

@Article{Zhao2014Positive,
  author =   {Zhao, Tuo and Roeder, Kathryn and Liu, Han},
  title =    {Positive semidefinite rank-based correlation matrix estimation with application to semiparametric graph estimation},
  journal =  jcgs_s,
  year =     {2014},
  volume =   {23},
  number =   {4},
  pages =    {895--922},
  doi =      {10.1080/10618600.2013.858633},
  file =     {Zhao2014Positive.pdf:Zhao2014Positive.pdf:PDF},
  fjournal = {Journal of Computational and Graphical Statistics},
  issn =     {1061-8600},
  mrclass =  {62H12 (62G05 62H99)},
  mrnumber = {3270703},
  url =      {http://dx.doi.org/10.1080/10618600.2013.858633}
}

@InCollection{Zhao2014Accelerated,
  author =    {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
  title =     {Accelerated Mini-batch Randomized Block Coordinate Descent Method},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year =      {2014},
  editor =    {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages =     {3329--3337},
  file =      {:Zhao2014Accelerated.pdf:PDF},
  url =       {http://papers.nips.cc/paper/5614-accelerated-mini-batch-randomized-block-coordinate-descent-method.pdf}
}

@Article{Zhao2013Robust,
  Title                    = {Robust variable selection for the varying coefficient model based on composite $L_1-L_2$ regression},
  Author                   = {Weihua Zhao and Riquan Zhang and Jicai Liu},
  Journal                  = {J. Appl. Statist.},
  Year                     = {2013},
  Number                   = {9},
  Pages                    = {2024--2040},
  Volume                   = {40},

  Owner                    = {mkolar},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2014.02.13},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1080/02664763.2013.804040}
}

@Article{Zheng2013Adaptive,
  Title                    = {Adaptive penalized quantile regression for high dimensional data},
  Author                   = {Qi Zheng and Colin Gallagher and K.~B. Kulasekera},
  Journal                  = jspi_s,
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {1029--1038},
  Volume                   = {143},

  Coden                    = {JSPIDN},
  Doi                      = {10.1016/j.jspi.2012.12.009},
  Fjournal                 = {Journal of Statistical Planning and Inference},
  ISSN                     = {0378-3758},
  Mrclass                  = {62J07 (62G20 62G30 62H12 62J05)},
  Mrnumber                 = {3029230},
  Url                      = {http://dx.doi.org/10.1016/j.jspi.2012.12.009}
}

@Article{Zhou2015Extreme,
  author =         {Cheng Zhou and Fang Han and Xinsheng Zhang and Han Liu},
  title =          {An Extreme-Value Approach for Testing the Equality of Large U-Statistic Based Correlation Matrices},
  journal =        {ArXiv e-prints, arXiv:1502.03211},
  year =           {2015},
  month =          feb,
  abstract =       {There has been an increasing interest in testing the equality of large Pearson's correlation matrices. However, in many applications it is more important to test the equality of large rank-based correlation matrices since they are more robust to outliers and nonlinearity. Unlike the Pearson's case, testing the equality of large rank-based statistics has not been well explored and requires us to develop new methods and theory. In this paper, we provide a framework for testing the equality of two large U-statistic based correlation matrices, which include the rank-based correlation matrices as special cases. Our approach exploits extreme value statistics and the Jackknife estimator for uncertainty assessment and is valid under a fully nonparametric model. Theoretically, we develop a theory for testing the equality of U-statistic based correlation matrices. We then apply this theory to study the problem of testing large Kendall's tau correlation matrices and demonstrate its optimality. For proving this optimality, a novel construction of least favourable distributions is developed for the correlation matrix comparison.},
  comments =       {60 pages, 1 figure, 3 tables},
  eprint =         {1502.03211},
  file =           {:Zhou2015Extreme.pdf:PDF},
  oai2identifier = {1502.03211}
}

@Article{Zhou2013Modeling,
  author =   {Jiayu Zhou and Jun Liu and Vaibhav A. Narayan and Jieping Ye},
  title =    {Modeling disease progression via multi-task learning},
  journal =  {NeuroImage },
  year =     {2013},
  volume =   {78},
  pages =    {233 - 248},
  doi =      {http://dx.doi.org/10.1016/j.neuroimage.2013.03.073},
  issn =     {1053-8119},
  keywords = {Alzheimer's disease},
  url =      {http://www.sciencedirect.com/science/article/pii/S1053811913003261}
}

@Article{Zhou1996Direct,
  author =     {Kenneth Q. Zhou and } # sportnoy,
  title =      {Direct use of regression quantiles to construct confidence sets in linear models},
  journal =    aos_s,
  year =       {1996},
  volume =     {24},
  number =     {1},
  pages =      {287--306},
  coden =      {ASTSC7},
  doi =        {10.1214/aos/1033066210},
  file =       {Zhou1996Direct.pdf:Zhou1996Direct.pdf:PDF},
  fjournal =   {The Annals of Statistics},
  issn =       {0090-5364},
  mrclass =    {62J05 (62G15)},
  mrnumber =   {1389891 (97c:62158)},
  mrreviewer = {Michael Falk},
  url =        {http://dx.doi.org/10.1214/aos/1033066210}
}

@Article{Zhou08time,
  Title                    = {Time Varying Undirected Graphs},
  Author                   = {S. Zhou and } # jlafferty # { and } # lwasser,
  Journal                  = {Mach. Learn.},
  Year                     = {2010},
  Number                   = {2-3},
  Pages                    = {295--319},
  Volume                   = {80},

  Newspaper                = {Mach. Learn.},
  Publisher                = {Springer}
}

@Article{Zhou1998Local,
  author     = {S. Zhou and X. Shen and D.~A. Wolfe},
  title      = {Local asymptotics for regression splines and confidence regions},
  journal    = aos_s,
  year       = {1998},
  volume     = {26},
  number     = {5},
  pages      = {1760--1782},
  issn       = {0090-5364},
  coden      = {ASTSC7},
  doi        = {10.1214/aos/1024691356},
  file       = {:Zhou1998Local.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G08 (62G15 62G20)},
  mrnumber   = {1673277 (2000a:62100)},
  mrreviewer = {M. Hu{\v{s}}kov{\'a}},
  timestamp  = {2018.06.01},
  url        = {http://dx.doi.org/10.1214/aos/1024691356},
}

@Article{Zhou2015Two,
  author =         {Wen-Xin Zhou and Chao Zheng and Zhen Zhang},
  title =          {Two-Sample Smooth Tests for the Equality of Distributions},
  journal =        {ArXiv e-prints, arXiv:1509.03459},
  year =           {2015},
  month =          sep,
  abstract =       {This paper considers the problem of testing the equality of two unspecified distributions. The classical omnibus tests such as the Kolmogorov-Smirnov and Cram\`er-von Mises are known to suffer from low power against essentially all but location-scale alternatives. We propose a new two-sample test that modifies the Neyman's smooth test and extend it to the multivariate case based on the idea of projection pursue. The asymptotic null property of the test and its power against local alternatives are studied. The multiplier bootstrap method is employed to compute the critical value of the multivariate test. We establish validity of the bootstrap approximation in the case where the dimension is allowed to grow with the sample size. Numerical studies show that the new testing procedures perform well even for small sample sizes and are powerful in detecting local features or high-frequency components.},
  comments =       {40 pages, 3 figures},
  eprint =         {1509.03459},
  file =           {:Zhou2015Two.pdf:PDF},
  oai2identifier = {1509.03459}
}

@Article{Zhu2004Classification,
  author =   {Ji Zhu and } # thastie,
  title =    {Classification of gene microarrays by penalized logistic regression},
  journal =  {Biostatistics},
  year =     {2004},
  volume =   {5},
  number =   {3},
  pages =    {427-443},
  abstract = { Classification of patient samples is an important aspect of cancer diagnosis and treatment. The support vector machine (SVM) has been successfully applied to microarray cancer diagnosis problems. However, one weakness of the SVM is that given a tumor sample, it only predicts a cancer class label but does not provide any estimate of the underlying probability. We propose penalized logistic regression (PLR) as an alternative to the SVM for the microarray cancer diagnosis problem. We show that when using the same set of genes, PLR and the SVM perform similarly in cancer classification, but PLR has the advantage of additionally providing an estimate of the underlying probability. Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classification, rather than class prediction. We consider two gene selection methods in this paper, univariate ranking (UR) and recursive feature elimination (RFE). Empirical results indicate that PLR combined with RFE tends to select fewer genes than other methods and also performs well in both crossvalidation and test samples. A fast algorithm for solving PLR is also described. },
  doi =      {10.1093/biostatistics/kxg046},
  eprint =   {http://biostatistics.oxfordjournals.org/content/5/3/427.full.pdf+html},
  url =      {http://biostatistics.oxfordjournals.org/content/5/3/427.abstract}
}

@Article{Zhu2013Structural,
  author =     {Zhu, Yunzhang and Shen, Xiaotong and Pan, Wei},
  title =      {Structural pursuit over multiple undirected graphs},
  journal =    jasa_s,
  year =       {2014},
  volume =     {109},
  number =     {508},
  pages =      {1683--1696},
  doi =        {10.1080/01621459.2014.921182},
  file =       {Zhu2013Structural.pdf:Zhu2013Structural.pdf:PDF},
  fjournal =   {Journal of the American Statistical Association},
  issn =       {0162-1459},
  mrclass =    {62H12 (62H30 62H99)},
  mrnumber =   {3293620},
  mrreviewer = {Tatjana von Rosen},
  url =        {http://dx.doi.org/10.1080/01621459.2014.921182}
}

@Book{zieglerGEE,
  Title                    = {Generalized Estimating Equations},
  Author                   = {A. Ziegler},
  Publisher                = {Springer},
  Year                     = {2011},
  Number                   = {204},
  Series                   = {Lecture Notes in Statistics}
}

@InProceedings{Zinkevich2010Parallelized,
  author =    {Martin Zinkevich and Markus Weimer and Alexander J. Smola and Lihong Li},
  title =     {Parallelized Stochastic Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.},
  year =      {2010},
  editor =    {John D. Lafferty and Christopher K. I. Williams and John Shawe{-}Taylor and Richard S. Zemel and Aron Culotta},
  pages =     {2595--2603},
  publisher = {Curran Associates, Inc.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp2.uni-trier.de/rec/bib/conf/nips/ZinkevichWSL10},
  file =      {Zinkevich2010Parallelized.pdf:Zinkevich2010Parallelized.pdf:PDF},
  timestamp = {Thu, 11 Dec 2014 17:34:08 +0100},
  url =       {http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent}
}

@Book{Godambe1991Estimating,
  title =     {Estimating functions},
  publisher = {The Clarendon Press, Oxford University Press, New York},
  year =      {1991},
  editor =    {Godambe, V. P.},
  volume =    {7},
  series =    {Oxford Statistical Science Series},
  isbn =      {0-19-852228-2},
  mrclass =   {62-06},
  mrnumber =  {1163992 (92k:62001)},
  pages =     {xii+344}
}

@Article{Aliferis2010Local,
  author   = {Aliferis, Constantin F. and Statnikov, Alexander and Tsamardinos, Ioannis and Mani, Subramani and Koutsoukos, Xenofon D.},
  title    = {Local causal and {M}arkov blanket induction for causal discovery and feature selection for classification. {P}art {I}: algorithms and empirical evaluation},
  journal  = jmlr_s,
  year     = {2010},
  volume   = {11},
  pages    = {171--234},
  issn     = {1532-4435},
  file     = {:Aliferis2010Local.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {68T05 (62C10 62H30 62P10)},
  mrnumber = {2591625},
}

@Article{Tan2017Penalized,
  author      = {Zhiqiang Tan and } # chzhang,
  title       = {Penalized Estimation in Additive Regression with High-Dimensional Data},
  journal     = {ArXiv e-prints, arXiv: 1704.07229},
  abstract    = {Additive regression provides an extension of linear regression by modeling the signal of a response as a sum of functions of covariates of relatively low complexity. We study penalized estimation in high-dimensional nonparametric additive regression where functional semi-norms are used to induce smoothness of component functions and the empirical $L_2$ norm is used to induce sparsity. The functional semi-norms can be of Sobolev or bounded variation types and are allowed to be different amongst individual component functions. We establish new oracle inequalities for the predictive performance of such methods under three simple technical conditions: a sub-gaussian condition on the noise, a compatibility condition on the design and the functional classes under consideration, and an entropy condition on the functional classes. For random designs, the sample compatibility condition can be replaced by its population version under an additional condition to ensure suitable convergence of empirical norms. In homogeneous settings where the complexities of the component functions are of the same order, our results provide a spectrum of explicit convergence rates, from the so-called slow rate without requiring the compatibility condition to the fast rate under the hard sparsity or certain $L_q$ sparsity to allow many small components in the true regression function. These results significantly broadens and sharpens existing ones in the literature.},
  date        = {2017-04-24},
  eprint      = {1704.07229v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Tan2017Penalized.pdf:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Narayan2015Two,
  author      = {Manjari Narayan and Genevera I. Allen and Steffie Tomson},
  title       = {Two Sample Inference for Populations of Graphical Models with Applications to Functional Connectivity},
  year        = {2015},
  abstract    = {Gaussian Graphical Models (GGM) are popularly used in neuroimaging studies based on fMRI, EEG or MEG to estimate functional connectivity, or relationships between remote brain regions. In multi-subject studies, scientists seek to identify the functional brain connections that are different between two groups of subjects, i.e. connections present in a diseased group but absent in controls or vice versa. This amounts to conducting two-sample large scale inference over network edges post graphical model selection, a novel problem we call Population Post Selection Inference. Current approaches to this problem include estimating a network for each subject, and then assuming the subject networks are fixed, conducting two-sample inference for each edge. These approaches, however, fail to account for the variability associated with estimating each subject's graph, thus resulting in high numbers of false positives and low statistical power. By using resampling and random penalization to estimate the post selection variability together with proper random effects test statistics, we develop a new procedure we call $R^{3}$ that solves these problems. Through simulation studies we show that $R^{3}$ offers major improvements over current approaches in terms of error control and statistical power. We apply our method to identify functional connections present or absent in autistic subjects using the ABIDE multi-subject fMRI study.},
  date        = {2015-02-12},
  eprint      = {1502.03853v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1502.03853v1:PDF},
  keywords    = {stat.ME},
}

@InProceedings{Fazayeli2016Generalized,
  author    = {Farideh Fazayeli and Arindam Banerjee},
  title     = {Generalized Direct Change Estimation in {I}sing Model Structure},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2281--2290},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {We consider the problem of estimating change in the dependency structure of two p-dimensional Ising models, based on respectively n_1 and n_2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n_2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as \fracc\sqrt\min(n_1,n_2), where c depends on the Gaussian width of the unit norm ball. For example, for \ell_1 norm applied to s-sparse change, the change can be accurately estimated with \min(n_1,n_2)=O(s \log p) which is sharper than an existing result n_1= O(s^2 \log p) and n_2 = O(n_1^2). Experimental results illustrating the effectiveness of the proposed estimator are presented.},
  file      = {:Fazayeli2016Generalized.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/fazayeli16.html},
}

@InCollection{Belilovsky2016Testing,
  author    = {Belilovsky, Eugene and Varoquaux, Ga\"{e}l and Blaschko, Matthew B},
  title     = {Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {595--603},
  file      = {:Belilovsky2016Testing.pdf:PDF},
  url       = {http://papers.nips.cc/paper/6531-testing-for-differences-in-gaussian-graphical-models-applications-to-brain-connectivity.pdf},
}

@InCollection{Xu2016Semiparametric,
  author    = {Xu, Pan and Gu, Quanquan},
  title     = {Semiparametric Differential Graph Models},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {1064--1072},
  file      = {:Xu2016Semiparametric.pdf:PDF},
  url       = {http://papers.nips.cc/paper/6529-semiparametric-differential-graph-models.pdf},
}

@Article{Cai2017Global,
  author    = tcai,
  title     = {Global Testing and Large-Scale Multiple Testing for High-Dimensional Covariance Structures},
  journal   = {Annual Review of Statistics and Its Application},
  year      = {2017},
  volume    = {4},
  number    = {1},
  pages     = {423--446},
  month     = {mar},
  doi       = {10.1146/annurev-statistics-060116-053754},
  file      = {:Cai2017Global.pdf:PDF},
  publisher = {Annual Reviews},
}

@Article{Xia2015Testing,
  author    = {Yin Xia and Tianxi Cai and T. Tony Cai},
  title     = {Testing differential networks with applications to the detection of gene-gene interactions},
  journal   = {Biometrika},
  year      = {2015},
  volume    = {102},
  number    = {2},
  pages     = {247--266},
  month     = {mar},
  doi       = {10.1093/biomet/asu074},
  file      = {:Xia2015Testing.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Xia2017Testing,
  author    = {Yin Xia},
  title     = {Testing and support recovery of multiple high-dimensional covariance matrices with false discovery rate control},
  journal   = {{TEST}},
  year      = {2017},
  month     = {mar},
  doi       = {10.1007/s11749-017-0533-7},
  file      = {Xia2017Testing.pdf:Xia2017Testing.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Xia2016Multiple,
  author    = {Yin Xia and Tianxi Cai and T. Tony Cai},
  title     = {Multiple Testing of Submatrices of a Precision Matrix with Applications to Identification of Between Pathway Interactions},
  journal   = {Journal of the American Statistical Association},
  year      = {2016},
  pages     = {0--0},
  month     = {dec},
  doi       = {10.1080/01621459.2016.1251930},
  file      = {Xia2016Multiple.pdf:Xia2016Multiple.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Chen2016High,
  author      = {Jinghui Chen and Quanquan Gu},
  title       = {High Dimensional Multivariate Regression and Precision Matrix Estimation via Nonconvex Optimization},
  journal     = {Technical report},
  year        = {2016},
  abstract    = {We propose a nonconvex estimator for joint multivariate regression and precision matrix estimation in the high dimensional regime, under sparsity constraints. A gradient descent algorithm with hard thresholding is developed to solve the nonconvex estimator, and it attains a linear rate of convergence to the true regression coefficients and precision matrix simultaneously, up to the statistical error. Compared with existing methods along this line of research, which have little theoretical guarantee, the proposed algorithm not only is computationally much more efficient with provable convergence guarantee, but also attains the optimal finite sample statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real datasets back up our theory.},
  date        = {2016-06-02},
  eprint      = {1606.00832v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Chen2016High.pdf:PDF},
  keywords    = {stat.ML},
}

@Article{Xu2017Speeding,
  author      = {Pan Xu and Jian Ma and Quanquan Gu},
  title       = {Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimizations},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix. In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory.},
  date        = {2017-02-28},
  eprint      = {1702.08651v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Xu2017Speeding.pdf:PDF;online:http\://arxiv.org/pdf/1702.08651v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@InProceedings{Meng2014Learning,
  author    = {Zhaoshi Meng and Brian Eriksson and Hero, III, Alfred O.},
  title     = {Learning Latent Variable Gaussian Graphical Models},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year      = {2014},
  editor    = {Eric P. Xing and Tony Jebara},
  volume    = {32},
  number    = {2},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1269--1277},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  file      = {:Meng2014Learning.pdf:PDF;online:http\://arxiv.org/pdf/1406.2721v1:PDF;meng14.pdf:http\://proceedings.mlr.press/v32/meng14.pdf:PDF},
  keywords  = {stat.ML, cs.LG, math.ST, stat.TH},
  url       = {http://proceedings.mlr.press/v32/meng14.html},
}

@Article{Agarwal2012Noisy,
  author    = {Alekh Agarwal and Sahand Negahban and Martin J. Wainwright},
  title     = {Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {2},
  pages     = {1171--1197},
  month     = {apr},
  doi       = {10.1214/12-aos1000},
  file      = {Agarwal2012Noisy.pdf:Agarwal2012Noisy.pdf:PDF;:Agarwal2012Noisy_supp.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@InProceedings{Gu2016Low,
  author    = {Quanquan Gu and Zhaoran Wang Wang and Han Liu},
  title     = {Low-Rank and Sparse Structure Pursuit via Alternating Minimization},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  series    = {Proceedings of Machine Learning Research},
  pages     = {600--609},
  address   = {Cadiz, Spain},
  month     = {09--11 May},
  publisher = {PMLR},
  abstract  = {In this paper, we present a nonconvex alternating minimization optimization algorithm for low-rank and sparse structure pursuit. Compared with convex relaxation based methods, the proposed algorithm is computationally more efficient for large scale problems.    In our study, we define a notion of bounded difference of gradients, based on which we rigorously prove that with suitable initialization, the proposed nonconvex optimization algorithm enjoys linear convergence to the global optima and exactly recovers the underlying low rank and sparse matrices under standard conditions such as incoherence and sparsity conditions. For a wide range of statistical   models such as multi-task learning and robust principal component analysis (RPCA), our algorithm provides a principled approach to learning the low rank and sparse structures with provable guarantee. Thorough experiments on both synthetic and real datasets backup our theory.},
  file      = {Gu2016Low.pdf:Gu2016Low.pdf:PDF;gu16.pdf:http\://proceedings.mlr.press/v51/gu16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v51/gu16.html},
}

@Article{Lee2015Joint,
  author  = {Wonyul Lee and Yufeng Liu},
  title   = {Joint Estimation of Multiple Precision Matrices with Common Structures},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  pages   = {1035-1062},
  file    = {:Lee2015Joint.pdf:PDF},
  url     = {http://jmlr.org/papers/v16/lee15a.html},
}

@Article{Xie2016Joint,
  author   = {Xie, Yuying and Liu, Yufeng and Valdar, William},
  title    = {Joint estimation of multiple dependent {G}aussian graphical models with applications to mouse genomics},
  journal  = {Biometrika},
  year     = {2016},
  volume   = {103},
  number   = {3},
  pages    = {493--511},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/asw035},
  file     = {Xie2016Joint.pdf:Xie2016Joint.pdf:PDF},
  fjournal = {Biometrika},
  mrclass  = {92D10 (62H12 62H99)},
  mrnumber = {3551780},
  url      = {http://dx.doi.org/10.1093/biomet/asw035},
}

@Article{Wang2016constrained,
  author      = {Beilun Wang and Ritambhara Singh and Yanjun Qi},
  title       = {A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models},
  abstract    = {Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines.},
  date        = {2016-05-11},
  eprint      = {1605.03468v5},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1605.03468v5:PDF;:Wang2016constrained.pdf:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@InProceedings{Zhang2017Nonconvex,
  author    = {Xiao Zhang and Lingxiao Wang and Quanquan Gu},
  title     = {A Unified Framework for Nonconvex Low-Rank plus Sparse Matrix Recovery},
  booktitle = {International Conference on Artificial Intelligence and Statistics, {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain},
  year      = {2018},
  editor    = {Amos J. Storkey and Fernando P{\'{e}}rez{-}Cruz},
  volume    = {84},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1097--1107},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/ZhangWG18},
  file      = {:Zhang2017Nonconvex.pdf:PDF},
  timestamp = {2019.06.13},
  url       = {http://proceedings.mlr.press/v84/zhang18c.html},
}

@Article{Li2017Geometry,
  author      = {Qiuwei Li and Zhihui Zhu and Gongguo Tang},
  title       = {Geometry of Factored Nuclear Norm Regularization},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {This work investigates the geometry of a nonconvex reformulation of minimizing a general convex loss function $f(X)$ regularized by the matrix nuclear norm $\|X\|_*$. Nuclear-norm regularized matrix inverse problems are at the heart of many applications in machine learning, signal processing, and control. The statistical performance of nuclear norm regularization has been studied extensively in literature using convex analysis techniques. Despite its optimal performance, the resulting optimization has high computational complexity when solved using standard or even tailored fast convex solvers. To develop faster and more scalable algorithms, we follow the proposal of Burer-Monteiro to factor the matrix variable $X$ into the product of two smaller rectangular matrices $X=UV^T$ and also replace the nuclear norm $\|X\|_*$ with $(\|U\|_F^2+\|V\|_F^2)/2$. In spite of the nonconvexity of the factored formulation, we prove that when the convex loss function $f(X)$ is $(2r,4r)$-restricted well-conditioned, each critical point of the factored problem either corresponds to the optimal solution $X^\star$ of the original convex optimization or is a strict saddle point where the Hessian matrix has a strictly negative eigenvalue. Such a geometric structure of the factored formulation allows many local search algorithms to converge to the global optimum with random initializations.},
  date        = {2017-04-05},
  eprint      = {1704.01265v1},
  eprintclass = {cs.NA},
  eprinttype  = {arXiv},
  file        = {:Li2017Geometry.pdf:PDF;online:http\://arxiv.org/pdf/1704.01265v1:PDF},
  keywords    = {cs.NA, cs.IT, cs.LG, math.IT, math.OC},
}

@Article{Li2016Symmetry,
  author      = {Xingguo Li and Zhaoran Wang and Junwei Lu and Raman Arora and Jarvis Haupt and Han Liu and Tuo Zhao},
  title       = {Symmetry, Saddle Points, and Global Geometry of Nonconvex Matrix Factorization},
  journal     = {Technical report},
  year        = {2016},
  abstract    = {We propose a general theory for studying the geometry of nonconvex objective functions with underlying symmetric structures. In specific, we characterize the locations of stationary points and the null space of the associated Hessian matrices via the lens of invariant groups. As a major motivating example, we apply the proposed general theory to characterize the global geometry of the low-rank matrix factorization problem. In particular, we illustrate how the rotational symmetry group gives rise to infinitely many non-isolated strict saddle points and equivalent global minima of the objective function. By explicitly identifying all stationary points, we divide the entire parameter space into three regions: ($\cR_1$) the region containing the neighborhoods of all strict saddle points, where the objective has negative curvatures; ($\cR_2$) the region containing neighborhoods of all global minima, where the objective enjoys strong convexity along certain directions; and ($\cR_3$) the complement of the above regions, where the gradient has sufficiently large magnitudes. We further extend our result to the matrix sensing problem. This allows us to establish strong global convergence guarantees for popular iterative algorithms with arbitrary initial solutions.},
  date        = {2016-12-29},
  eprint      = {1612.09296v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Li2016Symmetry.pdf:PDF;online:http\://arxiv.org/pdf/1612.09296v2:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Article{Fort2017Stochastic,
  author      = {Gersende Fort and Edouard Ollier and Adeline Samson},
  title       = {Stochastic Proximal Gradient Algorithms for Penalized Mixed Models},
  abstract    = {Motivated by penalized likelihood maximization in complex models, we study optimization problems where neither the function to optimize nor its gradient have an explicit expression, but its gradient can be approximated by a Monte Carlo technique. We propose a new algorithm based on a stochastic approximation of the Proximal-Gradient (PG) algorithm. This new algorithm, named Stochastic Approximation PG (SAPG) is the combination of a stochastic gradient descent step which - roughly speaking - computes a smoothed approximation of the past gradient along the iterations, and a proximal step. The choice of the step size and the Monte Carlo batch size for the stochastic gradient descent step in SAPG are discussed. Our convergence results cover the cases of biased and unbiased Monte Carlo approximations. While the convergence analysis of the Monte Carlo-PG is already addressed in the literature (see Atchad\'e et al. [2016]), the convergence analysis of SAPG is new. The two algorithms are compared on a linear mixed effect model as a toy example. A more challenging application is proposed on non-linear mixed effect models in high dimension with a pharmacokinetic data set including genomic covariates. To our best knowledge, our work provides the first convergence result of a numerical method designed to solve penalized Maximum Likelihood in a non-linear mixed effect model.},
  date        = {2017-04-28},
  eprint      = {1704.08891v1},
  eprintclass = {stat.CO},
  eprinttype  = {arXiv},
  file        = {:Fort2017Stochastic.pdf:PDF;online:http\://arxiv.org/pdf/1704.08891v1:PDF},
  keywords    = {stat.CO, stat.ME},
}

@Article{Netrapalli2014Non,
  author      = {Praneeth Netrapalli and U N Niranjan and Sujay Sanghavi and Animashree Anandkumar and Prateek Jain},
  title       = {Non-convex Robust PCA},
  abstract    = {We propose a new method for robust PCA -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is {\em non-convex} but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an $m \times n$ input matrix ($m \leq n)$, our method has a running time of $O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reach an accuracy of $\epsilon$. This is close to the running time of simple PCA via the power method, which requires $O(rmn)$ per iteration, and $O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA, which are based on convex optimization, have $O(m^2n)$ complexity per iteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.},
  date        = {2014-10-28},
  eprint      = {1410.7660v1},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  file        = {:Netrapalli2014Non.pdf:PDF;online:http\://arxiv.org/pdf/1410.7660v1:PDF},
  keywords    = {cs.IT, cs.LG, math.IT, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.05.02},
}

@InCollection{Yi2016Fast,
  author    = {Yi, Xinyang and Park, Dohyung and Chen, Yudong and Caramanis, Constantine},
  title     = {Fast Algorithms for Robust {PCA} via Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {4152--4160},
  file      = {:Yi2016Fasta.pdf:PDF},
  url       = {http://papers.nips.cc/paper/6445-fast-algorithms-for-robust-pca-via-gradient-descent.pdf},
}

@Article{Ge2017No,
  author      = {Rong Ge and Chi Jin and Yi Zheng},
  title       = {No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis},
  abstract    = {In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.},
  date        = {2017-04-03},
  eprint      = {1704.00708v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Ge2017No.pdf:PDF;online:http\://arxiv.org/pdf/1704.00708v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.05.02},
}

@Article{Yuan2012Discussion,
  author    = {Ming Yuan},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1968--1972},
  month     = {aug},
  doi       = {10.1214/12-aos979},
  file      = {Yuan2012Discussion.pdf:Yuan2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Lauritzen2012Discussion,
  author    = {Steffen Lauritzen and Nicolai Meinshausen},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1973--1977},
  month     = {aug},
  doi       = {10.1214/12-aos980},
  file      = {Lauritzen2012Discussion.pdf:Lauritzen2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Wainwright2012Discussion,
  author    = {Martin J. Wainwright},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1978--1983},
  month     = {aug},
  doi       = {10.1214/12-aos981},
  file      = {Wainwright2012Discussion.pdf:Wainwright2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Giraud2012Discussion,
  author    = {Christophe Giraud and Alexandre Tsybakov},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1984--1988},
  month     = {aug},
  doi       = {10.1214/12-aos984},
  file      = {Giraud2012Discussion.pdf:Giraud2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Ren2012Discussion,
  author    = {Zhao Ren and Harrison H. Zhou},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1989--1996},
  month     = {aug},
  doi       = {10.1214/12-aos985},
  file      = {Ren2012Discussion.pdf:Ren2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Candes2012Discussion,
  author    = {Emmanuel J. Cand{\'{e}}s and Mahdi Soltanolkotabi},
  title     = {Discussion: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {1997--2004},
  month     = {aug},
  doi       = {10.1214/12-aos1001},
  file      = {Candes2012Discussion.pdf:Candes2012Discussion.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Chandrasekaran2012Rejoinder,
  author    = {Venkat Chandrasekaran and Pablo A. Parrilo and Alan S. Willsky},
  title     = {Rejoinder: Latent variable graphical model selection via convex optimization},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {4},
  pages     = {2005--2013},
  month     = {aug},
  doi       = {10.1214/12-aos1020},
  file      = {Chandrasekaran2012Rejoinder.pdf:Chandrasekaran2012Rejoinder.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Staedler2013Network,
  author    = {St\"adler, Nicolas and Mukherjee, Sach},
  title     = {Multivariate gene-set testing based on graphical models},
  journal   = {Biostatistics},
  year      = {2015},
  volume    = {16},
  number    = {1},
  pages     = {47--59},
  issn      = {1468-4357},
  doi       = {10.1093/biostatistics/kxu027},
  file      = {:Staedler2013Network.pdf:PDF},
  fjournal  = {Biostatistics},
  mrclass   = {Expansion},
  mrnumber  = {3365410},
  timestamp = {2018.08.21},
  url       = {http://dx.doi.org/10.1093/biostatistics/kxu027},
}

@Article{Staedler2017Two,
  author   = {St\"adler, Nicolas and Mukherjee, Sach},
  title    = {Two-sample testing in high dimensions},
  journal  = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year     = {2017},
  volume   = {79},
  number   = {1},
  pages    = {225--246},
  issn     = {1369-7412},
  doi      = {10.1111/rssb.12173},
  file     = {Staedler2017Two.pdf:Staedler2017Two.pdf:PDF},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62H15},
  mrnumber = {3597971},
  url      = {http://dx.doi.org/10.1111/rssb.12173},
}

@Article{Monti2014Estimating,
  author    = {Ricardo Pio Monti and Peter Hellyer and David Sharp and Robert Leech and Christoforos Anagnostopoulos and Giovanni Montana},
  title     = {Estimating time-varying brain connectivity networks from functional {MRI} time series},
  journal   = {{NeuroImage}},
  year      = {2014},
  volume    = {103},
  pages     = {427--443},
  month     = {dec},
  doi       = {10.1016/j.neuroimage.2014.07.033},
  file      = {:Monti2014Estimating.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@InCollection{Huang2011Identifying,
  author    = {Shuai Huang and Jing Li and Ye, Jieping and Teresa Wu and Kewei Chen and Adam Fleisher and Eric Reiman},
  title     = {Identifying Alzheimer\textquotesingle s Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {Curran Associates, Inc.},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {1431--1439},
  file      = {:Huang2011Identifying.pdf:PDF},
  url       = {http://papers.nips.cc/paper/4440-identifying-alzheimers-disease-related-brain-regions-from-multi-modality-neuroimaging-data-using-sparse-composite-linear-discrimination-analysis.pdf},
}

@Article{Smith2011Network,
  author    = {Stephen M. Smith and Karla L. Miller and Gholamreza Salimi-Khorshidi and Matthew Webster and Christian F. Beckmann and Thomas E. Nichols and Joseph D. Ramsey and Mark W. Woolrich},
  title     = {Network modelling methods for {FMRI}},
  journal   = {{NeuroImage}},
  year      = {2011},
  volume    = {54},
  number    = {2},
  pages     = {875--891},
  month     = {jan},
  doi       = {10.1016/j.neuroimage.2010.08.063},
  publisher = {Elsevier {BV}},
}

@Article{Barron1991Approximation,
  author     = {Barron, Andrew R. and Sheu, Chyong-Hwa},
  title      = {Approximation of density functions by sequences of exponential families},
  journal    = {Ann. Statist.},
  year       = {1991},
  volume     = {19},
  number     = {3},
  pages      = {1347--1369},
  issn       = {0090-5364},
  doi        = {10.1214/aos/1176348252},
  file       = {Barron1991Approximation.pdf:Barron1991Approximation.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (41A10 62B10 94A17)},
  mrnumber   = {1126328},
  mrreviewer = {M. Bertrand-Retali},
  url        = {http://dx.doi.org/10.1214/aos/1176348252},
}

@Article{Barron1991Correction,
  author     = {Barron, Andrew R. and Sheu, Chyong-Hwa},
  title      = {Correction: ``{A}pproximation of density functions by sequences of exponential families''},
  journal    = {Ann. Statist.},
  year       = {1991},
  volume     = {19},
  number     = {4},
  pages      = {2284},
  issn       = {0090-5364},
  doi        = {10.1214/aos/1176348405},
  file       = {Barron1991Correction.pdf:Barron1991Correction.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (41A10 62G10 94A17)},
  mrnumber   = {1135183},
  mrreviewer = {M. Bertrand-Retali},
  url        = {http://dx.doi.org/10.1214/aos/1176348405},
}

@Article{Ma2013Alternating,
  author    = {Shiqian Ma and Lingzhou Xue and Hui Zou},
  title     = {Alternating Direction Methods for Latent Variable Gaussian Graphical Model Selection},
  journal   = {Neural Computation},
  year      = {2013},
  volume    = {25},
  number    = {8},
  pages     = {2172--2198},
  month     = {aug},
  doi       = {10.1162/neco_a_00379},
  file      = {Ma2013Alternating.pdf:Ma2013Alternating.pdf:PDF},
  publisher = {{MIT} Press - Journals},
}

@Article{Mei2016Landscape,
  author      = {Song Mei and Yu Bai and Andrea Montanari},
  title       = {The Landscape of Empirical Risk for Non-convex Losses},
  abstract    = {Most high-dimensional estimation and prediction methods propose to minimize a cost function (empirical risk) that is written as a sum of losses associated to each data point. In this paper we focus on the case of non-convex losses, which is practically important but still poorly understood. Classical empirical process theory implies uniform convergence of the empirical risk to the population risk. While uniform convergence implies consistency of the resulting M-estimator, it does not ensure that the latter can be computed efficiently. In order to capture the complexity of computing M-estimators, we propose to study the landscape of the empirical risk, namely its stationary points and their properties. We establish uniform convergence of the gradient and Hessian of the empirical risk to their population counterparts, as soon as the number of samples becomes larger than the number of unknown parameters (modulo logarithmic factors). Consequently, good properties of the population risk can be carried to the empirical risk, and we can establish one-to-one correspondence of their stationary points. We demonstrate that in several problems such as non-convex binary classification, robust regression, and Gaussian mixture model, this result implies a complete characterization of the landscape of the empirical risk, and of the convergence properties of descent algorithms. We extend our analysis to the very high-dimensional setting in which the number of parameters exceeds the number of samples, and provide a characterization of the empirical risk landscape under a nearly information-theoretically minimal condition. Namely, if the number of samples exceeds the sparsity of the unknown parameters vector (modulo logarithmic factors), then a suitable uniform convergence result takes place. We apply this result to non-convex binary classification and robust regression in very high-dimension.},
  date        = {2016-07-22},
  eprint      = {1607.06534v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Mei2016Landscape.pdf:PDF;online:http\://arxiv.org/pdf/1607.06534v3:PDF},
  keywords    = {stat.ML},
}

@InProceedings{Vanhaesebrouck2016Decentralized,
  author    = {Paul Vanhaesebrouck and Aurlien Bellet and Marc Tommasi},
  title     = {{Decentralized Collaborative Learning of Personalized Models over Networks}},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year      = {2017},
  editor    = {Aarti Singh and Jerry Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  pages     = {509--517},
  address   = {Fort Lauderdale, FL, USA},
  month     = {20--22 Apr},
  publisher = {PMLR},
  abstract  = {We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. To optimize this challenging objective, our decentralized algorithm is based on ADMM.},
  file      = {vanhaesebrouck17a.pdf:http\://proceedings.mlr.press/v54/vanhaesebrouck17a/vanhaesebrouck17a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v54/vanhaesebrouck17a.html},
}

@Article{Hirnschall2017Coordinated,
  author      = {Christoph Hirnschall and Adish Singla and Sebastian Tschiatschek and Andreas Krause},
  title       = {Coordinated Online Learning With Applications to Learning User Preferences},
  abstract    = {We study an online multi-task learning setting, in which instances of related tasks arrive sequentially, and are handled by task-specific online learners. We consider an algorithmic framework to model the relationship of these tasks via a set of convex constraints. To exploit this relationship, we design a novel algorithm -- COOL -- for coordinating the individual online learners: Our key idea is to coordinate their parameters via weighted projections onto a convex set. By adjusting the rate and accuracy of the projection, the COOL algorithm allows for a trade-off between the benefit of coordination and the required computation/communication. We derive regret bounds for our approach and analyze how they are influenced by these trade-off factors. We apply our results on the application of learning users' preferences on the Airbnb marketplace with the goal of incentivizing users to explore under-reviewed apartments.},
  date        = {2017-02-09},
  eprint      = {1702.02849v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Hirnschall2017Coordinated.pdf:PDF;online:http\://arxiv.org/pdf/1702.02849v1:PDF},
  keywords    = {cs.LG, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.05.16},
}

@InProceedings{Liu2016Distributed,
  author    = {Liu, Sulin and Pan, Sinno Jialin and Ho, Qirong},
  title     = {Distributed Multi-Task Relationship Learning},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2017},
  series    = {KDD '17},
  pages     = {937--946},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3098136},
  doi       = {10.1145/3097983.3098136},
  isbn      = {978-1-4503-4887-4},
  keywords  = {distributed multi-task learning, transfer learning},
  location  = {Halifax, NS, Canada},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/3097983.3098136},
}

@Article{Baytas2016Asynchronous,
  author      = {Inci M. Baytas and Ming Yan and Anil K. Jain and Jiayu Zhou},
  title       = {Asynchronous Multi-Task Learning},
  abstract    = {Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework.},
  date        = {2016-09-30},
  eprint      = {1609.09563v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Baytas2016Asynchronous.pdf:PDF;online:http\://arxiv.org/pdf/1609.09563v1:PDF},
  keywords    = {cs.LG, cs.DC},
  owner       = {mkolar},
  timestamp   = {2017.05.16},
}

@Article{Alain2014What,
  author   = {Alain, Guillaume and Bengio, Yoshua},
  title    = {What regularized auto-encoders learn from the data-generating distribution},
  journal  = {J. Mach. Learn. Res.},
  year     = {2014},
  volume   = {15},
  pages    = {3563--3593},
  issn     = {1532-4435},
  file     = {:Alain2014What.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {62G07 (62H11)},
  mrnumber = {3291406},
}

@Article{Bunea2012Joint,
  author     = {Bunea, Florentina and She, Yiyuan and Wegkamp, Marten H.},
  title      = {Joint variable and rank selection for parsimonious estimation of high-dimensional matrices},
  journal    = {Ann. Statist.},
  year       = {2012},
  volume     = {40},
  number     = {5},
  pages      = {2359--2388},
  issn       = {0090-5364},
  doi        = {10.1214/12-aos1039},
  file       = {Bunea2012Joint.pdf:Bunea2012Joint.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62J07)},
  mrnumber   = {3097606},
  mrreviewer = {Yuehua Wu},
  url        = {https://doi.org/10.1214/12-AOS1039},
}

@Article{Bunea2011Optimal,
  author    = {Florentina Bunea and Yiyuan She and Marten H. Wegkamp},
  title     = {Optimal selection of reduced rank estimators of high-dimensional matrices},
  journal   = {The Annals of Statistics},
  year      = {2011},
  volume    = {39},
  number    = {2},
  pages     = {1282--1309},
  month     = {apr},
  doi       = {10.1214/11-aos876},
  file      = {Bunea2011Optimal.pdf:Bunea2011Optimal.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Vounou2012Sparse,
  author    = {Maria Vounou and Eva Janousova and Robin Wolz and Jason L. Stein and Paul M. Thompson and Daniel Rueckert and Giovanni Montana},
  title     = {Sparse reduced-rank regression detects genetic associations with voxel-wise longitudinal phenotypes in Alzheimer's disease},
  journal   = {{NeuroImage}},
  year      = {2012},
  volume    = {60},
  number    = {1},
  pages     = {700--716},
  month     = {mar},
  doi       = {10.1016/j.neuroimage.2011.12.029},
  publisher = {Elsevier {BV}},
}

@Article{She2017Selective,
  author   = {She, Yiyuan},
  title    = {Selective factor extraction in high dimensions},
  journal  = {Biometrika},
  year     = {2017},
  volume   = {104},
  number   = {1},
  pages    = {97--110},
  issn     = {0006-3444},
  file     = {:She2017Selective.pdf:PDF},
  fjournal = {Biometrika},
  mrclass  = {92B15 (62H25 62J05)},
  mrnumber = {3626471},
}

@Article{Ma2014Learning,
  author    = {X. Ma and L. Xiao and W. H. Wong},
  title     = {Learning regulatory programs by threshold {SVD} regression},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2014},
  volume    = {111},
  number    = {44},
  pages     = {15675--15680},
  month     = {oct},
  doi       = {10.1073/pnas.1417808111},
  file      = {:Ma2014Learning.pdf:PDF},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Article{Ma2014Adaptive,
  author      = {Zhuang Ma and Zongming Ma and Tingni Sun},
  title       = {Adaptive Estimation in Two-way Sparse Reduced-rank Regression},
  journal     = {Technical report},
  year        = {2014},
  abstract    = {This paper studies the problem of estimating a large coefficient matrix in a multiple response linear regression model when the coefficient matrix could be both of low rank and sparse in the sense that most nonzero entries concentrate on a few rows and columns. We are especially interested in the high dimensional settings where the number of predictors and/or response variables can be much larger than the number of observations. We propose a new estimation scheme, which achieves competitive numerical performance and at the same time allows fast computation. Moreover, we show that (a slight variant of) the proposed estimator achieves near optimal non-asymptotic minimax rates of estimation under a collection of squared Schatten norm losses simultaneously by providing both the error bounds for the estimator and minimax lower bounds. The effectiveness of the proposed algorithm is also demonstrated on an \textit{in vivo} calcium imaging dataset.},
  date        = {2014-03-08},
  eprint      = {1403.1922v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1403.1922v2:PDF;:Ma2014Adaptive.pdf:PDF},
  keywords    = {stat.ME, math.ST, stat.TH},
}

@Article{Chen2012Sparse,
  author    = {Lisha Chen and Jianhua Z. Huang},
  title     = {Sparse Reduced-Rank Regression for Simultaneous Dimension Reduction and Variable Selection},
  journal   = {Journal of the American Statistical Association},
  year      = {2012},
  volume    = {107},
  number    = {500},
  pages     = {1533--1545},
  month     = {oct},
  doi       = {10.1080/01621459.2012.734178},
  file      = {Chen2012Sparse.pdf:Chen2012Sparse.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Bahmani2015Optimal,
  author      = {Sohail Bahmani and Justin Romberg},
  title       = {Near-Optimal Estimation of Simultaneously Sparse and Low-Rank Matrices from Nested Linear Measurements},
  abstract    = {In this paper we consider the problem of estimating simultaneously low-rank and row-wise sparse matrices from nested linear measurements where the linear operator consists of the product of a linear operator $\mathcal{W}$ and a matrix $\mathbf{\varPsi}$. Leveraging the nested structure of the measurement operator, we propose a computationally efficient two-stage algorithm for estimating the simultaneously structured target matrix. Assuming that $\mathcal{W}$ is a restricted isometry for low-rank matrices and $\mathbf{\varPsi}$ is a restricted isometry for row-wise sparse matrices, we establish an accuracy guarantee that holds uniformly for all sufficiently low-rank and row-wise sparse matrices with high probability. Furthermore, using standard tools from information theory, we establish a minimax lower bound for estimation of simultaneously low-rank and row-wise sparse matrices from linear measurements that need not be nested. The accuracy bounds established for the algorithm, that also serve as a minimax upper bound, differ from the derived minimax lower bound merely by a polylogarithmic factor of the dimensions. Therefore, the proposed algorithm is nearly minimax optimal. We also discuss some applications of the proposed observation model and evaluate our algorithm through numerical simulation.},
  date        = {2015-06-26},
  eprint      = {1506.08159v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Bahmani2015Optimal.pdf:PDF;online:http\://arxiv.org/pdf/1506.08159v2:PDF},
  keywords    = {math.ST, cs.IT, math.IT, math.OC, stat.AP, stat.TH},
}

@InProceedings{Richard2012Estimation,
  author    = {Emile Richard and Pierre-andre Savalle and Nicolas Vayatis},
  title     = {Estimation of Simultaneously Sparse and Low Rank Matrices},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  year      = {2012},
  editor    = {John Langford and Joelle Pineau},
  pages     = {1351--1358},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {:Richard2012Estimation.pdf:PDF},
  url       = {http://icml.cc/2012/papers/674.pdf},
}

@Article{Keshavan2010Matrix,
  author    = {Keshavan, Raghunandan H and Montanari, Andrea and Oh, Sewoong},
  title     = {Matrix completion from a few entries},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2010},
  volume    = {56},
  number    = {6},
  pages     = {2980--2998},
  publisher = {IEEE},
}

@Article{Keshavan2010Matrixa,
  author  = {Keshavan, Raghunandan H and Montanari, Andrea and Oh, Sewoong},
  title   = {Matrix completion from noisy entries},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {Jul},
  pages   = {2057--2078},
}

@InProceedings{Zhao2015Nonconvex,
  author    = {Tuo Zhao and Zhaoran Wang and Han Liu},
  title     = {A Nonconvex Optimization Framework for Low Rank Matrix Estimation},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  year      = {2015},
  editor    = {Corinna Cortes and Neil D. Lawrence and Daniel D. Lee and Masashi Sugiyama and Roman Garnett},
  pages     = {559--567},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/ZhaoWL15},
  file      = {:Zhao2015Nonconvex.pdf:PDF},
  timestamp = {Sun, 22 Apr 2018 15:20:14 +0200},
  url       = {http://papers.nips.cc/paper/5733-a-nonconvex-optimization-framework-for-low-rank-matrix-estimation},
}

@InProceedings{Ge2016Matrix,
  author    = {Ge, Rong and Lee, Jason D and Ma, Tengyu},
  title     = {Matrix completion has no spurious local minimum},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  pages     = {2973--2981},
  file      = {:Ge2016Matrix.pdf:PDF},
}

@Article{Zhu2017Global,
  author      = {Zhihui Zhu and Qiuwei Li and Gongguo Tang and Michael B. Wakin},
  title       = {The Global Optimization Geometry of Nonsymmetric Matrix Factorization and Sensing},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {In this paper we characterize the optimization geometry of a matrix factorization problem where we aim to find $n\times r$ and $m\times r$ matrices $U$ and $V$ such that $UV^T$ approximates a given matrix $X^\star$. We show that the objective function of the matrix factorization problem has no spurious local minima and obeys the strict saddle property not only for the exact-parameterization case where $rank(X^\star) = r$, but also for the over-parameterization case where $rank(X^\star) < r$ and under-parameterization case where $rank(X^\star) > r$. These geometric properties imply that a number of iterative optimization algorithms (such as gradient descent) converge to a global solution with random initialization. For the exact-parameterization case, we further show that the objective function satisfies the robust strict saddle property, ensuring global convergence of many local search algorithms in polynomial time. We extend the geometric analysis to the matrix sensing problem with the factorization approach and prove that this global optimization geometry is preserved as long as the measurement operator satisfies the standard restricted isometry property.},
  date        = {2017-03-03},
  eprint      = {1703.01256v1},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  file        = {:Zhu2017Global.pdf:PDF;online:http\://arxiv.org/pdf/1703.01256v1:PDF},
  keywords    = {cs.IT, math.IT, math.OC},
}

@Article{Aldrin1996Moderate,
  author    = {Aldrin, Magne},
  title     = {Moderate projection pursuit regression for multivariate response data},
  journal   = {Computational statistics \& data analysis},
  year      = {1996},
  volume    = {21},
  number    = {5},
  pages     = {501--531},
  publisher = {Elsevier},
}

@Article{Biswal2010discovery,
  author    = {Biswal, Bharat B and Mennes, Maarten and Zuo, Xi-Nian and Gohel, Suril and Kelly, Clare and Smith, Steve M and Beckmann, Christian F and Adelstein, Jonathan S and Buckner, Randy L and Colcombe, Stan},
  title     = {Toward discovery science of human brain function},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2010},
  volume    = {107},
  number    = {10},
  pages     = {4734--4739},
  publisher = {National Acad Sciences},
}

@Article{Power2012Spurious,
  author   = {Jonathan D. Power and Kelly A. Barnes and Abraham Z. Snyder and Bradley L. Schlaggar and Steven E. Petersen},
  title    = {Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion},
  journal  = {NeuroImage},
  year     = {2012},
  volume   = {59},
  number   = {3},
  pages    = {2142 - 2154},
  issn     = {1053-8119},
  doi      = {http://dx.doi.org/10.1016/j.neuroimage.2011.10.018},
  keywords = {fMRI, fcMRI, Resting state, Network, Motion, Movement, Artifact, Noise},
  url      = {http://www.sciencedirect.com/science/article/pii/S1053811911011815},
}

@Article{Edgar2002Gene,
  author    = {Edgar, Ron and Domrachev, Michael and Lash, Alex E},
  title     = {Gene Expression Omnibus: NCBI gene expression and hybridization array data repository},
  journal   = {Nucleic acids research},
  year      = {2002},
  volume    = {30},
  number    = {1},
  pages     = {207--210},
  publisher = {Oxford University Press},
}

@Article{Xing2014High,
  author    = {Xing, Eric P and Kolar, Mladen and Kim, Seyoung and Chen, Xi},
  title     = {High-dimensional sparse structured input-output models, with applications to gwas},
  journal   = {Practical Applications of Sparse Modeling},
  year      = {2014},
  volume    = {37},
  owner     = {mkolar},
  publisher = {MIT Press},
  timestamp = {2017.07.14},
}

@InCollection{Yuan2016Learning,
  author    = {Yuan, Xiaotong and Li, Ping and Zhang, Tong and Liu, Qingshan and Liu, Guangcan},
  title     = {Learning Additive Exponential Family Graphical Models via $\ell_{2,1}$ -norm Regularized M-Estimation},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {4367--4375},
  url       = {http://papers.nips.cc/paper/6106-learning-additive-exponential-family-graphical-models-via-ell_21-norm-regularized-m-estimation.pdf},
}

@Article{Bradic2017Uniform,
  author  = {Bradic, Jelena and Kolar, Mladen},
  title   = {Uniform inference for high-dimensional quantile regression: linear functionals and regression rank scores},
  journal = {arXiv preprint arXiv:1702.06209},
  year    = {2017},
}

@Book{Sporns2010Networks,
  title     = {Networks of the Brain},
  publisher = {MIT press},
  year      = {2010},
  author    = {Sporns, Olaf},
}

@Article{Supekar2008Network,
  author    = {Kaustubh Supekar and Vinod Menon and Daniel Rubin and Mark Musen and Michael D. Greicius},
  title     = {Network Analysis of Intrinsic Functional Brain Connectivity in Alzheimer's Disease},
  journal   = {{PLoS} Computational Biology},
  year      = {2008},
  volume    = {4},
  number    = {6},
  pages     = {e1000100},
  month     = {jun},
  doi       = {10.1371/journal.pcbi.1000100},
  editor    = {Olaf Sporns},
  publisher = {Public Library of Science ({PLoS})},
  url       = {https://doi.org/10.1371/journal.pcbi.1000100},
}

@Article{Chen2017Double,
  author      = {Jingxiang Chen and Chong Zhang and Michael R. Kosorok and Yufeng Liu},
  title       = {Double Sparsity Kernel Learning with Automatic Variable Selection and Data Extraction},
  journal     = {arXiv e-prints, arXiv: 1706.01426},
  year        = {2017},
  abstract    = {Learning with Reproducing Kernel Hilbert Spaces (RKHS) has been widely used in many scientific disciplines. Because a RKHS can be very flexible, it is common to impose a regularization term in the optimization to prevent overfitting. Standard RKHS learning employs the squared norm penalty of the learning function. Despite its success, many challenges remain. In particular, one cannot directly use the squared norm penalty for variable selection or data extraction. Therefore, when there exists noise predictors, or the underlying function has a sparse representation in the dual space, the performance of standard RKHS learning can be suboptimal. In the literature,work has been proposed on how to perform variable selection in RKHS learning, and a data sparsity constraint was considered for data extraction. However, how to learn in a RKHS with both variable selection and data extraction simultaneously remains unclear. In this paper, we propose a unified RKHS learning method, namely, DOuble Sparsity Kernel (DOSK) learning, to overcome this challenge. An efficient algorithm is provided to solve the corresponding optimization problem. We prove that under certain conditions, our new method can asymptotically achieve variable selection consistency. Simulated and real data results demonstrate that DOSK is highly competitive among existing approaches for RKHS learning.},
  date        = {2017-06-05},
  eprint      = {1706.01426v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1706.01426v1:PDF},
  keywords    = {stat.ME},
}

@Article{Dijk2012influence,
  author    = {Koene R.A. Van Dijk and Mert R. Sabuncu and Randy L. Buckner},
  title     = {The influence of head motion on intrinsic functional connectivity {MRI}},
  journal   = {{NeuroImage}},
  year      = {2012},
  volume    = {59},
  number    = {1},
  pages     = {431--438},
  month     = {jan},
  doi       = {10.1016/j.neuroimage.2011.07.044},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.neuroimage.2011.07.044},
}

@Article{Power2014Methods,
  author    = {Jonathan D. Power and Anish Mitra and Timothy O. Laumann and Abraham Z. Snyder and Bradley L. Schlaggar and Steven E. Petersen},
  title     = {Methods to detect, characterize, and remove motion artifact in resting state {fMRI}},
  journal   = {{NeuroImage}},
  year      = {2014},
  volume    = {84},
  pages     = {320--341},
  month     = {jan},
  doi       = {10.1016/j.neuroimage.2013.08.048},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.neuroimage.2013.08.048},
}

@Article{Satterthwaite2012Impact,
  author    = {Theodore D. Satterthwaite and Daniel H. Wolf and James Loughead and Kosha Ruparel and Mark A. Elliott and Hakon Hakonarson and Ruben C. Gur and Raquel E. Gur},
  title     = {Impact of in-scanner head motion on multiple measures of functional connectivity: Relevance for studies of neurodevelopment in youth},
  journal   = {{NeuroImage}},
  year      = {2012},
  volume    = {60},
  number    = {1},
  pages     = {623--632},
  month     = {mar},
  doi       = {10.1016/j.neuroimage.2011.12.063},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.neuroimage.2011.12.063},
}

@Article{Sutherland2017Efficient,
  author      = {Dougal J. Sutherland and Heiko Strathmann and Michael Arbel and Arthur Gretton},
  title       = {Efficient and principled score estimation},
  journal     = {arXiv e-prints, arXiv:1705.08360},
  year        = {2017},
  abstract    = {We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. We improved the computational efficiency of an earlier solution with a low-rank, Nystr\"om-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees.},
  date        = {2017-05-23},
  eprint      = {1705.08360v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1705.08360v3:PDF},
  keywords    = {stat.ML, cs.LG, stat.ME},
}

@Article{Halko2011Finding,
  author    = {N. Halko and P. G. Martinsson and J. A. Tropp},
  title     = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
  journal   = {{SIAM} Review},
  year      = {2011},
  volume    = {53},
  number    = {2},
  pages     = {217--288},
  month     = {jan},
  doi       = {10.1137/090771806},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  url       = {https://doi.org/10.1137/090771806},
}

@Article{Mahoney2011Randomized,
  author  = {Michael W. Mahoney},
  title   = {Randomized Algorithms for Matrices and Data},
  journal = {Foundations and Trends in Machine Learning},
  year    = {2011},
  volume  = {3},
  number  = {2},
  pages   = {123-224},
  issn    = {1935-8237},
  doi     = {10.1561/2200000035},
  url     = {http://dx.doi.org/10.1561/2200000035},
}

@Article{Raskutti2016Statistical,
  author     = {Raskutti, Garvesh and Mahoney, Michael W.},
  title      = {A Statistical Perspective on Randomized Sketching for Ordinary Least-squares},
  journal    = {J. Mach. Learn. Res.},
  year       = {2016},
  volume     = {17},
  number     = {1},
  pages      = {7508--7538},
  month      = jan,
  issn       = {1532-4435},
  acmid      = {3053495},
  issue_date = {January 2016},
  keywords   = {algorithmic leveraging, random projection, randomized linear algebra, sketching, statistical efficiency, statistical leverage},
  numpages   = {31},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=2946645.3053495},
}

@InCollection{Lu2013Faster,
  author    = {Lu, Yichao and Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},
  title     = {Faster Ridge Regression via the Subsampled Randomized Hadamard Transform},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {369--377},
  url       = {http://papers.nips.cc/paper/5106-faster-ridge-regression-via-the-subsampled-randomized-hadamard-transform.pdf},
}

@Article{Drineas2016RandNLA,
  author    = {Petros Drineas and Michael W. Mahoney},
  title     = {{RandNLA}},
  journal   = {Communications of the {ACM}},
  year      = {2016},
  volume    = {59},
  number    = {6},
  pages     = {80--90},
  month     = {may},
  doi       = {10.1145/2842602},
  publisher = {Association for Computing Machinery ({ACM})},
  url       = {https://doi.org/10.1145/2842602},
}

@Article{Zhang2014Random,
  author    = {Lijun Zhang and Mehrdad Mahdavi and Rong Jin and Tianbao Yang and Shenghuo Zhu},
  title     = {Random Projections for Classification: A Recovery Approach},
  journal   = {{IEEE} Transactions on Information Theory},
  year      = {2014},
  volume    = {60},
  number    = {11},
  pages     = {7300--7316},
  month     = {nov},
  doi       = {10.1109/tit.2014.2359204},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url       = {https://doi.org/10.1109/tit.2014.2359204},
}

@Article{Woodruff2014Sketching,
  author  = {David P. Woodruff},
  title   = {Sketching as a Tool for Numerical Linear Algebra},
  journal = {Foundations and Trends in Theoretical Computer Science},
  year    = {2014},
  volume  = {10},
  number  = {12},
  pages   = {1-157},
  issn    = {1551-305X},
  doi     = {10.1561/0400000060},
  url     = {http://dx.doi.org/10.1561/0400000060},
}

@Article{Ramsay1982When,
  author   = {Ramsay, J. O.},
  title    = {When the data are functions},
  journal  = {Psychometrika},
  year     = {1982},
  volume   = {47},
  number   = {4},
  pages    = {379--396},
  issn     = {0033-3123},
  doi      = {10.1007/BF02293704},
  fjournal = {Psychometrika. A Journal Devoted to the Development of Psychology as a Quantitative Rational Science},
  mrclass  = {62H25 (62-07 62H20)},
  mrnumber = {691828},
  url      = {http://dx.doi.org/10.1007/BF02293704},
}

@Article{Ramsay1991Some,
  author     = {Ramsay, J. O. and Dalzell, C. J.},
  title      = {Some tools for functional data analysis},
  journal    = {J. Roy. Statist. Soc. Ser. B},
  year       = {1991},
  volume     = {53},
  number     = {3},
  pages      = {539--572},
  issn       = {0035-9246},
  note       = {With discussion and a reply by the authors},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Methodological},
  mrclass    = {62H25},
  mrnumber   = {1125714},
  mrreviewer = {P. A. K. Covey-Crump},
  url        = {http://links.jstor.org/sici?sici=0035-9246(1991)53:3<539:STFFDA>2.0.CO;2-W&origin=MSN},
}

@Book{Ramsay2002Applied,
  title     = {Applied functional data analysis},
  publisher = {Springer-Verlag, New York},
  year      = {2002},
  author    = {Ramsay, J. O. and Silverman, B. W.},
  series    = {Springer Series in Statistics},
  isbn      = {0-387-95414-7},
  note      = {Methods and case studies},
  doi       = {10.1007/b98886},
  mrclass   = {62-02 (62G07 62H25 62H30)},
  mrnumber  = {1910407},
  pages     = {x+190},
  url       = {http://dx.doi.org/10.1007/b98886},
}

@Book{Ramsay2005Functional,
  title     = {Functional data analysis},
  publisher = {Springer, New York},
  year      = {2005},
  author    = {Ramsay, J. O. and Silverman, B. W.},
  series    = {Springer Series in Statistics},
  edition   = {Second},
  isbn      = {978-0387-40080-8; 0-387-40080-X},
  mrclass   = {62-02 (62-07 62H25 62H30)},
  mrnumber  = {2168993},
  pages     = {xx+426},
}

@Article{Wang2016Functional,
  author    = {Jane-Ling Wang and Jeng-Min Chiou and Hans-Georg M\"{u}ller},
  title     = {Functional Data Analysis},
  journal   = {Annual Review of Statistics and Its Application},
  year      = {2016},
  volume    = {3},
  number    = {1},
  pages     = {257--295},
  month     = {jun},
  doi       = {10.1146/annurev-statistics-041715-033624},
  file      = {:Wang2016Functional.pdf:PDF},
  publisher = {Annual Reviews},
  url       = {https://doi.org/10.1146/annurev-statistics-041715-033624},
}

@Article{Mueller2005Functional,
  author   = {M\"uller, Hans-Georg},
  title    = {Functional modelling and classification of longitudinal data},
  journal  = {Scand. J. Statist.},
  year     = {2005},
  volume   = {32},
  number   = {2},
  pages    = {223--246},
  issn     = {0303-6898},
  note     = {With discussions by Ivar Heuch, Rima Izem, and James O. Ramsay and a rejoinder by the author},
  doi      = {10.1111/j.1467-9469.2005.00429.x},
  fjournal = {Scandinavian Journal of Statistics. Theory and Applications},
  mrclass  = {62G05 (62H25)},
  mrnumber = {2188671},
  url      = {http://dx.doi.org/10.1111/j.1467-9469.2005.00429.x},
}

@InCollection{Mueller2009Functional,
  author    = {M\"uller, Hans-Georg},
  title     = {Functional modeling of longitudinal data},
  booktitle = {Longitudinal data analysis},
  publisher = {CRC Press, Boca Raton, FL},
  year      = {2009},
  series    = {Chapman \& Hall/CRC Handb. Mod. Stat. Methods},
  pages     = {223--251},
  mrclass   = {62G08 (62G07)},
  mrnumber  = {1500120},
}

@Article{Rice2004Functional,
  author   = {Rice, John A.},
  title    = {Functional and longitudinal data analysis: perspectives on smoothing},
  journal  = {Statist. Sinica},
  year     = {2004},
  volume   = {14},
  number   = {3},
  pages    = {631--647},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {62-07 (62G05)},
  mrnumber = {2087966},
}

@Article{Zhao2004functional,
  author   = {Zhao, Xin and Marron, J. S. and Wells, Martin T.},
  title    = {The functional data analysis view of longitudinal data},
  journal  = {Statist. Sinica},
  year     = {2004},
  volume   = {14},
  number   = {3},
  pages    = {789--808},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {62-07},
  mrnumber = {2087973},
}

@Book{Ferraty2006Nonparametric,
  title      = {Nonparametric functional data analysis},
  publisher  = {Springer, New York},
  year       = {2006},
  author     = {Ferraty, Fr\'ed\'eric and Vieu, Philippe},
  series     = {Springer Series in Statistics},
  isbn       = {0-387-30369-3; 978-0387-30369-7},
  note       = {Theory and practice},
  mrclass    = {62-02 (62G05 62G07 62G08 62G20 62H30 62M10)},
  mrnumber   = {2229687},
  mrreviewer = {Fazil A. Aliev},
  pages      = {xx+258},
}

@Book{Hsing2015Theoretical,
  title      = {Theoretical foundations of functional data analysis, with an introduction to linear operators},
  publisher  = {John Wiley \& Sons, Ltd., Chichester},
  year       = {2015},
  author     = {Hsing, Tailen and Eubank, Randall},
  series     = {Wiley Series in Probability and Statistics},
  isbn       = {978-0-470-01691-6},
  doi        = {10.1002/9781118762547},
  mrclass    = {62-02 (46-01 47-01 60G17 62G05 62H20 62H25 62J05)},
  mrnumber   = {3379106},
  mrreviewer = {David Benner Hitchcock},
  pages      = {xiv+334},
  url        = {http://dx.doi.org/10.1002/9781118762547},
}

@Book{Horvath2012Inference,
  title      = {Inference for functional data with applications},
  publisher  = {Springer, New York},
  year       = {2012},
  author     = {Horv\'ath, Lajos and Kokoszka, Piotr},
  series     = {Springer Series in Statistics},
  isbn       = {978-1-4614-3654-6},
  doi        = {10.1007/978-1-4614-3655-3},
  mrclass    = {62-02 (62G08 62H20 62H25 62J05 62M10 62M30)},
  mrnumber   = {2920735},
  mrreviewer = {Antonio Cuevas},
  pages      = {xiv+422},
  url        = {http://dx.doi.org/10.1007/978-1-4614-3655-3},
}

@Book{Ramsay2009Functional,
  title     = {Functional data analysis with R and MATLAB},
  publisher = {Springer Science \& Business Media},
  year      = {2009},
  author    = {Ramsay, James O and Hooker, Giles and Graves, Spencer},
  owner     = {mkolar},
  timestamp = {2017.09.13},
}

@Book{Wu2006Nonparametric,
  title      = {Nonparametric regression methods for longitudinal data analysis},
  publisher  = {Wiley-Interscience [John Wiley \& Sons], Hoboken, NJ},
  year       = {2006},
  author     = {Wu, Hulin and Zhang, Jin-Ting},
  series     = {Wiley Series in Probability and Statistics},
  isbn       = {978-0-471-48350-2; 0-471-48350-8},
  mrclass    = {62-02 (62-07 62G05)},
  mrnumber   = {2216899},
  mrreviewer = {Atanu Biswas},
  pages      = {xxii+369},
}

@Article{Yao2005Functional,
  author     = {Yao, Fang and M\"uller, Hans-Georg and Wang, Jane-Ling},
  title      = {Functional data analysis for sparse longitudinal data},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {2005},
  volume     = {100},
  number     = {470},
  pages      = {577--590},
  issn       = {0162-1459},
  doi        = {10.1198/016214504000001745},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62H25 (62G05)},
  mrnumber   = {2160561},
  mrreviewer = {M. Riedel},
  url        = {http://dx.doi.org/10.1198/016214504000001745},
}

@Article{Li2010Uniform,
  author     = {Li, Yehua and Hsing, Tailen},
  title      = {Uniform convergence rates for nonparametric regression and principal component analysis in functional/longitudinal data},
  journal    = {Ann. Statist.},
  year       = {2010},
  volume     = {38},
  number     = {6},
  pages      = {3321--3351},
  issn       = {0090-5364},
  doi        = {10.1214/10-AOS813},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62J05 (60F15 62G08 62G20 62H25 62M20)},
  mrnumber   = {2766854},
  mrreviewer = {Daniela Rodriguez},
  url        = {http://dx.doi.org/10.1214/10-AOS813},
}

@Article{Zhang2016sparse,
  author    = {Zhang, Xiaoke and Wang, Jane-Ling},
  title     = {From sparse to dense functional data and beyond},
  journal   = {Ann. Statist.},
  year      = {2016},
  volume    = {44},
  number    = {5},
  pages     = {2281--2321},
  issn      = {0090-5364},
  doi       = {10.1214/16-AOS1446},
  file      = {:Zhang2016sparse.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  mrclass   = {62G20 (62G05 62G08)},
  mrnumber  = {3546451},
  timestamp = {2018.05.09},
  url       = {http://dx.doi.org/10.1214/16-AOS1446},
}

@Book{Jolliffe2002Principal,
  title     = {Principal component analysis},
  publisher = {Springer-Verlag, New York},
  year      = {2002},
  author    = {Jolliffe, I. T.},
  series    = {Springer Series in Statistics},
  edition   = {Second},
  isbn      = {0-387-95442-2},
  mrclass   = {62-02 (62H25)},
  mrnumber  = {2036084},
  pages     = {xxx+487},
}

@Article{Dauxois1982Asymptotic,
  author     = {Dauxois, J. and Pousse, A. and Romain, Y.},
  title      = {Asymptotic theory for the principal component analysis of a vector random function: some applications to statistical inference},
  journal    = {J. Multivariate Anal.},
  year       = {1982},
  volume     = {12},
  number     = {1},
  pages      = {136--154},
  issn       = {0047-259X},
  doi        = {10.1016/0047-259X(82)90088-4},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {62H25},
  mrnumber   = {650934},
  mrreviewer = {S. John},
  url        = {http://dx.doi.org/10.1016/0047-259X(82)90088-4},
}

@Article{Karhunen1946Zur,
  author     = {Karhunen, Kari},
  title      = {Zur {S}pektraltheorie stochastischer {P}rozesse},
  journal    = {Ann. Acad. Sci. Fennicae. Ser. A. I. Math.-Phys.},
  year       = {1946},
  volume     = {1946},
  number     = {34},
  pages      = {7},
  mrclass    = {60.0X},
  mrnumber   = {0023012},
  mrreviewer = {J. L. Doob},
}

@Article{Loeve1946Fonctions,
  author     = {Lo\`eve, Michel},
  title      = {Fonctions al\'eatoires \`a d\'ecomposition orthogonale exponentielle},
  journal    = {Revue Sci.},
  year       = {1946},
  volume     = {84},
  pages      = {159--162},
  mrclass    = {60.0X},
  mrnumber   = {0017892},
  mrreviewer = {J. L. Doob},
}

@Article{Hall2006Properties,
  author     = {Hall, Peter and M\"uller, Hans-Georg and Wang, Jane-Ling},
  title      = {Properties of principal component methods for functional and longitudinal data analysis},
  journal    = {Ann. Statist.},
  year       = {2006},
  volume     = {34},
  number     = {3},
  pages      = {1493--1517},
  issn       = {0090-5364},
  doi        = {10.1214/009053606000000272},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G08 (62H25 62M09)},
  mrnumber   = {2278365},
  mrreviewer = {Stephan Morgenthaler},
  url        = {http://dx.doi.org/10.1214/009053606000000272},
}

@Article{Besse1986Principal,
  author   = {Besse, Philippe and Ramsay, J. O.},
  title    = {Principal components analysis of sampled functions},
  journal  = {Psychometrika},
  year     = {1986},
  volume   = {51},
  number   = {2},
  pages    = {285--311},
  issn     = {0033-3123},
  doi      = {10.1007/BF02293986},
  fjournal = {Psychometrika. A Journal Devoted to the Development of Psychology as a Quantitative Rational Science},
  mrclass  = {62H25},
  mrnumber = {848110},
  url      = {http://dx.doi.org/10.1007/BF02293986},
}

@Article{Silverman1996Smoothed,
  author   = {Silverman, Bernard W.},
  title    = {Smoothed functional principal components analysis by choice of norm},
  journal  = {Ann. Statist.},
  year     = {1996},
  volume   = {24},
  number   = {1},
  pages    = {1--24},
  issn     = {0090-5364},
  doi      = {10.1214/aos/1033066196},
  fjournal = {The Annals of Statistics},
  mrclass  = {62G05 (62H25)},
  mrnumber = {1389877},
  url      = {http://dx.doi.org/10.1214/aos/1033066196},
}

@Book{Bosq2000Linear,
  title      = {Linear processes in function spaces},
  publisher  = {Springer-Verlag, New York},
  year       = {2000},
  author     = {Bosq, D.},
  volume     = {149},
  series     = {Lecture Notes in Statistics},
  isbn       = {0-387-95052-4},
  note       = {Theory and applications},
  doi        = {10.1007/978-1-4612-1154-9},
  mrclass    = {60G12 (46N30 47N30 60B12 62G99 62M09 62M20)},
  mrnumber   = {1783138},
  mrreviewer = {Jos\'e Rafael Le\'on},
  pages      = {xiv+283},
  url        = {http://dx.doi.org/10.1007/978-1-4612-1154-9},
}

@Article{Boente2000Kernel,
  author   = {Boente, Graciela and Fraiman, Ricardo},
  title    = {Kernel-based functional principal components},
  journal  = {Statist. Probab. Lett.},
  year     = {2000},
  volume   = {48},
  number   = {4},
  pages    = {335--345},
  issn     = {0167-7152},
  doi      = {10.1016/S0167-7152(00)00014-6},
  fjournal = {Statistics \& Probability Letters},
  mrclass  = {62G08 (62H25 62M99)},
  mrnumber = {1771495},
  url      = {http://dx.doi.org/10.1016/S0167-7152(00)00014-6},
}

@Article{Hall2006onproperties,
  author   = {Hall, Peter and Hosseini-Nasab, Mohammad},
  title    = {On properties of functional principal components analysis},
  journal  = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  year     = {2006},
  volume   = {68},
  number   = {1},
  pages    = {109--126},
  issn     = {1369-7412},
  doi      = {10.1111/j.1467-9868.2005.00535.x},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62H25 (62-07)},
  mrnumber = {2212577},
  url      = {http://dx.doi.org/10.1111/j.1467-9868.2005.00535.x},
}

@Article{Castro1986Principal,
  author    = {P. E. Castro and W. H. Lawton and E. A. Sylvestre},
  title     = {Principal Modes of Variation for Processes with Continuous Sample Curves},
  journal   = {Technometrics},
  year      = {1986},
  volume    = {28},
  number    = {4},
  pages     = {329},
  month     = {nov},
  doi       = {10.2307/1268982},
  publisher = {{JSTOR}},
  url       = {https://doi.org/10.2307/1268982},
}

@Article{Rice1991Estimating,
  author   = {Rice, John A. and Silverman, B. W.},
  title    = {Estimating the mean and covariance structure nonparametrically when the data are curves},
  journal  = {J. Roy. Statist. Soc. Ser. B},
  year     = {1991},
  volume   = {53},
  number   = {1},
  pages    = {233--243},
  issn     = {0035-9246},
  fjournal = {Journal of the Royal Statistical Society. Series B. Methodological},
  mrclass  = {62G05},
  mrnumber = {1094283},
  url      = {http://links.jstor.org/sici?sici=0035-9246(1991)53:1<233:ETMACS>2.0.CO;2-C&origin=MSN},
}

@Article{Pezzulli1993Some,
  author     = {Pezzulli, S. and Silverman, B. W.},
  title      = {Some properties of smoothed principal components analysis for functional data},
  journal    = {Comput. Statist.},
  year       = {1993},
  volume     = {8},
  number     = {1},
  pages      = {1--16},
  issn       = {0943-4062},
  fjournal   = {Computational Statistics},
  mrclass    = {62H25},
  mrnumber   = {1220336},
  mrreviewer = {Jacques Dauxois},
}

@Article{Cardot2000Nonparametric,
  author   = {Cardot, Herv\'e},
  title    = {Nonparametric estimation of smoothed principal components analysis of sampled noisy functions},
  journal  = {J. Nonparametr. Statist.},
  year     = {2000},
  volume   = {12},
  number   = {4},
  pages    = {503--538},
  issn     = {1048-5252},
  doi      = {10.1080/10485250008832820},
  fjournal = {Journal of Nonparametric Statistics},
  mrclass  = {62G08 (62G20 62H25)},
  mrnumber = {1785396},
  url      = {http://dx.doi.org/10.1080/10485250008832820},
}

@Article{Cardot2007Conditional,
  author   = {Cardot, Herv\'e},
  title    = {Conditional functional principal components analysis},
  journal  = {Scand. J. Statist.},
  year     = {2007},
  volume   = {34},
  number   = {2},
  pages    = {317--335},
  issn     = {0303-6898},
  doi      = {10.1111/j.1467-9469.2006.00521.x},
  fjournal = {Scandinavian Journal of Statistics. Theory and Applications},
  mrclass  = {62H25 (62G05 62H20)},
  mrnumber = {2346642},
  url      = {http://dx.doi.org/10.1111/j.1467-9469.2006.00521.x},
}

@Article{Shi1996Analysis,
  author    = {Minggao Shi and Robert E. Weiss and Jeremy M. G. Taylor},
  title     = {An Analysis of Paediatric {CD}4 Counts for Acquired Immune Deficiency Syndrome Using Flexible Random Curves},
  journal   = {Applied Statistics},
  year      = {1996},
  volume    = {45},
  number    = {2},
  pages     = {151},
  doi       = {10.2307/2986151},
  publisher = {{JSTOR}},
  url       = {https://doi.org/10.2307/2986151},
}

@Article{Staniswalis1998Nonparametric,
  author   = {Staniswalis, Joan G. and Lee, J. Jack},
  title    = {Nonparametric regression analysis of longitudinal data},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {1998},
  volume   = {93},
  number   = {444},
  pages    = {1403--1418},
  issn     = {0162-1459},
  doi      = {10.2307/2670055},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62G07 (62P10)},
  mrnumber = {1666636},
  url      = {http://dx.doi.org/10.2307/2670055},
}

@Article{James2000Principal,
  author   = {James, Gareth M. and Hastie, Trevor J. and Sugar, Catherine A.},
  title    = {Principal component models for sparse functional data},
  journal  = {Biometrika},
  year     = {2000},
  volume   = {87},
  number   = {3},
  pages    = {587--602},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/87.3.587},
  fjournal = {Biometrika},
  mrclass  = {62H25 (62G05)},
  mrnumber = {1789811},
  url      = {http://dx.doi.org/10.1093/biomet/87.3.587},
}

@Article{Rice2001Nonparametric,
  author   = {Rice, John A. and Wu, Colin O.},
  title    = {Nonparametric mixed effects models for unequally sampled noisy curves},
  journal  = {Biometrics},
  year     = {2001},
  volume   = {57},
  number   = {1},
  pages    = {253--259},
  issn     = {0006-341X},
  doi      = {10.1111/j.0006-341X.2001.00253.x},
  fjournal = {Biometrics. Journal of the International Biometric Society},
  mrclass  = {Expansion},
  mrnumber = {1833314},
  url      = {http://dx.doi.org/10.1111/j.0006-341X.2001.00253.x},
}

@Article{Yao2005Functionala,
  author   = {Yao, Fang and M\"uller, Hans-Georg and Wang, Jane-Ling},
  title    = {Functional linear regression analysis for longitudinal data},
  journal  = {Ann. Statist.},
  year     = {2005},
  volume   = {33},
  number   = {6},
  pages    = {2873--2903},
  issn     = {0090-5364},
  doi      = {10.1214/009053605000000660},
  fjournal = {The Annals of Statistics},
  mrclass  = {62M20 (60G15 62G05)},
  mrnumber = {2253106},
  url      = {http://dx.doi.org/10.1214/009053605000000660},
}

@Article{Yao2006Penalized,
  author     = {Yao, Fang and Lee, Thomas C. M.},
  title      = {Penalized spline models for functional principal component analysis},
  journal    = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  year       = {2006},
  volume     = {68},
  number     = {1},
  pages      = {3--25},
  issn       = {1369-7412},
  doi        = {10.1111/j.1467-9868.2005.00530.x},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62H25 (62H20 65D07 93E14)},
  mrnumber   = {2212572},
  mrreviewer = {Nicoleta M. Breaz},
  url        = {http://dx.doi.org/10.1111/j.1467-9868.2005.00530.x},
}

@Article{Paul2009Consistency,
  author     = {Paul, Debashis and Peng, Jie},
  title      = {Consistency of restricted maximum likelihood estimators of principal components},
  journal    = {Ann. Statist.},
  year       = {2009},
  volume     = {37},
  number     = {3},
  pages      = {1229--1271},
  issn       = {0090-5364},
  doi        = {10.1214/08-AOS608},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G20 (62H25)},
  mrnumber   = {2509073},
  mrreviewer = {Yingcun Xia},
  url        = {http://dx.doi.org/10.1214/08-AOS608},
}

@Article{Chiou2007Diagnostics,
  author   = {Chiou, Jeng-Min and M\"uller, Hans-Georg},
  title    = {Diagnostics for functional regression via residual processes},
  journal  = {Comput. Statist. Data Anal.},
  year     = {2007},
  volume   = {51},
  number   = {10},
  pages    = {4849--4863},
  issn     = {0167-9473},
  doi      = {10.1016/j.csda.2006.07.042},
  fjournal = {Computational Statistics \& Data Analysis},
  mrclass  = {Expansion},
  mrnumber = {2364544},
  url      = {http://dx.doi.org/10.1016/j.csda.2006.07.042},
}

@Article{Chiou2003Functional,
  author   = {Chiou, Jeng-Min and M\"uller, Hans-Georg and Wang, Jane-Ling},
  title    = {Functional quasi-likelihood regression models with smooth random effects},
  journal  = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  year     = {2003},
  volume   = {65},
  number   = {2},
  pages    = {405--423},
  issn     = {1369-7412},
  doi      = {10.1111/1467-9868.00393},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62G08 (62H05)},
  mrnumber = {1983755},
  url      = {http://dx.doi.org/10.1111/1467-9868.00393},
}

@Article{Chiou2009Modeling,
  author   = {Chiou, Jeng-Min and M\"uller, Hans-Georg},
  title    = {Modeling hazard rates as functional data for the analysis of cohort lifetables and mortality forecasting},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2009},
  volume   = {104},
  number   = {486},
  pages    = {572--585},
  issn     = {0162-1459},
  doi      = {10.1198/jasa.2009.0023},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {Expansion},
  mrnumber = {2751439},
  url      = {http://dx.doi.org/10.1198/jasa.2009.0023},
}

@Article{Jiang2010Covariate,
  author   = {Jiang, Ci-Ren and Wang, Jane-Ling},
  title    = {Covariate adjusted functional principal components analysis for longitudinal data},
  journal  = {Ann. Statist.},
  year     = {2010},
  volume   = {38},
  number   = {2},
  pages    = {1194--1226},
  issn     = {0090-5364},
  doi      = {10.1214/09-AOS742},
  fjournal = {The Annals of Statistics},
  mrclass  = {62H25 (62G20 62M15)},
  mrnumber = {2604710},
  url      = {http://dx.doi.org/10.1214/09-AOS742},
}

@Article{Crambes2008Robust,
  author     = {Crambes, C. and Delsol, L. and Laksaci, A.},
  title      = {Robust nonparametric estimation for functional data},
  journal    = {J. Nonparametr. Stat.},
  year       = {2008},
  volume     = {20},
  number     = {7},
  pages      = {573--598},
  issn       = {1048-5252},
  doi        = {10.1080/10485250802331524},
  fjournal   = {Journal of Nonparametric Statistics},
  mrclass    = {62G08 (62G35)},
  mrnumber   = {2454613},
  mrreviewer = {Anil Kumar Ghosh},
  url        = {http://dx.doi.org/10.1080/10485250802331524},
}

@Article{Gervini2008Robust,
  author   = {Gervini, Daniel},
  title    = {Robust functional estimation using the median and spherical principal components},
  journal  = {Biometrika},
  year     = {2008},
  volume   = {95},
  number   = {3},
  pages    = {587--600},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/asn031},
  fjournal = {Biometrika},
  mrclass  = {62M09},
  mrnumber = {2443177},
  url      = {http://dx.doi.org/10.1093/biomet/asn031},
}

@Article{Bali2011Robust,
  author     = {Bali, Juan Lucas and Boente, Graciela and Tyler, David E. and Wang, Jane-Ling},
  title      = {Robust functional principal components: a projection-pursuit approach},
  journal    = {Ann. Statist.},
  year       = {2011},
  volume     = {39},
  number     = {6},
  pages      = {2852--2882},
  issn       = {0090-5364},
  doi        = {10.1214/11-AOS923},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G35 (62G20 62H25)},
  mrnumber   = {3012394},
  mrreviewer = {David Benner Hitchcock},
  url        = {http://dx.doi.org/10.1214/11-AOS923},
}

@Article{Kraus2012Dispersion,
  author    = {D. Kraus and V. M. Panaretos},
  title     = {Dispersion operators and resistant second-order functional data analysis},
  journal   = {Biometrika},
  year      = {2012},
  volume    = {99},
  number    = {4},
  pages     = {813--832},
  month     = {aug},
  doi       = {10.1093/biomet/ass037},
  publisher = {Oxford University Press ({OUP})},
  url       = {https://doi.org/10.1093/biomet/ass037},
}

@Article{Boente2015S,
  author   = {Boente, Graciela and Salibian-Barrera, Mat\'\i as},
  title    = {{$S$}-estimators for functional principal component analysis},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2015},
  volume   = {110},
  number   = {511},
  pages    = {1100--1111},
  issn     = {0162-1459},
  doi      = {10.1080/01621459.2014.946991},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62H25 (62G35)},
  mrnumber = {3420687},
  url      = {http://dx.doi.org/10.1080/01621459.2014.946991},
}

@Article{Grenander1950Stochastic,
  author     = {Grenander, Ulf},
  title      = {Stochastic processes and statistical inference},
  journal    = {Ark. Mat.},
  year       = {1950},
  volume     = {1},
  pages      = {195--277},
  issn       = {0004-2080},
  doi        = {10.1007/BF02590638},
  fjournal   = {Arkiv f\"or Matematik},
  mrclass    = {62.0X},
  mrnumber   = {0039202},
  mrreviewer = {J. L. Doob},
  url        = {http://dx.doi.org/10.1007/BF02590638},
}

@Article{Morris2015Functional,
  author    = {Jeffrey S. Morris},
  title     = {Functional Regression},
  journal   = {Annual Review of Statistics and Its Application},
  year      = {2015},
  volume    = {2},
  number    = {1},
  pages     = {321--359},
  month     = {apr},
  doi       = {10.1146/annurev-statistics-010814-020413},
  publisher = {Annual Reviews},
  url       = {https://doi.org/10.1146/annurev-statistics-010814-020413},
}

@Article{Cardot1999Functional,
  author   = {Cardot, Herv\'e and Ferraty, Fr\'ed\'eric and Sarda, Pascal},
  title    = {Functional linear model},
  journal  = {Statist. Probab. Lett.},
  year     = {1999},
  volume   = {45},
  number   = {1},
  pages    = {11--22},
  issn     = {0167-7152},
  doi      = {10.1016/S0167-7152(99)00036-X},
  fjournal = {Statistics \& Probability Letters},
  mrclass  = {62J02 (62G08)},
  mrnumber = {1718346},
  url      = {http://dx.doi.org/10.1016/S0167-7152(99)00036-X},
}

@Article{Cardot2003Spline,
  author   = {Cardot, Herv\'e and Ferraty, Fr\'ed\'eric and Sarda, Pascal},
  title    = {Spline estimators for the functional linear model},
  journal  = {Statist. Sinica},
  year     = {2003},
  volume   = {13},
  number   = {3},
  pages    = {571--591},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {62G05},
  mrnumber = {1997162},
}

@Article{Hall2007Methodology,
  author     = {Hall, Peter and Horowitz, Joel L.},
  title      = {Methodology and convergence rates for functional linear regression},
  journal    = {Ann. Statist.},
  year       = {2007},
  volume     = {35},
  number     = {1},
  pages      = {70--91},
  issn       = {0090-5364},
  doi        = {10.1214/009053606000000957},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62J05 (62G20)},
  mrnumber   = {2332269},
  mrreviewer = {Yong Song Qin},
  url        = {http://dx.doi.org/10.1214/009053606000000957},
}

@Article{Hu2004Profile,
  author    = {Z. Hu},
  title     = {Profile-kernel versus backfitting in the partially linear models for longitudinal/clustered data},
  journal   = {Biometrika},
  year      = {2004},
  volume    = {91},
  number    = {2},
  pages     = {251--262},
  month     = {jun},
  doi       = {10.1093/biomet/91.2.251},
  publisher = {Oxford University Press ({OUP})},
  url       = {https://doi.org/10.1093/biomet/91.2.251},
}

@Article{James2002Generalized,
  author   = {James, Gareth M.},
  title    = {Generalized linear models with functional predictors},
  journal  = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  year     = {2002},
  volume   = {64},
  number   = {3},
  pages    = {411--432},
  issn     = {1369-7412},
  doi      = {10.1111/1467-9868.00342},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62J12 (62G08 62H25)},
  mrnumber = {1924298},
  url      = {http://dx.doi.org/10.1111/1467-9868.00342},
}

@Article{Cardot2005Estimation,
  author     = {Cardot, Herv\'e and Sarda, Pacal},
  title      = {Estimation in generalized linear models for functional data via penalized likelihood},
  journal    = {J. Multivariate Anal.},
  year       = {2005},
  volume     = {92},
  number     = {1},
  pages      = {24--41},
  issn       = {0047-259X},
  doi        = {10.1016/j.jmva.2003.08.008},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {62J12 (62G05 62M20)},
  mrnumber   = {2102242},
  mrreviewer = {Luis Firinguetti},
  url        = {http://dx.doi.org/10.1016/j.jmva.2003.08.008},
}

@Article{Wang2010Generalized,
  author   = {Wang, Suojin and Qian, Lianfen and Carroll, Raymond J.},
  title    = {Generalized empirical likelihood methods for analyzing longitudinal data},
  journal  = {Biometrika},
  year     = {2010},
  volume   = {97},
  number   = {1},
  pages    = {79--93},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/asp073},
  fjournal = {Biometrika},
  mrclass  = {62F10 (62F25)},
  mrnumber = {2594418},
  url      = {http://dx.doi.org/10.1093/biomet/asp073},
}

@Article{Mueller2005Generalized,
  author     = {M\"uller, Hans-Georg and Stadtm\"uller, Ulrich},
  title      = {Generalized functional linear models},
  journal    = {Ann. Statist.},
  year       = {2005},
  volume     = {33},
  number     = {2},
  pages      = {774--805},
  issn       = {0090-5364},
  doi        = {10.1214/009053604000001156},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (62G20 62H30 62M09)},
  mrnumber   = {2163159},
  mrreviewer = {Arnak S. Dalalyan},
  url        = {http://dx.doi.org/10.1214/009053604000001156},
}

@Article{Chen2011Single,
  author     = {Chen, Dong and Hall, Peter and M\"uller, Hans-Georg},
  title      = {Single and multiple index functional regression models with nonparametric link},
  journal    = {Ann. Statist.},
  year       = {2011},
  volume     = {39},
  number     = {3},
  pages      = {1720--1747},
  issn       = {0090-5364},
  doi        = {10.1214/11-AOS882},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (62G08)},
  mrnumber   = {2850218},
  mrreviewer = {Dimitris A. Ioannides},
  url        = {http://dx.doi.org/10.1214/11-AOS882},
}

@Article{Ferre2003Functional,
  author    = {L. Ferr{\'{e}} and A. F. Yao},
  title     = {Functional sliced inverse regression analysis},
  journal   = {Statistics},
  year      = {2003},
  volume    = {37},
  number    = {6},
  pages     = {475--488},
  month     = {nov},
  doi       = {10.1080/0233188031000112845},
  publisher = {Informa {UK} Limited},
  url       = {https://doi.org/10.1080/0233188031000112845},
}

@Article{Ferre2007Reply,
  author   = {Ferr\'e, L. and Yao, A. F.},
  title    = {Reply to the paper by {L}iliana {F}orzani and {R}. {D}ennis {C}ook: ``{A} note on smoothed functional inverse regression''},
  journal  = {Statist. Sinica},
  year     = {2007},
  volume   = {17},
  number   = {4},
  pages    = {1683--1687},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {Expansion},
  mrnumber = {2413540},
}

@Article{Ferre2005Smoothed,
  author     = {Ferr\'e, Louis and Yao, Anne-Fran\c{c}oise},
  title      = {Smoothed functional inverse regression},
  journal    = {Statist. Sinica},
  year       = {2005},
  volume     = {15},
  number     = {3},
  pages      = {665--683},
  issn       = {1017-0405},
  fjournal   = {Statistica Sinica},
  mrclass    = {62G05},
  mrnumber   = {2233905},
  mrreviewer = {Yves Lepage},
}

@Article{Cook2010Necessary,
  author     = {Cook, R. D. and Forzani, L. and Yao, A. F.},
  title      = {Necessary and sufficient conditions for consistency of a method for smoothed functional inverse regression},
  journal    = {Statist. Sinica},
  year       = {2010},
  volume     = {20},
  number     = {1},
  pages      = {235--238},
  issn       = {1017-0405},
  fjournal   = {Statistica Sinica},
  mrclass    = {62G08 (62H25)},
  mrnumber   = {2640692},
  mrreviewer = {Tianfa Xie},
}

@Article{Jiang2014Inverse,
  author     = {Jiang, Ci-Ren and Yu, Wei and Wang, Jane-Ling},
  title      = {Inverse regression for longitudinal data},
  journal    = {Ann. Statist.},
  year       = {2014},
  volume     = {42},
  number     = {2},
  pages      = {563--591},
  issn       = {0090-5364},
  doi        = {10.1214/13-AOS1193},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (62G08 62G20)},
  mrnumber   = {3210979},
  mrreviewer = {Gilles Stupfler},
  url        = {http://dx.doi.org/10.1214/13-AOS1193},
}

@Article{Fan1999Statistical,
  author     = {Fan, Jianqing and Zhang, Wenyang},
  title      = {Statistical estimation in varying coefficient models},
  journal    = {Ann. Statist.},
  year       = {1999},
  volume     = {27},
  number     = {5},
  pages      = {1491--1518},
  issn       = {0090-5364},
  doi        = {10.1214/aos/1017939139},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G07 (62J12)},
  mrnumber   = {1742497},
  mrreviewer = {Alexander G. Kukush},
  url        = {http://dx.doi.org/10.1214/aos/1017939139},
}

@Article{Hoover1998Nonparametric,
  author     = {Hoover, Donald R. and Rice, John A. and Wu, Colin O. and Yang, Li-Ping},
  title      = {Nonparametric smoothing estimates of time-varying coefficient models with longitudinal data},
  journal    = {Biometrika},
  year       = {1998},
  volume     = {85},
  number     = {4},
  pages      = {809--822},
  issn       = {0006-3444},
  doi        = {10.1093/biomet/85.4.809},
  fjournal   = {Biometrika},
  mrclass    = {62G08 (62G20)},
  mrnumber   = {1666699},
  mrreviewer = {Theo Gasser},
  url        = {http://dx.doi.org/10.1093/biomet/85.4.809},
}

@Article{Wu2000Kernel,
  author   = {Wu, Colin O. and Chiang, Chin-Tsang},
  title    = {Kernel smoothing on varying coefficient models with longitudinal dependent variable},
  journal  = {Statist. Sinica},
  year     = {2000},
  volume   = {10},
  number   = {2},
  pages    = {433--456},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {62G07 (62G08)},
  mrnumber = {1769751},
}

@Article{Chiang2001Smoothing,
  author   = {Chiang, Chin-Tsang and Rice, John A. and Wu, Colin O.},
  title    = {Smoothing spline estimation for varying coefficient models with repeatedly measured dependent variables},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2001},
  volume   = {96},
  number   = {454},
  pages    = {605--619},
  issn     = {0162-1459},
  doi      = {10.1198/016214501753168280},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62G05 (62G20 62P10)},
  mrnumber = {1946428},
  url      = {http://dx.doi.org/10.1198/016214501753168280},
}

@Article{Eggermont2010Convergence,
  author   = {Eggermont, P. P. B. and Eubank, R. L. and LaRiccia, V. N.},
  title    = {Convergence rates for smoothing spline estimators in varying coefficient models},
  journal  = {J. Statist. Plann. Inference},
  year     = {2010},
  volume   = {140},
  number   = {2},
  pages    = {369--381},
  issn     = {0378-3758},
  doi      = {10.1016/j.jspi.2009.06.017},
  fjournal = {Journal of Statistical Planning and Inference},
  mrclass  = {62G08 (62G20)},
  mrnumber = {2558370},
  url      = {http://dx.doi.org/10.1016/j.jspi.2009.06.017},
}

@Article{Huang2002Varying,
  author   = {Huang, Jianhua Z. and Wu, Colin O. and Zhou, Lan},
  title    = {Varying-coefficient models and basis function approximations for the analysis of repeated measurements},
  journal  = {Biometrika},
  year     = {2002},
  volume   = {89},
  number   = {1},
  pages    = {111--128},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/89.1.111},
  fjournal = {Biometrika},
  mrclass  = {62G10 (62G15 62P10)},
  mrnumber = {1888349},
  url      = {http://dx.doi.org/10.1093/biomet/89.1.111},
}

@Article{Wu2002Nonparametric,
  author    = {Colin O. Wu and Kai F. Yu},
  title     = {Nonparametric Varying-Coefficient Models for the Analysis of Longitudinal Data},
  journal   = {International Statistical Review / Revue Internationale de Statistique},
  year      = {2002},
  volume    = {70},
  number    = {3},
  pages     = {373},
  month     = {dec},
  doi       = {10.2307/1403863},
  publisher = {{JSTOR}},
  url       = {https://doi.org/10.2307/1403863},
}

@Article{cSentuerk2005Covariate,
  author     = {\c Sent\"urk, Damla and M\"uller, Hans-Georg},
  title      = {Covariate adjusted correlation analysis via varying coefficient models},
  journal    = {Scand. J. Statist.},
  year       = {2005},
  volume     = {32},
  number     = {3},
  pages      = {365--383},
  issn       = {0303-6898},
  doi        = {10.1111/j.1467-9469.2005.00450.x},
  fjournal   = {Scandinavian Journal of Statistics. Theory and Applications},
  mrclass    = {62H20},
  mrnumber   = {2204625},
  mrreviewer = {Ji\v r\'\i And\v el},
  url        = {http://dx.doi.org/10.1111/j.1467-9469.2005.00450.x},
}

@Article{Zhu2014Spatially,
  author   = {Zhu, Hongtu and Fan, Jianqing and Kong, Linglong},
  title    = {Spatially varying coefficient model for neuroimaging data with jump discontinuities},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2014},
  volume   = {109},
  number   = {507},
  pages    = {1084--1098},
  issn     = {0162-1459},
  doi      = {10.1080/01621459.2014.881742},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62M40 (62H25 62P10)},
  mrnumber = {3265682},
  url      = {http://dx.doi.org/10.1080/01621459.2014.881742},
}

@Article{Zhu2016Bayesian,
  author   = {Zhu, Hongxiao and Strawn, Nate and Dunson, David B.},
  title    = {Bayesian graphical models for multivariate functional data},
  journal  = {J. Mach. Learn. Res.},
  year     = {2016},
  volume   = {17},
  pages    = {Paper No. 204, 27},
  issn     = {1532-4435},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {62H99 (62F15)},
  mrnumber = {3580357},
}

@Article{Qiao2015Functional,
  author    = {Qiao, Xinghao and Guo, Shaojun and James, Gareth M.},
  title     = {Functional {G}raphical {M}odels},
  journal   = {J. Amer. Statist. Assoc.},
  year      = {2019},
  volume    = {114},
  number    = {525},
  pages     = {211--222},
  issn      = {0162-1459},
  doi       = {10.1080/01621459.2017.1390466},
  file      = {:Qiao2015Functional.pdf:PDF},
  fjournal  = {Journal of the American Statistical Association},
  mrclass   = {62H12 (62H25 62H99 62J07)},
  mrnumber  = {3941249},
  timestamp = {2019.05.22},
  url       = {https://doi.org/10.1080/01621459.2017.1390466},
}

@TechReport{Qiao2017Doubly,
  author      = {Qiao, Xinghao and Qian, Cheng and James, Gareth M},
  title       = {Doubly Functional Graphical Models in High Dimensions},
  institution = {Technical report, University of Southern California},
  year        = {2017},
}

@Article{Konecny2015Federated,
  author      = {Jakub {Kone\v{c}n\'{y}} and Brendan McMahan and Daniel Ramage},
  title       = {Federated Optimization:Distributed Optimization Beyond the Datacenter},
  year        = {2015},
  abstract    = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of \nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization.},
  date        = {2015-11-11},
  eprint      = {1511.03575v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1511.03575v1:PDF},
  keywords    = {cs.LG, math.OC},
  owner       = {mkolar},
  timestamp   = {2017.09.14},
}

@InCollection{Maurer2006Rademacher,
  author     = {Maurer, Andreas},
  title      = {The {R}ademacher complexity of linear transformation classes},
  booktitle  = {Learning theory},
  publisher  = {Springer, Berlin},
  year       = {2006},
  volume     = {4005},
  series     = {Lecture Notes in Comput. Sci.},
  pages      = {65--78},
  doi        = {10.1007/11776420_8},
  mrclass    = {62H30 (47B07 47N30 68Q32 68T05)},
  mrnumber   = {2277919},
  mrreviewer = {M. Iosifescu},
  url        = {http://dx.doi.org/10.1007/11776420_8},
}

@Article{Shalev-Shwartz2009Stochastic,
  author    = {Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  title     = {Stochastic Convex Optimization.},
  year      = {2009},
  month     = {01},
  booktitle = {COLT 2009 - The 22nd Conference on Learning Theory},
}

@Article{Nedic2009Distributed,
  author   = {Nedi\'c, Angelia and Ozdaglar, Asuman},
  title    = {Distributed subgradient methods for multi-agent optimization},
  journal  = {IEEE Trans. Automat. Control},
  year     = {2009},
  volume   = {54},
  number   = {1},
  pages    = {48--61},
  issn     = {0018-9286},
  doi      = {10.1109/TAC.2008.2009515},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Automatic Control},
  mrclass  = {90C25 (90C29)},
  mrnumber = {2478070},
  url      = {http://dx.doi.org/10.1109/TAC.2008.2009515},
}

@Article{Yuan2016convergence,
  author   = {Yuan, Kun and Ling, Qing and Yin, Wotao},
  title    = {On the convergence of decentralized gradient descent},
  journal  = {SIAM J. Optim.},
  year     = {2016},
  volume   = {26},
  number   = {3},
  pages    = {1835--1854},
  issn     = {1052-6234},
  doi      = {10.1137/130943170},
  fjournal = {SIAM Journal on Optimization},
  mrclass  = {90C25 (90C30)},
  mrnumber = {3544854},
  url      = {http://dx.doi.org/10.1137/130943170},
}

@Article{NEYMAN1928USE,
  author    = {J. NEYMAN and E. S. PEARSON},
  title     = {{ON} {THE} {USE} {AND} {INTERPRETATION} {OF} {CERTAIN} {TEST} {CRITERIA} {FOR} {PURPOSES} {OF} {STATISTICAL} {INFERENCE}},
  journal   = {Biometrika},
  year      = {1928},
  volume    = {20A},
  number    = {3-4},
  pages     = {263--294},
  doi       = {10.1093/biomet/20a.3-4.263},
  publisher = {Oxford University Press ({OUP})},
  url       = {https://doi.org/10.1093/biomet/20a.3-4.263},
}

@Article{Wald1943Tests,
  author     = {Wald, Abraham},
  title      = {Tests of statistical hypotheses concerning several parameters when the number of observations is large},
  journal    = {Trans. Amer. Math. Soc.},
  year       = {1943},
  volume     = {54},
  pages      = {426--482},
  issn       = {0002-9947},
  doi        = {10.2307/1990256},
  fjournal   = {Transactions of the American Mathematical Society},
  mrclass    = {62.0X},
  mrnumber   = {0012401},
  mrreviewer = {J. Wolfowitz},
  url        = {http://dx.doi.org/10.2307/1990256},
}

@Article{Rao1950Methods,
  author    = {C Radhakrishna Rao},
  title     = {Methods of scoring linkage data giving the simultaneous segregation of three factors},
  journal   = {Heredity},
  year      = {1950},
  volume    = {4},
  number    = {1},
  pages     = {37--59},
  month     = {apr},
  doi       = {10.1038/hdy.1950.3},
  publisher = {Springer Nature},
  url       = {https://doi.org/10.1038/hdy.1950.3},
}

@Article{Aitchison1958Maximum,
  author     = {Aitchison, J. and Silvey, S. D.},
  title      = {Maximum-likelihood estimation of parameters subject to restraints},
  journal    = {Ann. Math. Statist.},
  year       = {1958},
  volume     = {29},
  pages      = {813--828},
  issn       = {0003-4851},
  doi        = {10.1214/aoms/1177706538},
  fjournal   = {Annals of Mathematical Statistics},
  mrclass    = {62.00},
  mrnumber   = {0094873},
  mrreviewer = {R. L. Anderson},
  url        = {http://dx.doi.org/10.1214/aoms/1177706538},
}

@Article{Silvey1959Lagrangian,
  author     = {Silvey, S. D.},
  title      = {The {L}agrangian multiplier test},
  journal    = {Ann. Math. Statist.},
  year       = {1959},
  volume     = {30},
  pages      = {389--407},
  issn       = {0003-4851},
  doi        = {10.1214/aoms/1177706259},
  fjournal   = {Annals of Mathematical Statistics},
  mrclass    = {62.00},
  mrnumber   = {0104307},
  mrreviewer = {D. H. Shaffer},
  url        = {http://dx.doi.org/10.1214/aoms/1177706259},
}

@Article{Chandra1985Comparison,
  author     = {Chandra, Tapas K. and Mukerjee, Rahul},
  title      = {Comparison of the likelihood ratio, {W}ald's and {R}ao's tests},
  journal    = {Sankhy\=a Ser. A},
  year       = {1985},
  volume     = {47},
  number     = {2},
  pages      = {271--284},
  issn       = {0581-572X},
  fjournal   = {Sankhy\=a (Statistics). The Indian Journal of Statistics. Series A},
  mrclass    = {62F05 (62E20)},
  mrnumber   = {844028},
  mrreviewer = {Kumar Joag-Dev},
}

@Article{Chandra1983Comparison,
  author     = {Chandra, Tapas K. and Joshi, S. N.},
  title      = {Comparison of the likelihood ratio, {R}ao's and {W}ald's tests and a conjecture of {C}. {R}. {R}ao},
  journal    = {Sankhy\=a Ser. A},
  year       = {1983},
  volume     = {45},
  number     = {2},
  pages      = {226--246},
  issn       = {0581-572X},
  fjournal   = {Sankhy\=a (Statistics). The Indian Journal of Statistics. Series A},
  mrclass    = {62F03 (62F05)},
  mrnumber   = {748461},
  mrreviewer = {Rasul A. Khan},
}

@Article{Chandra1984optimality,
  author   = {Chandra, Tapas K. and Mukerjee, Rahul},
  title    = {On the optimality of {R}ao's statistic},
  journal  = {Comm. Statist. A---Theory Methods},
  year     = {1984},
  volume   = {13},
  number   = {12},
  pages    = {1507--1515},
  issn     = {0361-0926},
  doi      = {10.1080/03610928408828773},
  fjournal = {Communications in Statistics. A. Theory and Methods},
  mrclass  = {62F05 (62F03)},
  mrnumber = {742516},
  url      = {http://dx.doi.org/10.1080/03610928408828773},
}

@Article{Chandra1988second,
  author   = {Chandra, Tapas K. and Samanta, Tapas},
  title    = {On the second order local comparison between perturbed maximum likelihood estimators and {R}ao's statistic as test statistics},
  journal  = {J. Multivariate Anal.},
  year     = {1988},
  volume   = {25},
  number   = {2},
  pages    = {201--222},
  issn     = {0047-259X},
  doi      = {10.1016/0047-259X(88)90048-6},
  fjournal = {Journal of Multivariate Analysis},
  mrclass  = {62F05 (62E20)},
  mrnumber = {940540},
  url      = {http://dx.doi.org/10.1016/0047-259X(88)90048-6},
}

@Article{Ghosh1991Higher,
  author     = {Ghosh, J. K.},
  title      = {Higher order asymptotics for the likelihood ratio, {R}ao's and {W}ald's tests},
  journal    = {Statist. Probab. Lett.},
  year       = {1991},
  volume     = {12},
  number     = {6},
  pages      = {505--509},
  issn       = {0167-7152},
  doi        = {10.1016/0167-7152(91)90005-C},
  fjournal   = {Statistics \& Probability Letters},
  mrclass    = {62F05 (62F03 62H15)},
  mrnumber   = {1143747},
  mrreviewer = {Zhi-Dong Bai},
  url        = {http://dx.doi.org/10.1016/0167-7152(91)90005-C},
}

@Article{Voorman2014Inference,
  author      = {Arend Voorman and Ali Shojaie and Daniela Witten},
  title       = {Inference in High Dimensions with the Penalized Score Test},
  abstract    = {In recent years, there has been considerable theoretical development regarding variable selection consistency of penalized regression techniques, such as the lasso. However, there has been relatively little work on quantifying the uncertainty in these selection procedures. In this paper, we propose a new method for inference in high dimensions using a score test based on penalized regression. In this test, we perform penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature. This procedure is applied to each feature in turn. Interestingly, when an $\ell_1$ penalty is used, the sparsity pattern of the lasso corresponds exactly to a decision based on the proposed test. Further, when an $\ell_2$ penalty is used, the test corresponds precisely to a score test in a mixed effects model, in which the effects of all but one feature are assumed to be random. We formulate the hypothesis being tested as a compromise between the null hypotheses tested in simple linear regression on each feature and in multiple linear regression on all features, and develop reference distributions for some well-known penalties. We also examine the behavior of the test on real and simulated data.},
  date        = {2014-01-12},
  eprint      = {1401.2678v3},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1401.2678v3:PDF},
  keywords    = {stat.ME, stat.ML},
}

@Article{Begun1983Information,
  author     = {Begun, Janet M. and Hall, W. J. and Huang, Wei-Min and Wellner, Jon A.},
  title      = {Information and asymptotic efficiency in parametric--nonparametric models},
  journal    = {Ann. Statist.},
  year       = {1983},
  volume     = {11},
  number     = {2},
  pages      = {432--452},
  issn       = {0090-5364},
  doi        = {10.1214/aos/1176346151},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62F12 (62G05 62G20)},
  mrnumber   = {696057},
  mrreviewer = {J. A. Melamed},
  url        = {http://dx.doi.org/10.1214/aos/1176346151},
}

@InCollection{Rao2005Score,
  author     = {Rao, C. R.},
  title      = {Score test: historical review and recent developments},
  booktitle  = {Advances in ranking and selection, multiple comparisons, and reliability},
  publisher  = {Birkh\"auser Boston, Boston, MA},
  year       = {2005},
  series     = {Stat. Ind. Technol.},
  pages      = {3--20},
  doi        = {10.1007/0-8176-4422-9_1},
  mrclass    = {62-03 (01A60 62F03 62F05)},
  mrnumber   = {2111500},
  mrreviewer = {Rasul A. Khan},
  url        = {http://dx.doi.org/10.1007/0-8176-4422-9_1},
}

@Article{Murphy2000profile,
  author     = {Murphy, S. A. and van der Vaart, A. W.},
  title      = {On profile likelihood},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {2000},
  volume     = {95},
  number     = {450},
  pages      = {449--485},
  issn       = {0162-1459},
  note       = {With comments and a rejoinder by the authors},
  doi        = {10.2307/2669386},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62N02 (62G05 62G10 62G20)},
  mrnumber   = {1803168},
  mrreviewer = {Per Kragh Andersen},
  url        = {http://dx.doi.org/10.2307/2669386},
}

@InCollection{Vuffray2016Interaction,
  author    = {Vuffray, Marc and Misra, Sidhant and Lokhov, Andrey and Chertkov, Michael},
  title     = {Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {2595--2603},
  file      = {:Vuffray2016Interaction.pdf:PDF},
  url       = {http://papers.nips.cc/paper/6375-interaction-screening-efficient-and-sample-optimal-learning-of-ising-models.pdf},
}

@Article{Power2011Functional,
  author    = {Power, Jonathan D and Cohen, Alexander L and Nelson, Steven M and Wig, Gagan S and Barnes, Kelly Anne and Church, Jessica A and Vogel, Alecia C and Laumann, Timothy O and Miezin, Fran M and Schlaggar, Bradley L and others},
  title     = {Functional network organization of the human brain},
  journal   = {Neuron},
  year      = {2011},
  volume    = {72},
  number    = {4},
  pages     = {665--678},
  publisher = {Elsevier},
}

@Article{Munos2008Finite,
  author  = {Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  title   = {Finite-time bounds for fitted value iteration},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {May},
  pages   = {815--857},
}

@InCollection{Snel2012Multi,
  author    = {Matthijs Snel and Shimon Whiteson},
  title     = {Multi-Task Reinforcement Learning: Shaping and Feature Selection},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  pages     = {237--248},
  doi       = {10.1007/978-3-642-29946-9_24},
}

@InProceedings{Wilson2007Multi,
  author    = {Aaron Wilson and Alan Fern and Soumya Ray and Prasad Tadepalli},
  title     = {Multi-task reinforcement learning},
  booktitle = {Proceedings of the 24th international conference on Machine learning - {ICML} {\textquotesingle}07},
  year      = {2007},
  publisher = {{ACM} Press},
  doi       = {10.1145/1273496.1273624},
}

@Book{Sutton1998Introduction,
  title     = {Introduction to Reinforcement Learning},
  publisher = {MIT Press},
  year      = {1998},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  address   = {Cambridge, MA, USA},
  edition   = {1st},
  isbn      = {0262193981},
}

@InProceedings{Bertsekas1995Neuro,
  author    = {D. P. Bertsekas and J. N. Tsitsiklis},
  title     = {Neuro-dynamic programming: an overview},
  booktitle = {Proceedings of 1995 34th IEEE Conference on Decision and Control},
  year      = {1995},
  volume    = {1},
  pages     = {560-564 vol.1},
  month     = {Dec},
  doi       = {10.1109/CDC.1995.478953},
  issn      = {0191-2216},
  keywords  = {approximation theory;cognitive systems;dynamic programming;neural nets;uncertainty handling;approximation theory;artificial intelligence;cognitive science;neural networks;neuro-dynamic programming;simulation;uncertainty;Artificial intelligence;Artificial neural networks;Control systems;Cost function;Dynamic programming;Equations;Laboratories;Optimal control;State-space methods;Uncertainty},
}

@InCollection{Calandriello2014Sparse,
  author    = {Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},
  title     = {Sparse Multi-Task Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {819--827},
  url       = {http://papers.nips.cc/paper/5247-sparse-multi-task-reinforcement-learning.pdf},
}

@Article{Ernst2005Tree,
  author     = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  title      = {Tree-Based Batch Mode Reinforcement Learning},
  journal    = {J. Mach. Learn. Res.},
  year       = {2005},
  volume     = {6},
  pages      = {503--556},
  month      = dec,
  issn       = {1532-4435},
  acmid      = {1088690},
  issue_date = {12/1/2005},
  numpages   = {54},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=1046920.1088690},
}

@InProceedings{Yu2017Influence,
  author    = {Ming Yu and Varun Gupta and Mladen Kolar},
  title     = {An Influence-Receptivity Model for Topic based Information Cascades},
  booktitle = {2017 {IEEE} International Conference on Data Mining, {ICDM} 2017, New Orleans, LA, USA, November 18-21, 2017},
  year      = {2017},
  editor    = {Vijay Raghavan and Srinivas Aluru and George Karypis and Lucio Miele and Xindong Wu},
  pages     = {1141--1146},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icdm/YuGK17},
  doi       = {10.1109/ICDM.2017.152},
  file      = {online:http\://arxiv.org/pdf/1709.01919v1:PDF},
  url       = {https://doi.org/10.1109/ICDM.2017.152},
}

@Article{Yu2017Multitask,
  author      = {Ming Yu and Addie M. Thompson and Karthikeyan Natesan Ramamurthy and Eunho Yang and Aurlie C. Lozano},
  title       = {Multitask Learning using Task Clustering with Applications to Predictive Modeling and GWAS of Plant Varieties},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {Inferring predictive maps between multiple input and multiple output variables or tasks has innumerable applications in data science. Multi-task learning attempts to learn the maps to several output tasks simultaneously with information sharing between them. We propose a novel multi-task learning framework for sparse linear regression, where a full task hierarchy is automatically inferred from the data, with the assumption that the task parameters follow a hierarchical tree structure. The leaves of the tree are the parameters for individual tasks, and the root is the global model that approximates all the tasks. We apply the proposed approach to develop and evaluate: (a) predictive models of plant traits using large-scale and automated remote sensing data, and (b) GWAS methodologies mapping such derived phenotypes in lieu of hand-measured traits. We demonstrate the superior performance of our approach compared to other methods, as well as the usefulness of discovering hierarchical groupings between tasks. Our results suggest that richer genetic mapping can indeed be obtained from the remote sensing data. In addition, our discovered groupings reveal interesting insights from a plant science perspective.},
  date        = {2017-10-04},
  eprint      = {1710.01788v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1710.01788v1:PDF},
  keywords    = {stat.ML},
}

@Article{Vogelstein2010Fast,
  author    = {J. T. Vogelstein and A. M. Packer and T. A. Machado and T. Sippy and B. Babadi and R. Yuste and L. Paninski},
  title     = {Fast Nonnegative Deconvolution for Spike Train Inference From Population Calcium Imaging},
  journal   = {Journal of Neurophysiology},
  year      = {2010},
  volume    = {104},
  number    = {6},
  pages     = {3691--3704},
  month     = {jun},
  doi       = {10.1152/jn.01073.2009},
  publisher = {American Physiological Society},
}

@Article{Akerboom2012Optimization,
  author    = {J. Akerboom and T.-W. Chen and T. J. Wardill and L. Tian and J. S. Marvin and S. Mutlu and N. C. Calderon and F. Esposti and B. G. Borghuis and X. R. Sun and A. Gordus and M. B. Orger and R. Portugues and F. Engert and J. J. Macklin and A. Filosa and A. Aggarwal and R. A. Kerr and R. Takagi and S. Kracun and E. Shigetomi and B. S. Khakh and H. Baier and L. Lagnado and S. S.- H. Wang and C. I. Bargmann and B. E. Kimmel and V. Jayaraman and K. Svoboda and D. S. Kim and E. R. Schreiter and L. L. Looger},
  title     = {Optimization of a {GCaMP} Calcium Indicator for Neural Activity Imaging},
  journal   = {Journal of Neuroscience},
  year      = {2012},
  volume    = {32},
  number    = {40},
  pages     = {13819--13840},
  month     = {oct},
  doi       = {10.1523/jneurosci.2601-12.2012},
  publisher = {Society for Neuroscience},
}

@Article{Pnevmatikakis2014structured,
  author      = {Eftychios A. Pnevmatikakis and Yuanjun Gao and Daniel Soudry and David Pfau and Clay Lacefield and Kira Poskanzer and Randy Bruno and Rafael Yuste and Liam Paninski},
  title       = {A structured matrix factorization framework for large scale calcium imaging data analysis},
  journal     = {Technical report},
  year        = {2014},
  abstract    = {We present a structured matrix factorization approach to analyzing calcium imaging recordings of large neuronal ensembles. Our goal is to simultaneously identify the locations of the neurons, demix spatially overlapping components, and denoise and deconvolve the spiking activity of each neuron from the slow dynamics of the calcium indicator. The matrix factorization approach relies on the observation that the spatiotemporal fluorescence activity can be expressed as a product of two matrices: a spatial matrix that encodes the location of each neuron in the optical field and a temporal matrix that characterizes the calcium concentration of each neuron over time. We present a simple approach for estimating the dynamics of the calcium indicator as well as the observation noise statistics from the observed data. These parameters are then used to set up the matrix factorization problem in a constrained form that requires no further parameter tuning. We discuss initialization and post-processing techniques that enhance the performance of our method, along with efficient and largely parallelizable algorithms. We apply our method to {\it in vivo} large scale multi-neuronal imaging data and also demonstrate how similar methods can be used for the analysis of {\it in vivo} dendritic imaging data.},
  date        = {2014-09-09},
  eprint      = {1409.2903v1},
  eprintclass = {q-bio.NC},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1409.2903v1:PDF},
  keywords    = {q-bio.NC, q-bio.QM, stat.AP},
}

@Article{Wang2014Nonconvex,
  author      = {Zhaoran Wang and Huanran Lu and Han Liu},
  title       = {Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time},
  journal     = {Technical report},
  year        = {2014},
  abstract    = {Sparse principal component analysis (PCA) involves nonconvex optimization for which the global solution is hard to obtain. To address this issue, one popular approach is convex relaxation. However, such an approach may produce suboptimal estimators due to the relaxation effect. To optimally estimate sparse principal subspaces, we propose a two-stage computational framework named "tighten after relax": Within the 'relax' stage, we approximately solve a convex relaxation of sparse PCA with early stopping to obtain a desired initial estimator; For the 'tighten' stage, we propose a novel algorithm called sparse orthogonal iteration pursuit (SOAP), which iteratively refines the initial estimator by directly solving the underlying nonconvex problem. A key concept of this two-stage framework is the basin of attraction. It represents a local region within which the `tighten' stage has desired computational and statistical guarantees. We prove that, the initial estimator obtained from the 'relax' stage falls into such a region, and hence SOAP geometrically converges to a principal subspace estimator which is minimax-optimal within a certain model class. Unlike most existing sparse PCA estimators, our approach applies to the non-spiked covariance models, and adapts to non-Gaussianity as well as dependent data settings. Moreover, through analyzing the computational complexity of the two stages, we illustrate an interesting phenomenon that larger sample size can reduce the total iteration complexity. Our framework motivates a general paradigm for solving many complex statistical problems which involve nonconvex optimization with provable guarantees.},
  date        = {2014-08-22},
  eprint      = {1408.5352v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1408.5352v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Book{Nesterov2013Introductory,
  title     = {Introductory Lectures on Convex Optimization},
  publisher = {Springer Us},
  year      = {2013},
  author    = {Nesterov, Y.},
  date      = {2013-12-01},
  ean       = {9781441988539},
  url       = {http://www.ebook.de/de/product/25437653/y_nesterov_introductory_lectures_on_convex_optimization.html},
}

@Book{Izenman2008Modern,
  title      = {Modern multivariate statistical techniques},
  publisher  = {Springer, New York},
  year       = {2008},
  author     = {Izenman, Alan Julian},
  series     = {Springer Texts in Statistics},
  isbn       = {978-0-387-78188-4},
  note       = {Regression, classification, and manifold learning},
  mrclass    = {62-01 (62Hxx)},
  mrnumber   = {2445017},
  mrreviewer = {Steen Arne Andersson},
  pages      = {xxvi+731},
  url        = {https://doi.org/10.1007/978-0-387-78189-1},
}

@Article{Candes2009Exact,
  author     = {Cand\`es, Emmanuel J. and Recht, Benjamin},
  title      = {Exact matrix completion via convex optimization},
  journal    = {Found. Comput. Math.},
  year       = {2009},
  volume     = {9},
  number     = {6},
  pages      = {717--772},
  issn       = {1615-3375},
  fjournal   = {Foundations of Computational Mathematics. The Journal of the Society for the Foundations of Computational Mathematics},
  mrclass    = {90C25 (15A83 90C22)},
  mrnumber   = {2565240},
  mrreviewer = {Gunter Semmler},
  url        = {https://doi.org/10.1007/s10208-009-9045-5},
}

@Article{Candes2010power,
  author     = {Cand\`es, Emmanuel J. and Tao, Terence},
  title      = {The power of convex relaxation: near-optimal matrix completion},
  journal    = {IEEE Trans. Inform. Theory},
  year       = {2010},
  volume     = {56},
  number     = {5},
  pages      = {2053--2080},
  issn       = {0018-9448},
  fjournal   = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass    = {15A83 (60B20 90C22 90C46 94A12)},
  mrnumber   = {2723472},
  mrreviewer = {Tian Zhou Xu},
  url        = {https://doi.org/10.1109/TIT.2010.2044061},
}

@Article{Candes2010Matrix,
  author    = {Emmanuel J Candes and Yaniv Plan},
  title     = {Matrix Completion With Noise},
  journal   = {Proceedings of the {IEEE}},
  year      = {2010},
  volume    = {98},
  number    = {6},
  pages     = {925--936},
  month     = {jun},
  doi       = {10.1109/jproc.2009.2035722},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Recht2010Guaranteed,
  author   = {Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A.},
  title    = {Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization},
  journal  = {SIAM Rev.},
  year     = {2010},
  volume   = {52},
  number   = {3},
  pages    = {471--501},
  issn     = {0036-1445},
  fjournal = {SIAM Review},
  mrclass  = {90C25 (15A24)},
  mrnumber = {2680543},
  url      = {https://doi.org/10.1137/070697835},
}

@Article{Lee2010ADMiRA:,
  author   = {Lee, Kiryung and Bresler, Yoram},
  title    = {A{DM}i{RA}: atomic decomposition for minimum rank approximation},
  journal  = {IEEE Trans. Inform. Theory},
  year     = {2010},
  volume   = {56},
  number   = {9},
  pages    = {4402--4416},
  issn     = {0018-9448},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass  = {94A12 (15A83 65F15 94A08)},
  mrnumber = {2807337},
  url      = {https://doi.org/10.1109/TIT.2010.2054251},
}

@InCollection{Jain2010Guaranteed,
  author    = {Jain, Prateek and Raghu Meka and Inderjit S. Dhillon},
  title     = {Guaranteed Rank Minimization via Singular Value Projection},
  booktitle = {Advances in Neural Information Processing Systems 23},
  publisher = {Curran Associates, Inc.},
  year      = {2010},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  pages     = {937--945},
  url       = {http://papers.nips.cc/paper/3904-guaranteed-rank-minimization-via-singular-value-projection.pdf},
}

@Article{Cai2010singular,
  author     = {Cai, Jian-Feng and Cand\`es, Emmanuel J. and Shen, Zuowei},
  title      = {A singular value thresholding algorithm for matrix completion},
  journal    = {SIAM J. Optim.},
  year       = {2010},
  volume     = {20},
  number     = {4},
  pages      = {1956--1982},
  issn       = {1052-6234},
  fjournal   = {SIAM Journal on Optimization},
  mrclass    = {90C25 (15A18 65K05)},
  mrnumber   = {2600248},
  mrreviewer = {Javier F. Pe\~na},
  url        = {https://doi.org/10.1137/080738970},
}

@Article{Recht2011simpler,
  author   = {Recht, Benjamin},
  title    = {A simpler approach to matrix completion},
  journal  = {J. Mach. Learn. Res.},
  year     = {2011},
  volume   = {12},
  pages    = {3413--3430},
  issn     = {1532-4435},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {62H12 (60B20 60E15)},
  mrnumber = {2877360},
}

@Article{Gross2011Recovering,
  author   = {Gross, David},
  title    = {Recovering low-rank matrices from few coefficients in any basis},
  journal  = {IEEE Trans. Inform. Theory},
  year     = {2011},
  volume   = {57},
  number   = {3},
  pages    = {1548--1566},
  issn     = {0018-9448},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass  = {94A12 (81P50)},
  mrnumber = {2815834},
  url      = {https://doi.org/10.1109/TIT.2011.2104999},
}

@Article{Chandrasekaran2011Rank,
  author   = {Chandrasekaran, Venkat and Sanghavi, Sujay and Parrilo, Pablo A. and Willsky, Alan S.},
  title    = {Rank-sparsity incoherence for matrix decomposition},
  journal  = {SIAM J. Optim.},
  year     = {2011},
  volume   = {21},
  number   = {2},
  pages    = {572--596},
  issn     = {1052-6234},
  fjournal = {SIAM Journal on Optimization},
  mrclass  = {90C25 (15A23 90C22)},
  mrnumber = {2817479},
  url      = {https://doi.org/10.1137/090761793},
}

@Article{Hsu2011Robust,
  author     = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
  title      = {Robust matrix decomposition with sparse corruptions},
  journal    = {IEEE Trans. Inform. Theory},
  year       = {2011},
  volume     = {57},
  number     = {11},
  pages      = {7221--7234},
  issn       = {0018-9448},
  fjournal   = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass    = {15A23},
  mrnumber   = {2883652},
  mrreviewer = {Miroslav T\ocirc uma},
  url        = {https://doi.org/10.1109/TIT.2011.2158250},
}

@Article{Rohde2011Estimation,
  author     = {Rohde, Angelika and Tsybakov, Alexandre B.},
  title      = {Estimation of high-dimensional low-rank matrices},
  journal    = {Ann. Statist.},
  year       = {2011},
  volume     = {39},
  number     = {2},
  pages      = {887--930},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G05 (62F10)},
  mrnumber   = {2816342},
  mrreviewer = {Ou Zhao},
  url        = {https://doi.org/10.1214/10-AOS860},
}

@InProceedings{Xiang2012Optimal,
  author    = {Shuo Xiang and Yunzhang Zhu and Xiaotong Shen and Jieping Ye},
  title     = {Optimal exact least squares rank minimization},
  booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} {\textquotesingle}12},
  year      = {2012},
  publisher = {{ACM} Press},
  doi       = {10.1145/2339530.2339609},
}

@Article{Negahban2012Restricted,
  author   = {Negahban, Sahand and Wainwright, Martin J.},
  title    = {Restricted strong convexity and weighted matrix completion: optimal bounds with noise},
  journal  = {J. Mach. Learn. Res.},
  year     = {2012},
  volume   = {13},
  pages    = {1665--1697},
  issn     = {1532-4435},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {62H12 (15A83 60B20 60E15 62G20)},
  mrnumber = {2930649},
}

@Article{Recht2013Parallel,
  author   = {Recht, Benjamin and R\'e, Christopher},
  title    = {Parallel stochastic gradient algorithms for large-scale matrix completion},
  journal  = {Math. Program. Comput.},
  year     = {2013},
  volume   = {5},
  number   = {2},
  pages    = {201--226},
  issn     = {1867-2949},
  fjournal = {Mathematical Programming Computation},
  mrclass  = {90C06 (90C15 90C25)},
  mrnumber = {3069879},
  url      = {https://doi.org/10.1007/s12532-013-0053-8},
}

@Article{Chen2015Incoherence,
  author   = {Chen, Yudong},
  title    = {Incoherence-optimal matrix completion},
  journal  = {IEEE Trans. Inform. Theory},
  year     = {2015},
  volume   = {61},
  number   = {5},
  pages    = {2909--2923},
  issn     = {0018-9448},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass  = {68Q25 (15A83 65F99 94A15)},
  mrnumber = {3342311},
  url      = {https://doi.org/10.1109/TIT.2015.2415195},
}

@InProceedings{Chen2014Coherent,
  author    = {Yudong Chen and Srinadh Bhojanapalli and Sujay Sanghavi and Rachel Ward},
  title     = {Coherent Matrix Completion},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year      = {2014},
  editor    = {Eric P. Xing and Tony Jebara},
  volume    = {32},
  number    = {1},
  series    = {Proceedings of Machine Learning Research},
  pages     = {674--682},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  abstract  = {Matrix completion concerns the recovery of a low-rank matrix from a subset of its revealed entries, and nuclear norm minimization has emerged as an effective surrogate for this combinatorial problem.  Here, we show that nuclear norm minimization can recover an arbitrary n \times n matrix of rank r from O(nr log^2(n)) revealed entries, provided that revealed entries are drawn proportionally to the local row and column coherences (closely related to leverage scores) of the underlying matrix.  Our results are order-optimal up to logarithmic factors, and extend existing results for nuclear norm minimization which require strong incoherence conditions on the types of matrices that can be recovered, due to assumed uniformly distributed revealed entries.  We further provide extensive numerical evidence that a proposed two-phase sampling algorithm can perform nearly as well as local-coherence sampling and without requiring a priori knowledge of the matrix coherence structure.  Finally, we apply our results to quantify how weighted nuclear norm minimization can improve on unweighted minimization given an arbitrary set of sampled entries.},
  file      = {chenc14.pdf:http\://proceedings.mlr.press/v32/chenc14.pdf:PDF},
  url       = {http://proceedings.mlr.press/v32/chenc14.html},
}

@Article{Chen2013Low,
  author    = {Yudong Chen and Ali Jalali and Sujay Sanghavi and Constantine Caramanis},
  title     = {Low-Rank Matrix Recovery From Errors and Erasures},
  journal   = {{IEEE} Transactions on Information Theory},
  year      = {2013},
  volume    = {59},
  number    = {7},
  pages     = {4324--4337},
  month     = {jul},
  doi       = {10.1109/tit.2013.2249572},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InCollection{Jain2013Low,
  author    = {Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  title     = {Low-rank matrix completion using alternating minimization (extended abstract)},
  booktitle = {S{TOC}'13---{P}roceedings of the 2013 {ACM} {S}ymposium on {T}heory of {C}omputing},
  publisher = {ACM, New York},
  year      = {2013},
  pages     = {665--674},
  file      = {:Jain2013Low.pdf:PDF},
  mrclass   = {68Q25 (15A83 65F99)},
  mrnumber  = {3210828},
  url       = {https://doi.org/10.1145/2488608.2488693},
}

@InCollection{Hardt2014Understanding,
  author    = {Hardt, Moritz},
  title     = {Understanding alternating minimization for matrix completion},
  booktitle = {55th {A}nnual {IEEE} {S}ymposium on {F}oundations of {C}omputer {S}cience---{FOCS} 2014},
  publisher = {IEEE Computer Soc., Los Alamitos, CA},
  year      = {2014},
  pages     = {651--660},
  mrclass   = {68Q17 (15A83)},
  mrnumber  = {3344916},
  url       = {https://doi.org/10.1109/FOCS.2014.75},
}

@Proceedings{DBLP:conf/colt/2014,
  title     = {Proceedings of The 27th Conference on Learning Theory, {COLT} 2014, Barcelona, Spain, June 13-15, 2014},
  year      = {2014},
  editor    = {Maria{-}Florina Balcan and Vitaly Feldman and Csaba Szepesv{\'{a}}ri},
  volume    = {35},
  series    = {{JMLR} Workshop and Conference Proceedings},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/colt/2014},
  timestamp = {Tue, 12 Jul 2016 21:51:13 +0200},
  url       = {http://jmlr.org/proceedings/papers/v35/},
}

@Article{Sun2016Guaranteed,
  author   = {Sun, Ruoyu and Luo, Zhi-Quan},
  title    = {Guaranteed matrix completion via non-convex factorization},
  journal  = {IEEE Trans. Inform. Theory},
  year     = {2016},
  volume   = {62},
  number   = {11},
  pages    = {6535--6579},
  issn     = {0018-9448},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass  = {94A12},
  mrnumber = {3565131},
  url      = {https://doi.org/10.1109/TIT.2016.2598574},
}

@Article{Hastie2015Matrix,
  author   = {Hastie, Trevor and Mazumder, Rahul and Lee, Jason D. and Zadeh, Reza},
  title    = {Matrix completion and low-rank {SVD} via fast alternating least squares},
  journal  = {J. Mach. Learn. Res.},
  year     = {2015},
  volume   = {16},
  pages    = {3367--3402},
  issn     = {1532-4435},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {15A83 (62H12 65F15 68W30)},
  mrnumber = {3450542},
}

@Article{Cai2015ROP:,
  author     = {Cai, T. Tony and Zhang, Anru},
  title      = {R{OP}: matrix recovery via rank-one projections},
  journal    = {Ann. Statist.},
  year       = {2015},
  volume     = {43},
  number     = {1},
  pages      = {102--138},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62C20)},
  mrnumber   = {3285602},
  mrreviewer = {Pierre Alquier},
  url        = {https://doi.org/10.1214/14-AOS1267},
}

@Article{Yan2015Simultaneous,
  author   = {Yan, Qi and Ye, Jieping and Shen, Xiaotong},
  title    = {Simultaneous pursuit of sparseness and rank structures for matrix decomposition},
  journal  = {J. Mach. Learn. Res.},
  year     = {2015},
  volume   = {16},
  pages    = {47--75},
  issn     = {1532-4435},
  file     = {:Yan2015Simultaneous.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {15A23 (62H35 62J05 62M10 90C26)},
  mrnumber = {3317229},
}

@Article{Zhu2016Personalized,
  author   = {Zhu, Yunzhang and Shen, Xiaotong and Ye, Changqing},
  title    = {Personalized prediction and sparsity pursuit in latent factor models},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2016},
  volume   = {111},
  number   = {513},
  pages    = {241--252},
  issn     = {0162-1459},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62M20 (62H25 62H30 62P25)},
  mrnumber = {3494656},
  url      = {https://doi.org/10.1080/01621459.2014.999158},
}

@Article{Wang2015Orthogonal,
  author   = {Wang, Zheng and Lai, Ming-Jun and Lu, Zhaosong and Fan, Wei and Davulcu, Hasan and Ye, Jieping},
  title    = {Orthogonal rank-one matrix pursuit for low rank matrix completion},
  journal  = {SIAM J. Sci. Comput.},
  year     = {2015},
  volume   = {37},
  number   = {1},
  pages    = {A488--A514},
  issn     = {1064-8275},
  file     = {:Wang2015Orthogonal.pdf:PDF},
  fjournal = {SIAM Journal on Scientific Computing},
  mrclass  = {15A83 (68W40 90C06 90C25)},
  mrnumber = {3313832},
  url      = {https://doi.org/10.1137/130934271},
}

@Proceedings{DBLP:conf/colt/2015,
  title     = {Proceedings of The 28th Conference on Learning Theory, {COLT} 2015, Paris, France, July 3-6, 2015},
  year      = {2015},
  editor    = {Peter Gr{\"{u}}nwald and Elad Hazan and Satyen Kale},
  volume    = {40},
  series    = {{JMLR} Workshop and Conference Proceedings},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/colt/2015},
  timestamp = {Tue, 12 Jul 2016 21:51:13 +0200},
  url       = {http://jmlr.org/proceedings/papers/v40/},
}

@Proceedings{DBLP:conf/icml/2010,
  title     = {Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel},
  year      = {2010},
  editor    = {Johannes F{\"{u}}rnkranz and Thorsten Joachims},
  publisher = {Omnipress},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.org/rec/bib/conf/icml/2010},
  timestamp = {Fri, 12 Jun 2015 19:15:11 +0200},
}

@InProceedings{Fazel2001rank,
  author    = {M. Fazel and H. Hindi and S.P. Boyd},
  title     = {A rank minimization heuristic with application to minimum order system approximation},
  booktitle = {Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)},
  year      = {2001},
  publisher = {{IEEE}},
  doi       = {10.1109/acc.2001.945730},
  file      = {:/home/mkolar/GoogleDriveBooth/JabRefDocs/Fazel2001rank.pdf:PDF},
  owner     = {mkolar},
  timestamp = {2017.11.09},
}

@InProceedings{Hsieh2014Nuclear,
  author    = {Cho-Jui Hsieh and Peder Olsen},
  title     = {Nuclear Norm Minimization via Active Subspace Selection},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year      = {2014},
  editor    = {Eric P. Xing and Tony Jebara},
  volume    = {32},
  number    = {1},
  series    = {Proceedings of Machine Learning Research},
  pages     = {575--583},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  abstract  = {We describe a novel approach to optimizing matrix problems involving nuclear norm regularization and apply it to the matrix completion problem. We combine methods from non-smooth and smooth optimization. At each step we use the proximal gradient to select an active subspace. We then find a smooth, convex relaxation of the smaller subspace problems and solve these using second order methods. We apply our methods to matrix completion problems including Netflix dataset, and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers. Also, this is the first paper to scale nuclear norm solvers to the Yahoo-Music dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like Alternating Least Squares (ALS).},
  file      = {:Hsieh2014Nuclear.pdf:PDF},
  url       = {http://proceedings.mlr.press/v32/hsiehb14.html},
}

@Article{Koren2009Matrix,
  author    = {Yehuda Koren and Robert Bell and Chris Volinsky},
  title     = {Matrix Factorization Techniques for Recommender Systems},
  journal   = {Computer},
  year      = {2009},
  volume    = {42},
  number    = {8},
  pages     = {30--37},
  month     = {aug},
  doi       = {10.1109/mc.2009.263},
  owner     = {mkolar},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2017.11.09},
}

@Article{Ha2017Alternating,
  author      = {Wooseok Ha and Rina Foygel Barber},
  title       = {Alternating minimization and alternating descent over nonconvex sets},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {We analyze the performance of alternating minimization for loss functions optimized over two variables, where each variable may be restricted to lie in some potentially nonconvex constraint set. This type of setting arises naturally in high-dimensional statistics and signal processing, where the variables often reflect different structures or components within the signals being considered. Our analysis depends strongly on the notion of local concavity coefficients, which have been recently proposed in Barber and Ha (2017) to measure and quantify the concavity of a general nonconvex set. Our results further reveal important distinctions between alternating and non-alternating methods. Since computing the alternating minimization steps may not be tractable for some problems, we also consider an inexact version of the algorithm and provide a set of sufficient conditions to ensure fast convergence of the inexact algorithms. We demonstrate our framework on several examples, including low rank + sparse decomposition and multitask regression, and provide numerical experiments to validate our theoretical results.},
  date        = {2017-09-13},
  eprint      = {1709.04451v2},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1709.04451v2:PDF},
  keywords    = {math.OC, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.11.09},
}

@Article{Barber2017Gradient,
  author      = {Rina Foygel Barber and Wooseok Ha},
  title       = {Gradient descent with nonconvex constraints: local concavity determines convergence},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {Many problems in high-dimensional statistics and optimization involve minimization over nonconvex constraints-for instance, a rank constraint for a matrix estimation problem-but little is known about the theoretical properties of such optimization problems for a general nonconvex constraint set. In this paper we study the interplay between the geometric properties of the constraint set and the convergence behavior of gradient descent for minimization over this set. We develop the notion of local concavity coefficients of the constraint set, measuring the extent to which convexity is violated, which govern the behavior of projected gradient descent over this set. We demonstrate the versatility of these concavity coefficients by computing them for a range of problems in low-rank estimation, sparse estimation, and other examples. Through our understanding of the role of these geometric properties in optimization, we then provide a convergence analysis when projections are calculated only approximately, leading to a more efficient method for projected gradient descent in low-rank estimation problems.},
  date        = {2017-03-22},
  eprint      = {1703.07755v3},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1703.07755v3:PDF},
  keywords    = {math.OC},
  owner       = {mkolar},
  timestamp   = {2017.11.09},
}

@Article{Takacs2007Major,
  author    = {G{\'{a}}bor Tak{\'{a}}cs and Istv{\'{a}}n Pil{\'{a}}szy and Botty{\'{a}}n N{\'{e}}meth and Domonkos Tikk},
  title     = {Major components of the gravity recommendation system},
  journal   = {{ACM} {SIGKDD} Explorations Newsletter},
  year      = {2007},
  volume    = {9},
  number    = {2},
  pages     = {80},
  month     = {dec},
  doi       = {10.1145/1345448.1345466},
  owner     = {mkolar},
  publisher = {Association for Computing Machinery ({ACM})},
  timestamp = {2017.11.09},
}

@InProceedings{Gemulla2011Large,
  author    = {Rainer Gemulla and Erik Nijkamp and Peter J. Haas and Yannis Sismanis},
  title     = {Large-scale matrix factorization with distributed stochastic gradient descent},
  booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} {\textquotesingle}11},
  year      = {2011},
  publisher = {{ACM} Press},
  doi       = {10.1145/2020408.2020426},
  owner     = {mkolar},
  timestamp = {2017.11.09},
}

@InProceedings{Zhuang2013fast,
  author    = {Yong Zhuang and Wei-Sheng Chin and Yu-Chin Juan and Chih-Jen Lin},
  title     = {A fast parallel {SGD} for matrix factorization in shared memory systems},
  booktitle = {Proceedings of the 7th {ACM} conference on Recommender systems - {RecSys} {\textquotesingle}13},
  year      = {2013},
  publisher = {{ACM} Press},
  doi       = {10.1145/2507157.2507164},
  owner     = {mkolar},
  timestamp = {2017.11.09},
}

@InCollection{Zheng2015Convergent,
  author    = {Qinqing Zheng and } # jlafferty,
  title     = {A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {109--117},
  url       = {http://papers.nips.cc/paper/5830-a-convergent-gradient-descent-algorithm-for-rank-minimization-and-semidefinite-programming-from-random-linear-measurements.pdf},
}

@InProceedings{Bhojanapalli2016Dropping,
  author    = {Srinadh Bhojanapalli and Anastasios Kyrillidis and Sujay Sanghavi},
  title     = {Dropping Convexity for Faster Semi-definite Optimization},
  booktitle = {29th Annual Conference on Learning Theory},
  year      = {2016},
  editor    = {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume    = {49},
  series    = {Proceedings of Machine Learning Research},
  pages     = {530--582},
  address   = {Columbia University, New York, New York, USA},
  month     = {23--26 Jun},
  publisher = {PMLR},
  abstract  = {We study the minimization of a convex function f(X) over the set of n \times n positive semi-definite matrices, but when the problem is recast as \min_U g(U) :=  f(UU^), with U \mathbbR^n \times r and r n. We study the performance of gradient descent on gwhich we refer to as Factored Gradient Descent (\textscFgd)under standard assumptions on the \em original function f. We provide a rule for selecting the step size and, with this choice, show that the \emphlocal convergence rate of \textscFgd mirrors that of standard gradient descent on the original f: \emphi.e., after k steps, the error is O(1/k) for smooth f, and exponentially small in k when f is (restricted) strongly convex. In addition, we provide a procedure to initialize \textscFgd for (restricted) strongly convex objectives and when one only has access to f via a first-order oracle; for several problem instances, such proper initialization leads to \emphglobal convergence guarantees. \textscFgd and similar procedures are widely used in practice for problems that can be posed as matrix factorization. To the best of our knowledge, this is the first paper to provide precise convergence rate guarantees for general convex functions under standard convex assumptions.},
  file      = {bhojanapalli16.pdf:http\://proceedings.mlr.press/v49/bhojanapalli16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v49/bhojanapalli16.html},
}

@InProceedings{Tu2016Low,
  author    = {Stephen Tu and Ross Boczar and Max Simchowitz and Mahdi Soltanolkotabi and Ben Recht},
  title     = {Low-rank Solutions of Linear Matrix Equations via Procrustes Flow},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {964--973},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {In this paper we study the problem of recovering a low-rank matrix from linear measurements. Our algorithm, which we call Procrustes Flow, starts from an initial estimate obtained by a thresholding scheme followed by gradient descent on a non-convex objective. We show that as long as the measurements obey a standard restricted isometry property, our algorithm converges to the unknown matrix at a geometric rate. In the case of Gaussian measurements, such convergence occurs for a n1 \times n2 matrix of rank r when the number of measurements exceeds a constant times (n1 + n2)r.},
  file      = {tu16.pdf:http\://proceedings.mlr.press/v48/tu16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/tu16.html},
}

@Article{Chen2015Fast,
  author      = {Yudong Chen and Martin J. Wainwright},
  title       = {Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees},
  journal     = {Technical report},
  year        = {2015},
  abstract    = {Optimization problems with rank constraints arise in many applications, including matrix regression, structured PCA, matrix completion and matrix decomposition problems. An attractive heuristic for solving such problems is to factorize the low-rank matrix, and to run projected gradient descent on the nonconvex factorized optimization problem. The goal of this problem is to provide a general theoretical framework for understanding when such methods work well, and to characterize the nature of the resulting fixed point. We provide a simple set of conditions under which projected gradient descent, when given a suitable initialization, converges geometrically to a statistically useful solution. Our results are applicable even when the initial solution is outside any region of local convexity, and even when the problem is globally concave. Working in a non-asymptotic framework, we show that our conditions are satisfied for a wide range of concrete models, including matrix regression, structured PCA, matrix completion with real and quantized observations, matrix decomposition, and graph clustering problems. Simulation results show excellent agreement with the theoretical predictions.},
  date        = {2015-09-10},
  eprint      = {1509.03025v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1509.03025v1:PDF;:Chen2015Fast.pdf:PDF},
  keywords    = {math.ST, cs.LG, stat.ML, stat.TH},
  owner       = {mkolar},
  timestamp   = {2017.11.09},
}

@Article{Davenport2016Overview,
  author    = {Mark A. Davenport and Justin Romberg},
  title     = {An Overview of Low-Rank Matrix Recovery From Incomplete Observations},
  journal   = {{IEEE} Journal of Selected Topics in Signal Processing},
  year      = {2016},
  volume    = {10},
  number    = {4},
  pages     = {608--622},
  month     = {jun},
  doi       = {10.1109/jstsp.2016.2539100},
  file      = {:Davenport2016Overview.pdf:PDF},
  owner     = {mkolar},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2017.11.09},
}

@Article{Aaronson2007learnability,
  author     = {Aaronson, Scott},
  title      = {The learnability of quantum states},
  journal    = {Proc. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci.},
  year       = {2007},
  volume     = {463},
  number     = {2088},
  pages      = {3089--3114},
  issn       = {1364-5021},
  fjournal   = {Proceedings of The Royal Society of London. Series A. Mathematical, Physical and Engineering Sciences},
  mrclass    = {81P68},
  mrnumber   = {2386653},
  mrreviewer = {Paul B. Slater},
  url        = {https://doi.org/10.1098/rspa.2007.0113},
}

@Article{Liu2009Interior,
  author     = {Liu, Zhang and Vandenberghe, Lieven},
  title      = {Interior-point method for nuclear norm approximation with application to system identification},
  journal    = {SIAM J. Matrix Anal. Appl.},
  year       = {2009},
  volume     = {31},
  number     = {3},
  pages      = {1235--1256},
  issn       = {0895-4798},
  fjournal   = {SIAM Journal on Matrix Analysis and Applications},
  mrclass    = {90C22 (90C51 93B30)},
  mrnumber   = {2558821},
  mrreviewer = {Warren L. Hare},
  url        = {https://doi.org/10.1137/090755436},
}

@InCollection{Srebro2005Maximum,
  author    = {Nathan Srebro and Jason Rennie and Tommi S. Jaakkola},
  title     = {Maximum-Margin Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 17},
  publisher = {MIT Press},
  year      = {2005},
  editor    = {L. K. Saul and Y. Weiss and L. Bottou},
  pages     = {1329--1336},
  url       = {http://papers.nips.cc/paper/2655-maximum-margin-matrix-factorization.pdf},
}

@InProceedings{Fazel2004Rank,
  author    = {M. Fazel and H. Hindi and S. Boyd},
  title     = {Rank minimization and applications in system theory},
  booktitle = {Proceedings of the 2004 American Control Conference},
  year      = {2004},
  volume    = {4},
  pages     = {3273-3278 vol.4},
  month     = {June},
  file      = {:Fazel2004Rank.pdf:PDF},
  issn      = {0743-1619},
  keywords  = {computational complexity;convex programming;covariance matrices;minimisation;system theory;NP-hard problem;convex optimization;convex set;heuristic methods;matrix rank minimization problem},
}

@InCollection{Bhojanapalli2016Global,
  author    = {Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  title     = {Global Optimality of Local Search for Low Rank Matrix Recovery},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {3873--3881},
  url       = {http://papers.nips.cc/paper/6271-global-optimality-of-local-search-for-low-rank-matrix-recovery.pdf},
}

@InProceedings{Harchaoui2012Large,
  author    = {Z. Harchaoui and M. Douze and M. Paulin and M. Dudik and J. Malick},
  title     = {Large-scale image classification with trace-norm regularization},
  booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
  year      = {2012},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2012.6248078},
  owner     = {mkolar},
  timestamp = {2017.11.10},
}

@Article{Burer2003nonlinear,
  author     = {Burer, Samuel and Monteiro, Renato D. C.},
  title      = {A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
  journal    = {Math. Program.},
  year       = {2003},
  volume     = {95},
  number     = {2, Ser. B},
  pages      = {329--357},
  issn       = {0025-5610},
  note       = {Computational semidefinite and second order cone programming: the state of the art},
  fjournal   = {Mathematical Programming. A Publication of the Mathematical Programming Society},
  mrclass    = {90C22 (90C30)},
  mrnumber   = {1976484},
  mrreviewer = {Zheng Hai Huang},
  url        = {https://doi.org/10.1007/s10107-002-0352-8},
}

@Article{Burer2005Local,
  author   = {Burer, Samuel and Monteiro, Renato D. C.},
  title    = {Local minima and convergence in low-rank semidefinite programming},
  journal  = {Math. Program.},
  year     = {2005},
  volume   = {103},
  number   = {3, Ser. A},
  pages    = {427--444},
  issn     = {0025-5610},
  fjournal = {Mathematical Programming. A Publication of the Mathematical Programming Society},
  mrclass  = {90C22 (90C27)},
  mrnumber = {2166543},
  url      = {https://doi.org/10.1007/s10107-004-0564-1},
}

@Article{Zhu2017Globala,
  author      = {Zhihui Zhu and Qiuwei Li and Gongguo Tang and Michael B. Wakin},
  title       = {Global Optimality in Low-rank Matrix Optimization},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {This paper considers the minimization of a general objective function $f(X)$ over the set of non-square $n\times m$ matrices where the optimal solution $X^\star$ is low-rank. To reduce the computational burden, we factorize the variable $X$ into a product of two smaller matrices and optimize over these two matrices instead of $X$. Despite the resulting nonconvexity, recent studies in matrix completion and sensing have shown that the factored problem has no spurious local minima and obeys the so-called strict saddle property (the function has a directional negative curvature at all critical points but local minima). We analyze the global geometry for a general and yet well-conditioned objective function $f(X)$ whose restricted strong convexity and restricted strong smoothness constants are comparable. In particular, we show that the reformulated objective function has no spurious local minima and obeys the strict saddle property. These geometric properties implies that a number of iterative optimization algorithms (such as gradient descent) can provably solve the factored problem with global convergence.},
  date        = {2017-02-25},
  eprint      = {1702.07945v2},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1702.07945v2:PDF},
  keywords    = {cs.IT, math.IT, math.OC},
  owner       = {mkolar},
  timestamp   = {2017.11.10},
}

@Article{Li2016Nonconvex,
  author      = {Xingguo Li and Tuo Zhao and Raman Arora and Han Liu and Jarvis Haupt},
  title       = {Nonconvex Sparse Learning via Stochastic Optimization with Progressive Variance Reduction},
  journal     = {Technical report},
  year        = {2016},
  abstract    = {We propose a stochastic variance reduced optimization algorithm for solving sparse learning problems with cardinality constraints. Sufficient conditions are provided, under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. We further extend the proposed algorithm to an asynchronous parallel variant with a near linear speedup. Numerical experiments demonstrate the efficiency of our algorithm in terms of both parameter estimation and computational performance.},
  date        = {2016-05-09},
  eprint      = {1605.02711v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1605.02711v4:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.11.10},
}

@Article{Amini2009High,
  author     = {Amini, Arash A. and Wainwright, Martin J.},
  title      = {High-dimensional analysis of semidefinite relaxations for sparse principal components},
  journal    = {Ann. Statist.},
  year       = {2009},
  volume     = {37},
  number     = {5B},
  pages      = {2877--2921},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H25 (62F12)},
  mrnumber   = {2541450},
  mrreviewer = {Pranesh Kumar},
  url        = {https://doi.org/10.1214/08-AOS664},
}

@Article{Berthet2013Optimal,
  author   = {Berthet, Quentin and Rigollet, Philippe},
  title    = {Optimal detection of sparse principal components in high dimension},
  journal  = {Ann. Statist.},
  year     = {2013},
  volume   = {41},
  number   = {4},
  pages    = {1780--1815},
  issn     = {0090-5364},
  fjournal = {The Annals of Statistics},
  mrclass  = {62H25 (60B20)},
  mrnumber = {3127849},
  url      = {https://doi.org/10.1214/13-AOS1127},
}

@Article{Birnbaum2013Minimax,
  author   = {Birnbaum, Aharon and Johnstone, Iain M. and Nadler, Boaz and Paul, Debashis},
  title    = {Minimax bounds for sparse {PCA} with noisy high-dimensional data},
  journal  = {Ann. Statist.},
  year     = {2013},
  volume   = {41},
  number   = {3},
  pages    = {1055--1084},
  issn     = {0090-5364},
  fjournal = {The Annals of Statistics},
  mrclass  = {62G20 (62C20 62H25)},
  mrnumber = {3113803},
  url      = {https://doi.org/10.1214/12-AOS1014},
}

@Article{Cai2013Sparse,
  author     = {Cai, T. Tony and Ma, Zongming and Wu, Yihong},
  title      = {Sparse {PCA}: optimal rates and adaptive estimation},
  journal    = {Ann. Statist.},
  year       = {2013},
  volume     = {41},
  number     = {6},
  pages      = {3074--3110},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62C20 62H25)},
  mrnumber   = {3161458},
  mrreviewer = {Steven (Shuangge) Ma},
  url        = {https://doi.org/10.1214/13-AOS1178},
}

@Article{Vu2013Minimax,
  author   = {Vu, Vincent Q. and Lei, Jing},
  title    = {Minimax sparse principal subspace estimation in high dimensions},
  journal  = {Ann. Statist.},
  year     = {2013},
  volume   = {41},
  number   = {6},
  pages    = {2905--2947},
  issn     = {0090-5364},
  fjournal = {The Annals of Statistics},
  mrclass  = {62H25 (62C20 62H12)},
  mrnumber = {3161452},
  url      = {https://doi.org/10.1214/13-AOS1151},
}

@Article{Uematsu2017SOFAR:,
  author      = {Yoshimasa Uematsu and Yingying Fan and Kun Chen and Jinchi Lv and Wei Lin},
  title       = {SOFAR: large-scale association network learning},
  journal     = {Technical report},
  year        = {2017},
  abstract    = {Many modern big data applications feature large scale in both numbers of responses and predictors. Better statistical efficiency and scientific insights can be enabled by understanding the large-scale response-predictor association network structures via layers of sparse latent factors ranked by importance. Yet sparsity and orthogonality have been two largely incompatible goals. To accommodate both features, in this paper we suggest the method of sparse orthogonal factor regression (SOFAR) via the sparse singular value decomposition with orthogonality constrained optimization to learn the underlying association networks, with broad applications to both unsupervised and supervised learning tasks such as biclustering with sparse singular value decomposition, sparse principal component analysis, sparse factor analysis, and spare vector autoregression analysis. Exploiting the framework of convexity-assisted nonconvex optimization, we derive nonasymptotic error bounds for the suggested procedure characterizing the theoretical advantages. The statistical guarantees are powered by an efficient SOFAR algorithm with convergence property. Both computational and theoretical advantages of our procedure are demonstrated with several simulation and real data examples.},
  date        = {2017-04-26},
  eprint      = {1704.08349v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Uematsu2017SOFAR\:.pdf:PDF},
  keywords    = {stat.ME, stat.ML},
  owner       = {mkolar},
  timestamp   = {2017.11.10},
}

@Article{Lee2010Biclustering,
  author    = {Mihee Lee and Haipeng Shen and Jianhua Z. Huang and J. S. Marron},
  title     = {Biclustering via Sparse Singular Value Decomposition},
  journal   = {Biometrics},
  year      = {2010},
  volume    = {66},
  number    = {4},
  pages     = {1087--1095},
  month     = {feb},
  doi       = {10.1111/j.1541-0420.2010.01392.x},
  file      = {:Lee2010Biclustering.pdf:PDF},
  owner     = {mkolar},
  publisher = {Wiley-Blackwell},
  timestamp = {2017.11.10},
}

@Article{Bahadori2016Scalable,
  author      = {Mohammad Taha Bahadori and Zemin Zheng and Yan Liu and Jinchi Lv},
  title       = {Scalable Interpretable Multi-Response Regression via SEED},
  journal     = {Technical report},
  year        = {2016},
  abstract    = {Sparse reduced-rank regression is an important tool to uncover meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-$r$ singular value decomposition to find the optimal low-rank and sparse matrix by solving a sparse generalized eigenvalue problem. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.},
  date        = {2016-08-12},
  eprint      = {1608.03686v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1608.03686v1:PDF;:Bahadori2016Scalable.pdf:PDF},
  keywords    = {stat.ME},
  owner       = {mkolar},
  timestamp   = {2017.11.10},
}

@Article{Yang2014Sparse,
  author   = {Yang, Dan and Ma, Zongming and Buja, Andreas},
  title    = {A sparse singular value decomposition method for high-dimensional data},
  journal  = {J. Comput. Graph. Statist.},
  year     = {2014},
  volume   = {23},
  number   = {4},
  pages    = {923--942},
  issn     = {1061-8600},
  file     = {:Yang2014Sparse.pdf:PDF},
  fjournal = {Journal of Computational and Graphical Statistics},
  mrclass  = {62G05 (15A23 62H25 65F15)},
  mrnumber = {3270704},
  url      = {https://doi.org/10.1080/10618600.2013.858632},
}

@TechReport{Mei2012Encoding,
  author      = {Mei, S and Cao, B and Sun, J},
  title       = {Encoding low-rank and sparse structures simultaneously in multi-task learning. TechReport},
  institution = {Microsoft Technical Report},
  year        = {2012},
  groups      = {[mkolar:]},
  owner       = {mkolar},
  timestamp   = {2017.11.10},
}

@Article{Lian2015Parametric,
  author   = {Lian, Heng and Feng, Sanying and Zhao, Kaifeng},
  title    = {Parametric and semiparametric reduced-rank regression with flexible sparsity},
  journal  = {J. Multivariate Anal.},
  year     = {2015},
  volume   = {136},
  pages    = {163--174},
  issn     = {0047-259X},
  file     = {:Lian2015Parametric.pdf:PDF},
  fjournal = {Journal of Multivariate Analysis},
  mrclass  = {62J05 (62F07 62G20 62J07 62P10)},
  mrnumber = {3321486},
  url      = {https://doi.org/10.1016/j.jmva.2015.01.013},
}

@Article{Chen2012Reduced,
  author     = {Chen, Kun and Chan, Kung-Sik and Stenseth, Nils Chr.},
  title      = {Reduced rank stochastic regression with a sparse singular value decomposition},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2012},
  volume     = {74},
  number     = {2},
  pages      = {203--221},
  issn       = {1369-7412},
  file       = {:Chen2012Reduced.pdf:PDF},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62P10 (62H12 62J05 62J07)},
  mrnumber   = {2899860},
  mrreviewer = {Girdhar G. Agarwal},
  url        = {https://doi.org/10.1111/j.1467-9868.2011.01002.x},
}

@Article{Ma2013Sparse,
  author     = {Ma, Zongming},
  title      = {Sparse principal component analysis and iterative thresholding},
  journal    = {Ann. Statist.},
  year       = {2013},
  volume     = {41},
  number     = {2},
  pages      = {772--801},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62G20 62H25)},
  mrnumber   = {3099121},
  mrreviewer = {Mina Towhidi},
  url        = {https://doi.org/10.1214/13-AOS1097},
}

@Article{Yuan2013Truncated,
  author   = {Yuan, Xiao-Tong and Zhang, Tong},
  title    = {Truncated power method for sparse eigenvalue problems},
  journal  = {J. Mach. Learn. Res.},
  year     = {2013},
  volume   = {14},
  pages    = {899--925},
  issn     = {1532-4435},
  file     = {:Yuan2013Truncated.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {90C20 (05C60 62H25)},
  mrnumber = {3063614},
}

@Article{Candes2011Robust,
  author     = {Cand\`es, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  title      = {Robust principal component analysis?},
  journal    = {J. ACM},
  year       = {2011},
  volume     = {58},
  number     = {3},
  pages      = {Art. 11, 37},
  issn       = {0004-5411},
  fjournal   = {Journal of the ACM},
  mrclass    = {65F30 (62H25)},
  mrnumber   = {2811000},
  mrreviewer = {Steven (Shuangge) Ma},
  url        = {https://doi.org/10.1145/1970392.1970395},
}

@InCollection{Jain2014Iterative,
  author    = {Jain, Prateek and Tewari, Ambuj and Kar, Purushottam},
  title     = {On Iterative Hard Thresholding Methods for High-dimensional M-Estimation},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {685--693},
  url       = {http://papers.nips.cc/paper/5293-on-iterative-hard-thresholding-methods-for-high-dimensional-m-estimation.pdf},
}

@Article{Park2016Finding,
  author    = {Dohyung Park and Anastasios Kyrillidis and Constantine Caramanis and Sujay Sanghavi},
  title     = {Finding Low-Rank Solutions via Nonconvex Matrix Factorization, Efficiently and Provably},
  journal   = {{SIAM} J. Imaging Sciences},
  year      = {2018},
  volume    = {11},
  number    = {4},
  pages     = {2165--2204},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/siamis/ParkKCS18},
  doi       = {10.1137/17M1150189},
  file      = {online:http\://arxiv.org/pdf/1606.03168v3:PDF},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1137/17M1150189},
}

@InCollection{Benini2016Varying,
  author    = {Giacomo Benini and Stefan Sperlich and Raoul Theler},
  title     = {Varying Coefficient Models Revisited: An Econometric View},
  booktitle = {Springer Proceedings in Mathematics {\&} Statistics},
  publisher = {Springer International Publishing},
  year      = {2016},
  pages     = {59--73},
  doi       = {10.1007/978-3-319-41582-6_5},
  file      = {:Benini2016Varying.pdf:PDF},
}

@Article{Li2015Model,
  author     = {Li, Degui and Ke, Yuan and Zhang, Wenyang},
  title      = {Model selection and structure specification in ultra-high dimensional generalised semi-varying coefficient models},
  journal    = {Ann. Statist.},
  year       = {2015},
  volume     = {43},
  number     = {6},
  pages      = {2676--2705},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G08 (62G20 62J07)},
  mrnumber   = {3405608},
  mrreviewer = {Yuehua Wu},
  url        = {https://doi.org/10.1214/15-AOS1356},
}

@Article{Koenker1994Lestimation,
  author    = {Roger Koenker and Quanshui Zhao},
  title     = {L-estimatton for linear heteroscedastic models},
  journal   = {Journal of Nonparametric Statistics},
  year      = {1994},
  volume    = {3},
  number    = {3-4},
  pages     = {223--235},
  month     = {jan},
  doi       = {10.1080/10485259408832584},
  file      = {:Koenker1994Lestimation.pdf:PDF},
  owner     = {mkolar},
  publisher = {Informa {UK} Limited},
  timestamp = {2017.12.21},
}

@Article{Koenker1999Goodness,
  author    = {Roger Koenker and Jos{\'{e}} A. F. Machado},
  title     = {Goodness of Fit and Related Inference Processes for Quantile Regression},
  journal   = {Journal of the American Statistical Association},
  year      = {1999},
  volume    = {94},
  number    = {448},
  pages     = {1296--1310},
  month     = {dec},
  doi       = {10.1080/01621459.1999.10473882},
  file      = {:Koenker1999Goodness.pdf:PDF},
  owner     = {mkolar},
  publisher = {Informa {UK} Limited},
  timestamp = {2017.12.21},
}

@Article{Nolan1987Uprocess,
  author     = {Nolan, Deborah and Pollard, David},
  title      = {{$U$}-processes: rates of convergence},
  journal    = {Ann. Statist.},
  year       = {1987},
  volume     = {15},
  number     = {2},
  pages      = {780--799},
  issn       = {0090-5364},
  fjournal   = {The Annals of Statistics},
  mrclass    = {60F15 (62G05)},
  mrnumber   = {888439},
  mrreviewer = {S\'andor Cs\"org\H o},
  url        = {https://doi.org/10.1214/aos/1176350374},
}

@Book{Boucheron2013Concentration,
  title      = {Concentration inequalities},
  publisher  = {Oxford University Press, Oxford},
  year       = {2013},
  author     = {Boucheron, St\'ephane and Lugosi, G\'abor and Massart, Pascal},
  isbn       = {978-0-19-953525-5},
  note       = {A nonasymptotic theory of independence, With a foreword by Michel Ledoux},
  mrclass    = {60E15 (60D05 60G15 60G50 60G70 62G20)},
  mrnumber   = {3185193},
  mrreviewer = {Sreenivasan Ravi},
  pages      = {x+481},
  url        = {https://doi.org/10.1093/acprof:oso/9780199535255.001.0001},
}

@Article{Lee2007Two,
  author    = {Sun Ho Lee and Seungjin Choi},
  title     = {Two-Dimensional Canonical Correlation Analysis},
  journal   = {{IEEE} Signal Processing Letters},
  year      = {2007},
  volume    = {14},
  number    = {10},
  pages     = {735--738},
  month     = {oct},
  doi       = {10.1109/lsp.2007.896438},
  file      = {:Lee2007Two.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url       = {https://doi.org/10.1109/lsp.2007.896438},
}

@Article{Gao2017Stochastic,
  author      = {Chao Gao and Dan Garber and Nathan Srebro and Jialei Wang and Weiran Wang},
  title       = {Stochastic Canonical Correlation Analysis},
  journal     = {arxiv: 1702.06533},
  year        = {2017},
  abstract    = {We tightly analyze the sample complexity of CCA, provide a learning algorithm that achieves optimal statistical performance in time linear in the required number of samples (up to log factors), as well as a streaming algorithm with similar guarantees.},
  date        = {2017-02-21},
  eprint      = {1702.06533v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Gao2017Stochastic.pdf:PDF},
  keywords    = {cs.LG, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.01},
}

@InCollection{Arora2017Stochastic,
  author    = {Arora, Raman and Marinov, Teodor Vanislavov and Mianjy, Poorya and Srebro, Nati},
  title     = {Stochastic Approximation for Canonical Correlation Analysis},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {4775--4784},
  file      = {:Arora2017Stochastic.pdf:PDF},
  url       = {http://papers.nips.cc/paper/7063-stochastic-approximation-for-canonical-correlation-analysis.pdf},
}

@InCollection{Wang2016Efficienta,
  author    = {Wang, Weiran and Wang, Jialei and Garber, Dan and Srebro, Nati},
  title     = {Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {766--774},
  file      = {:Wang2016Efficienta.pdf:PDF},
  url       = {http://papers.nips.cc/paper/6459-efficient-globally-convergent-stochastic-optimization-for-canonical-correlation-analysis.pdf},
}

@InProceedings{Wang2018Efficient,
  author    = {Jialei Wang and Weiran Wang and Dan Garber and Nathan Srebro},
  title     = {Efficient coordinate-wise leading eigenvector computation},
  booktitle = {Algorithmic Learning Theory, {ALT} 2018, 7-9 April 2018, Lanzarote, Canary Islands, Spain},
  year      = {2018},
  editor    = {Firdaus Janoos and Mehryar Mohri and Karthik Sridharan},
  volume    = {83},
  series    = {Proceedings of Machine Learning Research},
  pages     = {806--820},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/alt/WangWGS18},
  file      = {:Wang2017Efficient.pdf:PDF},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v83/wang18a.html},
}

@Article{Yao2018Inexact,
  author      = {Zhewei Yao and Peng Xu and Farbod Roosta-Khorasani and Michael W. Mahoney},
  title       = {Inexact Non-Convex Newton-Type Methods},
  journal     = {arxiv: 1802.06925},
  year        = {2018},
  abstract    = {For solving large-scale non-convex problems, we propose inexact variants of trust region and adaptive cubic regularization methods, which, to increase efficiency, incorporate various approximations. In particular, in addition to approximate sub-problem solves, both the Hessian and the gradient are suitably approximated. Using rather mild conditions on such approximations, we show that our proposed inexact methods achieve similar optimal worst-case iteration complexities as the exact counterparts. Our proposed algorithms, and their respective theoretical analysis, do not require knowledge of any unknowable problem-related quantities, and hence are easily implementable in practice. In the context of finite-sum problems, we then explore randomized sub-sampling methods as ways to construct the gradient and Hessian approximations and examine the empirical performance of our algorithms on some real datasets.},
  date        = {2018-02-20},
  eprint      = {1802.06925v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:Yao2018Inexact.pdf:PDF},
  keywords    = {math.OC},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Wang2017GIANT:,
  author      = {Shusen Wang and Farbod Roosta-Khorasani and Peng Xu and Michael W. Mahoney},
  title       = {GIANT: Globally Improved Approximate Newton Method for Distributed Optimization},
  journal     = {arxiv:1709.03528},
  year        = {2017},
  abstract    = {For distributed computing environments, we consider the canonical machine learning problem of empirical risk minimization (ERM) with quadratic regularization, and we propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, and then it sends this direction to the main driver. The driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. GIANT is highly communication efficient in that, for $d$-dimensional data uniformly distributed across $m$ workers, it has $4$ or $6$ rounds of communication and $O (d \log m)$ communication complexity per iteration. Theoretically, we show that GIANT's convergence rate is faster than first-order methods and existing distributed Newton-type methods. From a practical point-of-view, a highly beneficial feature of GIANT is that it has only one tuning parameter---the iterations of the local solver for computing an ANT direction. This is indeed in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, which have several tuning parameters, and whose performance can be greatly affected by the specific choices of such parameters. In this light, we empirically demonstrate the superior performance of GIANT compared with other competing methods.},
  date        = {2017-09-11},
  eprint      = {1709.03528v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Wang2017GIANT_.pdf:PDF},
  keywords    = {cs.LG, cs.DC, math.OC, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Xu2017Second,
  author      = {Peng Xu and Farbod Roosta-Khorasani and Michael W. Mahoney},
  title       = {Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study},
  journal     = {arxiv:1708.07827},
  year        = {2017},
  abstract    = {While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.},
  date        = {2017-08-25},
  eprint      = {1708.07827v2},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:Xu2017Second.pdf:PDF},
  keywords    = {math.OC, cs.LG, math.NA, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Xu2017Newton,
  author      = {Peng Xu and Farbod Roosta-Khorasani and Michael W. Mahoney},
  title       = {Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information},
  journal     = {arxiv:1708.07164},
  year        = {2017},
  abstract    = {We consider variants of trust-region and cubic regularization methods for non-convex optimization, in which the Hessian matrix is approximated. Under mild conditions on the inexact Hessian, and using approximate solution of the corresponding sub-problems, we provide iteration complexity to achieve $ \epsilon $-approximate second-order optimality which have shown to be tight. Our Hessian approximation conditions constitute a major relaxation over the existing ones in the literature. Consequently, we are able to show that such mild conditions allow for the construction of the approximate Hessian through various random sampling methods. In this light, we consider the canonical problem of finite-sum minimization, provide appropriate uniform and non-uniform sub-sampling strategies to construct such Hessian approximations, and obtain optimal iteration complexity for the corresponding sub-sampled trust-region and cubic regularization methods.},
  date        = {2017-08-23},
  eprint      = {1708.07164v3},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:Xu2017Newton.pdf:PDF},
  keywords    = {math.OC, cs.CC, cs.LG, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Wang2017Sketched,
  author      = {Shusen Wang and Alex Gittens and Michael W. Mahoney},
  title       = {Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging},
  journal     = {arxiv:1702.04837},
  year        = {2017},
  abstract    = {We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR---namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee, instead, the approximation error is governed by a subtle interplay between the "mass" in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR, these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.},
  date        = {2017-02-16},
  eprint      = {1702.04837v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Wang2017Sketched.pdf:PDF},
  keywords    = {stat.ML, cs.LG, cs.NA},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Xu2016Sub,
  author      = {Peng Xu and Jiyan Yang and Farbod Roosta-Khorasani and Christopher R and Michael W. Mahoney},
  title       = {Sub-sampled Newton Methods with Non-uniform Sampling},
  journal     = {arxiv:1607.00559},
  year        = {2016},
  abstract    = {We consider the problem of finding the minimizer of a convex function $F: \mathbb R^d \rightarrow \mathbb R$ of the form $F(w) := \sum_{i=1}^n f_i(w) + R(w)$ where a low-rank factorization of $\nabla^2 f_i(w)$ is readily available. We consider the regime where $n \gg d$. As second-order methods prove to be effective in finding the minimizer to a high-precision, in this work, we propose randomized Newton-type algorithms that exploit \textit{non-uniform} sub-sampling of $\{\nabla^2 f_i(w)\}_{i=1}^{n}$, as well as inexact updates, as means to reduce the computational complexity. Two non-uniform sampling distributions based on {\it block norm squares} and {\it block partial leverage scores} are considered in order to capture important terms among $\{\nabla^2 f_i(w)\}_{i=1}^{n}$. We show that at each iteration non-uniformly sampling at most $\mathcal O(d \log d)$ terms from $\{\nabla^2 f_i(w)\}_{i=1}^{n}$ is sufficient to achieve a linear-quadratic convergence rate in $w$ when a suitable initial point is provided. In addition, we show that our algorithms achieve a lower computational complexity and exhibit more robustness and better dependence on problem specific quantities, such as the condition number, compared to similar existing methods, especially the ones based on uniform sampling. Finally, we empirically demonstrate that our methods are at least twice as fast as Newton's methods with ridge logistic regression on several real datasets.},
  date        = {2016-07-02},
  eprint      = {1607.00559v2},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:Xu2016Sub.pdf:PDF},
  keywords    = {math.OC, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.06},
}

@Article{Xiao2017Fast,
  author    = {Luo Xiao and Cai Li and William Checkley and Ciprian Crainiceanu},
  title     = {Fast covariance estimation for sparse functional data},
  journal   = {Statistics and Computing},
  year      = {2017},
  volume    = {28},
  number    = {3},
  pages     = {511--522},
  month     = {apr},
  doi       = {10.1007/s11222-017-9744-8},
  file      = {:Xiao2017Fast.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Yao2003Shrinkage,
  author   = {Yao, Fang and M\"uller, Hans-Georg and Clifford, Andrew J. and Dueker, Steven R. and Follett, Jennifer and Lin, Yumei and Buchholz, Bruce A. and Vogel, John S.},
  title    = {Shrinkage estimation for functional principal component scores with application to the population kinetics of plasma folate},
  journal  = {Biometrics},
  year     = {2003},
  volume   = {59},
  number   = {3},
  pages    = {676--685},
  issn     = {0006-341X},
  file     = {:Yao2003Shrinkage.pdf:PDF},
  fjournal = {Biometrics. Journal of the International Biometric Society},
  mrclass  = {62H25 (62P10 92C50)},
  mrnumber = {2004273},
  url      = {https://doi.org/10.1111/1541-0420.00078},
}

@Article{Xiao2016Fast,
  author     = {Xiao, Luo and Zipunnikov, Vadim and Ruppert, David and Crainiceanu, Ciprian},
  title      = {Fast covariance estimation for high-dimensional functional data},
  journal    = {Stat. Comput.},
  year       = {2016},
  volume     = {26},
  number     = {1-2},
  pages      = {409--421},
  issn       = {0960-3174},
  file       = {:Xiao2016Fast.pdf:PDF},
  fjournal   = {Statistics and Computing},
  mrclass    = {62H12 (62G05 62H25)},
  mrnumber   = {3439382},
  mrreviewer = {Antonio Cuevas},
  url        = {https://doi.org/10.1007/s11222-014-9485-x},
}

@Article{Degras2017Simultaneous,
  author   = {Degras, David},
  title    = {Simultaneous confidence bands for the mean of functional data},
  journal  = {Wiley Interdiscip. Rev. Comput. Stat.},
  year     = {2017},
  volume   = {9},
  number   = {3},
  pages    = {e1397, 15},
  issn     = {1939-5108},
  fjournal = {Wiley Interdisciplinary Reviews. Computational Statistics (WIREs)},
  mrclass  = {Expansion},
  mrnumber = {3648600},
  url      = {https://doi.org/10.1002/wics.1397},
}

@Article{Chen2017Factor,
  author      = {Elynn Yi Chen and Rong Chen},
  title       = {Factor Models for High-Dimensional Dynamic Networks: with Application to International Trade Flow Time Series 1981-2015},
  journal     = {arxiv:1710.06325},
  year        = {2017},
  abstract    = {Dynamic network analysis has found an increasing interest in the literature because of the importance of different kinds of dynamic social networks, biological networks, and economic networks. Most available probability and statistical models for dynamic network data are deduced from random graph theory where the networks are characterized on the node and edge level. They are often very restrictive for applications and unscalable to high-dimensional dynamic network data which is very common nowadays. In this paper, we take a different perspective: The evolving sequence of networks are treated as a time series of network matrices. We adopt a matrix factor model where the observed surface dynamic network is assumed to be driven by a latent dynamic network with lower dimensions. The linear relationship between the surface network and the latent network is characterized by unknown but deterministic loading matrices. The latent network and the corresponding loadings are estimated via an eigenanalysis of a positive definite matrix constructed from the auto-cross-covariances of the network times series, thus capturing the dynamics presenting in the network. The proposed method is able to unveil the latent dynamic structure and achieve the objective of dimension reduction. Different from other dynamic network analytical methods that build on latent variables, our approach imposes neither any distributional assumptions on the underlying network nor any parametric forms on its covariance function. The latent network is learned directly from the data with little subjective input. We applied the proposed method to the monthly international trade flow data from 1981 to 2015. The results unveil an interesting evolution of the latent trading network and the relations between the latent entities and the countries.},
  date        = {2017-10-17},
  eprint      = {1710.06325v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Chen2017Factor.pdf:PDF},
  keywords    = {stat.ME},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Chen2017Constrained,
  author    = {Elynn Y. Chen and Ruey S. Tsay and Rong Chen},
  title     = {Constrained Factor Models for High-Dimensional Matrix-Variate Time Series},
  journal   = {Journal of the American Statistical Association},
  year      = {2019},
  volume    = {0},
  number    = {0},
  pages     = {1-37},
  doi       = {10.1080/01621459.2019.1584899},
  eprint    = {https://doi.org/10.1080/01621459.2019.1584899},
  file      = {:Chen2017Constrained.pdf:PDF},
  publisher = {Taylor \& Francis},
  timestamp = {2019.06.03},
  url       = { 
        https://doi.org/10.1080/01621459.2019.1584899
    
},
}

@Article{Wang2016Factor,
  author      = {Dong Wang and Xialu Liu and Rong Chen},
  title       = {Factor Models for Matrix-Valued High-Dimensional Time Series},
  journal     = {arxiv:1610.01889},
  year        = {2016},
  abstract    = {In finance, economics and many other fields, observations in a matrix form are often observed over time. For example, many economic indicators are obtained in different countries over time. Various financial characteristics of many companies are reported over time. Although it is natural to turn a matrix observation into a long vector then use standard vector time series models or factor analysis, it is often the case that the columns and rows of a matrix represent different sets of information that are closely interrelated in a very structural way. We propose a novel factor model that maintains and utilizes the matrix structure to achieve greater dimensional reduction as well as finding clearer and more interpretable factor structures. Estimation procedure and its theoretical properties are investigated and demonstrated with simulated and real examples.},
  date        = {2016-10-06},
  eprint      = {1610.01889v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Wang2016Factor.pdf:PDF},
  keywords    = {stat.ME, stat.AP},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Chen2018Adaptive,
  author      = {Xi Chen and Bo Jiang and Tianyi Lin and Shuzhong Zhang},
  title       = {On Adaptive Cubic Regularized Newton's Methods for Convex Optimization via Random Sampling},
  journal     = {arxiv:1802.05426},
  year        = {2018},
  abstract    = {In this paper, we consider an unconstrained optimization model where the objective is a sum of a large number of possibly nonconvex functions, though overall the objective is assumed to be smooth and convex. Our bid to solving such model uses the framework of cubic regularization of Newton's method.As well known, the crux in cubic regularization is its utilization of the Hessian information, which may be computationally expensive for large-scale problems. To tackle this, we resort to approximating the Hessian matrix via sub-sampling. In particular, we propose to compute an approximated Hessian matrix by either uniformly or non-uniformly sub-sampling the components of the objective. Based upon sub-sampling, we develop both standard and accelerated adaptive cubic regularization approaches and provide theoretical guarantees on global iteration complexity. We show that the standard and accelerated sub-sampled cubic regularization methods achieve iteration complexity in the order of $O(\epsilon^{-1/2})$ and $O(\epsilon^{-1/3})$ respectively, which match those of the original standard and accelerated cubic regularization methods \cite{Cartis-2012-Evaluation, Jiang-2017-Unified} using the full Hessian information. The performances of the proposed methods on regularized logistic regression problems show a clear effect of acceleration in terms of epochs on several real data sets.},
  date        = {2018-02-15},
  eprint      = {1802.05426v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:Chen2018Adaptive.pdf:PDF},
  keywords    = {math.OC},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Ma2017Implicit,
  author      = {Cong Ma and Kaizheng Wang and Yuejie Chi and Yuxin Chen},
  title       = {Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution},
  journal     = {arxiv:1711.10467},
  year        = {2017},
  abstract    = {Recent years have seen a flurry of activities in designing provably efficient nonconvex procedures for solving statistical estimation problems. Due to the highly nonconvex nature of the empirical loss, state-of-the-art procedures often require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. For vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in nonconvex optimization: even in the absence of explicit regularization, gradient descent enforces proper regularization implicitly under various statistical models. In fact, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This "implicit regularization" feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on three fundamental statistical estimation problems, i.e. phase retrieval, low-rank matrix completion, and blind deconvolution, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. In particular, by marrying statistical modeling with generic optimization theory, we develop a general recipe for analyzing the trajectories of iterative algorithms via a leave-one-out perturbation argument. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent achieves near-optimal error control --- measured entrywise and by the spectral norm --- which might be of independent interest.},
  date        = {2017-11-28},
  eprint      = {1711.10467v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Ma2017Implicit.pdf:PDF},
  keywords    = {cs.LG, cs.IT, math.IT, math.OC, math.ST, stat.ML, stat.TH},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Li2018Nonconvex,
  author      = {Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  title       = {Nonconvex Matrix Factorization from Rank-One Measurements},
  journal     = {arxiv:1802.06286},
  year        = {2018},
  abstract    = {We consider the problem of recovering low-rank matrices from random rank-one measurements, which spans numerous applications including covariance sketching, phase retrieval, quantum state tomography, and learning shallow polynomial neural networks, among others. Our approach is to directly estimate the low-rank factor by minimizing a nonconvex quadratic loss function via vanilla gradient descent, following a tailored spectral initialization. When the true rank is small, this algorithm is guaranteed to converge to the ground truth (up to global ambiguity) with near-optimal sample complexity and computational complexity. To the best of our knowledge, this is the first guarantee that achieves near-optimality in both metrics. In particular, the key enabler of near-optimal computational guarantees is an implicit regularization phenomenon: without explicit regularization, both spectral initialization and the gradient descent iterates automatically stay within a region incoherent with the measurement vectors. This feature allows one to employ much more aggressive step sizes compared with the ones suggested in prior literature, without the need of sample splitting.},
  date        = {2018-02-17},
  eprint      = {1802.06286v1},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  file        = {:Li2018Nonconvex.pdf:PDF},
  keywords    = {cs.IT, cs.LG, math.IT, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Chen2018Harnessing,
  author      = {Yudong Chen and Yuejie Chi},
  title       = {Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation},
  journal     = {arxiv:1802.08397},
  year        = {2018},
  abstract    = {Low-rank modeling plays a pivotal role in signal processing and machine learning, with applications ranging from collaborative filtering, video surveillance, medical imaging, to dimensionality reduction and adaptive filtering. Many modern high-dimensional data and interactions thereof can be modeled as lying approximately in a low-dimensional subspace or manifold, possibly with additional structures, and its proper exploitations lead to significant reduction of costs in sensing, computation and storage. In recent years, there is a plethora of progress in understanding how to exploit low-rank structures using computationally efficient procedures in a provable manner, including both convex and nonconvex approaches. On one side, convex relaxations such as nuclear norm minimization often lead to statistically optimal procedures for estimating low-rank matrices, where first-order methods are developed to address the computational challenges; on the other side, there is emerging evidence that properly designed nonconvex procedures, such as projected gradient descent, often provide globally optimal solutions with a much lower computational cost in many problems. This survey article will provide a unified overview of these recent advances on low-rank matrix estimation from incomplete measurements. Attention is paid to rigorous characterization of the performance of these algorithms, and to problems where the low-rank matrix have additional structural properties that require new algorithmic designs and theoretical analysis.},
  date        = {2018-02-23},
  eprint      = {1802.08397v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Chen2018Harnessing.pdf:PDF},
  keywords    = {stat.ML, cs.IT, cs.LG, eess.SP, math.IT},
  owner       = {mkolar},
  timestamp   = {2018.03.08},
}

@Article{Neyshabur2015Path,
  author      = {Behnam Neyshabur and Ruslan Salakhutdinov and Nathan Srebro},
  title       = {Path-SGD: Path-Normalized Optimization in Deep Neural Networks},
  journal     = {arxiv:1506.02617},
  year        = {2015},
  abstract    = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
  date        = {2015-06-08},
  eprint      = {1506.02617v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Neyshabur2015Path.pdf:PDF},
  keywords    = {cs.LG, cs.CV, cs.NE, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.09},
}

@Article{Chaudhari2016Entropy,
  author      = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  title       = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  journal     = {arxiv:1611.01838},
  year        = {2017},
  abstract    = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  date        = {2016-11-06},
  eprint      = {1611.01838v5},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Chaudhari2016Entropy.pdf:PDF},
  keywords    = {cs.LG, stat.ML},
  owner       = {mkolar},
  timestamp   = {2018.03.09},
}

@Article{Robbins1951stochastic,
  author     = {Robbins, Herbert and Monro, Sutton},
  title      = {A stochastic approximation method},
  journal    = {Ann. Math. Statistics},
  year       = {1951},
  volume     = {22},
  pages      = {400--407},
  issn       = {0003-4851},
  file       = {:Robbins1951stochastic.pdf:PDF},
  fjournal   = {Annals of Mathematical Statistics},
  mrclass    = {62.0X},
  mrnumber   = {0042668},
  mrreviewer = {R. P. Peterson},
}

@Article{Shalev-Shwartz2013Stochastic,
  author   = {Shalev-Shwartz, Shai and Zhang, Tong},
  title    = {Stochastic dual coordinate ascent methods for regularized loss minimization},
  journal  = {J. Mach. Learn. Res.},
  year     = {2013},
  volume   = {14},
  pages    = {567--599},
  issn     = {1532-4435},
  file     = {:Shalev-Shwartz2013Stochastic.pdf:PDF},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  mrclass  = {62L20 (68T05)},
  mrnumber = {3033340},
}

@Article{Shalev-Shwartz2016Accelerated,
  author     = {Shalev-Shwartz, Shai and Zhang, Tong},
  title      = {Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
  journal    = {Math. Program.},
  year       = {2016},
  volume     = {155},
  number     = {1-2, Ser. A},
  pages      = {105--145},
  issn       = {0025-5610},
  file       = {:Shalev-Shwartz2016Accelerated.pdf:PDF},
  fjournal   = {Mathematical Programming},
  mrclass    = {90C25 (90C06)},
  mrnumber   = {3439799},
  mrreviewer = {Wim van Ackooij},
  timestamp  = {2018.03.09},
  url        = {https://doi.org/10.1007/s10107-014-0839-0},
}

@InCollection{Roux2012Stochastic,
  author    = {Nicolas L. Roux and Schmidt, Mark and Francis R. Bach},
  title     = {A Stochastic Gradient Method with an Exponential Convergence \_Rate for Finite Training Sets},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {2663--2671},
  file      = {:Roux2012Stochastic.pdf:PDF},
  timestamp = {2018.03.09},
  url       = {http://papers.nips.cc/paper/4633-a-stochastic-gradient-method-with-an-exponential-convergence-_rate-for-finite-training-sets.pdf},
}

@InCollection{Johnson2013Accelerating,
  author    = {Johnson, Rie and Zhang, Tong},
  title     = {Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {315--323},
  file      = {:Johnson2013Accelerating.pdf:PDF},
  timestamp = {2018.03.09},
  url       = {http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf},
}

@InCollection{Defazio2014SAGA:,
  author    = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  title     = {SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {1646--1654},
  file      = {:Defazio2014SAGA_.pdf:PDF},
  timestamp = {2018.03.09},
  url       = {http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf},
}

@InCollection{Defazio2016Simple,
  author    = {Defazio, Aaron},
  title     = {A Simple Practical Accelerated Method for Finite Sums},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {676--684},
  file      = {:Defazio2016Simple.pdf:PDF},
  timestamp = {2018.03.09},
  url       = {http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf},
}

@Article{Liu2017Structural,
  author   = {Liu, Weidong},
  title    = {Structural similarity and difference testing on multiple sparse {G}aussian graphical models},
  journal  = {Ann. Statist.},
  year     = {2017},
  volume   = {45},
  number   = {6},
  pages    = {2680--2707},
  issn     = {0090-5364},
  file     = {:Liu2017Structural.pdf:PDF},
  fjournal = {The Annals of Statistics},
  mrclass  = {62H12 (62H15)},
  mrnumber = {3737906},
  url      = {https://doi.org/10.1214/17-AOS1539},
}

@Article{Zhang2012Learning,
  author      = {Bai Zhang and Yue Wang},
  title       = {Learning Structural Changes of Gaussian Graphical Models in Controlled Experiments},
  journal     = {arxiv:1203.3532},
  year        = {2010},
  abstract    = {Graphical models are widely used in scienti fic and engineering research to represent conditional independence structures between random variables. In many controlled experiments, environmental changes or external stimuli can often alter the conditional dependence between the random variables, and potentially produce significant structural changes in the corresponding graphical models. Therefore, it is of great importance to be able to detect such structural changes from data, so as to gain novel insights into where and how the structural changes take place and help the system adapt to the new environment. Here we report an effective learning strategy to extract structural changes in Gaussian graphical model using l1-regularization based convex optimization. We discuss the properties of the problem formulation and introduce an efficient implementation by the block coordinate descent algorithm. We demonstrate the principle of the approach on a numerical simulation experiment, and we then apply the algorithm to the modeling of gene regulatory networks under different conditions and obtain promising yet biologically plausible results.},
  date        = {2012-03-15},
  eprint      = {1203.3532v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Zhang2012Learning.pdf:PDF},
  keywords    = {cs.LG, stat.ML},
}

@InCollection{Sugiyama2008Direct,
  author    = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Paul V. Buenau and Kawanabe, Motoaki},
  title     = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},
  booktitle = {Advances in Neural Information Processing Systems 20},
  publisher = {Curran Associates, Inc.},
  year      = {2008},
  editor    = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
  pages     = {1433--1440},
  url       = {http://papers.nips.cc/paper/3248-direct-importance-estimation-with-model-selection-and-its-application-to-covariate-shift-adaptation.pdf},
}

@Article{Liu2017Support,
  author   = {Liu, Song and Suzuki, Taiji and Relator, Raissa and Sese, Jun and Sugiyama, Masashi and Fukumizu, Kenji},
  title    = {Support consistency of direct sparse-change learning in {M}arkov networks},
  journal  = {Ann. Statist.},
  year     = {2017},
  volume   = {45},
  number   = {3},
  pages    = {959--990},
  issn     = {0090-5364},
  file     = {:Liu2014Support.pdf:PDF;online:http\://arxiv.org/pdf/1407.0581v10:PDF},
  fjournal = {The Annals of Statistics},
  mrclass  = {62F12 (62H12 62H99 68T99)},
  mrnumber = {3662445},
  url      = {https://doi.org/10.1214/16-AOS1470},
}

@Article{Bassily2018Model,
  author      = {Raef Bassily and Om Thakkar and Abhradeep Thakurta},
  title       = {Model-Agnostic Private Learning via Stability},
  journal     = {arxiv:1803.05101},
  year        = {2018},
  abstract    = {We design differentially private learning algorithms that are agnostic to the learning model. Our algorithms are interactive in nature, i.e., instead of outputting a model based on the training data, they provide predictions for a set of $m$ feature vectors that arrive online. We show that, for the feature vectors on which an ensemble of models (trained on random disjoint subsets of a dataset) makes consistent predictions, there is almost no-cost of privacy in generating accurate predictions for those feature vectors. To that end, we provide a novel coupling of the distance to instability framework with the sparse vector technique. We provide algorithms with formal privacy and utility guarantees for both binary/multi-class classification, and soft-label classification. For binary classification in the standard (agnostic) PAC model, we show how to bootstrap from our privately generated predictions to construct a computationally efficient private learner that outputs a final accurate hypothesis. Our construction - to the best of our knowledge - is the first computationally efficient construction for a label-private learner. We prove sample complexity upper bounds for this setting. As in non-private sample complexity bounds, the only relevant property of the given concept class is its VC dimension. For soft-label classification, our techniques are based on exploiting the stability properties of traditional learning algorithms, like stochastic gradient descent (SGD). We provide a new technique to boost the average-case stability properties of learning algorithms to strong (worst-case) stability properties, and then exploit them to obtain private classification algorithms. In the process, we also show that a large class of SGD methods satisfy average-case stability properties, in contrast to a smaller class of SGD methods that are uniformly stable as shown in prior work.},
  date        = {2018-03-14},
  eprint      = {1803.05101v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Bassily2018Model.pdf:PDF},
  keywords    = {cs.LG},
}

@Article{Szabo2017asymptotic,
  author      = {Botond Szabo and Harry van Zanten},
  title       = {An asymptotic analysis of distributed nonparametric methods},
  abstract    = {We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness.},
  date        = {2017-11-08},
  eprint      = {1711.03149v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1711.03149v1:PDF},
  keywords    = {math.ST, stat.ML, stat.TH, 62G20, 62G15, 62G05},
  timestamp   = {2018.03.21},
}

@Article{Volgushev2017Distributed,
  author      = {Stanislav Volgushev and Shih-Kang Chao and Guang Cheng},
  title       = {Distributed inference for quantile regression processes},
  journal     = {arxiv:1701.06088},
  year        = {2017},
  abstract    = {The increased availability of massive data sets provides a unique opportunity to discover subtle patterns in their distributions, but also imposes overwhelming computational challenges. To fully utilize the information contained in big data, we propose a two-step procedure: (i) estimate conditional quantile functions at different levels in a parallel computing environment; (ii) construct a conditional quantile regression process through projection based on these estimated quantile curves. Our general quantile regression framework covers both linear models with fixed or growing dimension and series approximation models. We prove that the proposed procedure does not sacrifice any statistical inferential accuracy provided that the number of distributed computing units and quantile levels are chosen properly. In particular, a sharp upper bound for the former and a sharp lower bound for the latter are derived to capture the minimal computational cost from a statistical perspective. As an important application, the statistical inference on conditional distribution functions is considered. Moreover, we propose computationally efficient approaches to conducting inference in the distributed estimation setting described above. Those approaches directly utilize the availability of estimators from sub-samples and can be carried out at almost no additional computational cost. Simulations confirm our statistical inferential theory.},
  date        = {2017-01-21},
  eprint      = {1701.06088v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Volgushev2017Distributed.pdf:PDF},
  keywords    = {math.ST, stat.ME, stat.TH},
  timestamp   = {2018.03.21},
}

@InProceedings{Heinze2016DUAL,
  author    = {Christina Heinze and Brian McWilliams and Nicolai Meinshausen},
  title     = {DUAL-LOCO: Distributing Statistical Estimation Using Random Projections},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  series    = {Proceedings of Machine Learning Research},
  pages     = {875--883},
  address   = {Cadiz, Spain},
  month     = {09--11 May},
  publisher = {PMLR},
  abstract  = {We present DUAL-LOCO, a communication-efficient algorithm for distributed statistical estimation. DUAL-LOCO assumes that the data is distributed across workers according to the features rather than the samples. It requires only a single round of communication where low-dimensional random projections are used to approximate the dependencies between features available to different workers. We show that DUAL-LOCO has bounded approximation error which only depends weakly on the number of workers. We compare DUAL-LOCO against a state-of-the-art distributed optimization method on a variety of real world datasets and show that it obtains better speedups while retaining good accuracy. In particular, DUAL-LOCO allows for fast cross validation as only part of the algorithm depends on the regularization parameter.},
  file      = {heinze16.pdf:http\://proceedings.mlr.press/v51/heinze16.pdf:PDF},
  timestamp = {2018.03.21},
  url       = {http://proceedings.mlr.press/v51/heinze16.html},
}

@Article{Shang2015Nonparametric,
  author      = {Zuofeng Shang and Guang Cheng},
  title       = {Nonparametric Bayesian Aggregation for Massive Data},
  abstract    = {We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball.},
  date        = {2015-08-17},
  eprint      = {1508.04175v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1508.04175v2:PDF},
  keywords    = {math.ST, stat.TH},
  timestamp   = {2018.03.21},
}

@Article{Bellec2017Towards,
  author      = {Pierre C. Bellec and Guillaume Lecu and Alexandre B. Tsybakov},
  title       = {Towards the study of least squares estimators with convex penalty},
  journal     = {arxiv:1701.09120},
  year        = {2017},
  abstract    = {Penalized least squares estimation is a popular technique in high-dimensional statistics. It includes such methods as the LASSO, the group LASSO, and the nuclear norm penalized least squares. The existing theory of these methods is not fully satisfying since it allows one to prove oracle inequalities with fixed high probability only for the estimators depending on this probability. Furthermore, the control of compatibility factors appearing in the oracle bounds is often not explicit. Some very recent developments suggest that the theory of oracle inequalities can be revised in an improved way. In this paper, we provide an overview of ideas and tools leading to such an improved theory. We show that, along with overcoming the disadvantages mentioned above, the methodology extends to the hilbertian framework and it applies to a large class of convex penalties. This paper is partly expository. In particular, we provide adapted proofs of some results from other recent work.},
  date        = {2017-01-31},
  eprint      = {1701.09120v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Bellec2017Towards.pdf:PDF},
  keywords    = {math.ST, stat.TH},
  timestamp   = {2018.03.21},
}

@Article{Bellec2016prediction,
  author      = {Pierre C. Bellec and Arnak S. Dalalyan and Edwin Grappin and Quentin Paris},
  title       = {On the prediction loss of the lasso in the partially labeled setting},
  abstract    = {In this paper we revisit the risk bounds of the lasso estimator in the context of transductive and semi-supervised learning. In other terms, the setting under consideration is that of regression with random design under partial labeling. The main goal is to obtain user-friendly bounds on the off-sample prediction risk. To this end, the simple setting of bounded response variable and bounded (high-dimensional) covariates is considered. We propose some new adaptations of the lasso to these settings and establish oracle inequalities both in expectation and in deviation. These results provide non-asymptotic upper bounds on the risk that highlight the interplay between the bias due to the mis-specification of the linear model, the bias due to the approximate sparsity and the variance. They also demonstrate that the presence of a large number of unlabeled features may have significant positive impact in the situations where the restricted eigenvalue of the design matrix vanishes or is very small.},
  date        = {2016-06-20},
  eprint      = {1606.06179v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Bellec2016prediction.pdf:PDF},
  keywords    = {math.ST, stat.ML, stat.TH},
  timestamp   = {2018.03.21},
}

@Article{Dezeure2017High,
  author   = {Dezeure, Ruben and B\"uhlmann, Peter and Zhang, Cun-Hui},
  title    = {High-dimensional simultaneous inference with the bootstrap},
  journal  = {TEST},
  year     = {2017},
  volume   = {26},
  number   = {4},
  pages    = {685--719},
  issn     = {1133-0686},
  doi      = {10.1007/s11749-017-0554-2},
  file     = {:Dezeure2017High.pdf:PDF},
  fjournal = {TEST},
  mrclass  = {62J07 (62F12 62F40 62J15)},
  mrnumber = {3713586},
  url      = {https://doi.org/10.1007/s11749-017-0554-2},
}

@Article{Chernozhukov2016Double/Debiased,
  author      = {Victor Chernozhukov and Denis Chetverikov and Mert Demirer and Esther Duflo and Christian Hansen and Whitney Newey and James Robins},
  title       = {Double/Debiased Machine Learning for Treatment and Causal Parameters},
  abstract    = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
  date        = {2016-07-30},
  eprint      = {1608.00060v6},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Chernozhukov2016Double_Debiased.pdf:PDF},
  keywords    = {stat.ML, econ.EM, 62G},
}

@Article{Chernozhukov2016Locally,
  author      = {Victor Chernozhukov and Juan Carlos Escanciano and Hidehiko Ichimura and Whitney K. Newey},
  title       = {Locally Robust Semiparametric Estimation},
  abstract    = {This paper shows how to construct locally robust semiparametric GMM estimators, meaning equivalently moment conditions have zero derivative with respect to the first step and the first step does not affect the asymptotic variance. They are constructed by adding to the moment functions the adjustment term for first step estimation. Locally robust estimators have several advantages. They are vital for valid inference with machine learning in the first step, see Belloni et. al. (2012, 2014), and are less sensitive to the specification of the first step. They are doubly robust for affine moment functions, so moment conditions continue to hold when one first step component is incorrect. Locally robust moment conditions also have smaller bias that is flatter as a function of first step smoothing leading to improved small sample properties. Series first step estimators confer local robustness on any moment conditions and are doubly robust for affine moments, in the direction of the series approximation. Many new locally and doubly robust estimators are given here, including for economic structural models. We give simple asymptotic theory for estimators that use cross-fitting in the first step, including machine learning.},
  date        = {2016-07-29},
  eprint      = {1608.00033v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Chernozhukov2016Locally.pdf:PDF},
  keywords    = {math.ST, econ.EM, stat.TH, 62G05},
}

@Article{Mackey2017Orthogonal,
  author      = {Lester Mackey and Vasilis Syrgkanis and Ilias Zadik},
  title       = {Orthogonal Machine Learning: Power and Limitations},
  abstract    = {Double machine learning provides $\sqrt{n}$-consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance parameters are estimated at an $n^{-1/4}$ rate. The key is to employ Neyman-orthogonal moment equations which are first-order insensitive to perturbations in the nuisance parameters. We show that the $n^{-1/4}$ requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order notion of orthogonality that grants robustness to more complex or higher-dimensional nuisance parameters. In the partially linear regression setting popular in causal inference, we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed. Our proof relies on Stein's lemma and may be of independent interest. We conclude by demonstrating the robustness benefits of an explicit doubly-orthogonal estimation procedure for treatment effect.},
  date        = {2017-11-01},
  eprint      = {1711.00342v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Mackey2017Orthogonal.pdf:PDF},
  keywords    = {cs.LG, econ.EM, math.ST, stat.ML, stat.TH},
}

@Article{Zhu2017Linear,
  author    = {Yinchu Zhu and Jelena Bradic},
  title     = {Linear Hypothesis Testing in Dense High-Dimensional Linear Models},
  journal   = {Journal of the American Statistical Association},
  year      = {2017},
  pages     = {0--0},
  month     = {aug},
  doi       = {10.1080/01621459.2017.1356319},
  file      = {:Zhu2017Linear.pdf:PDF},
  publisher = {Informa {UK} Limited},
  url       = {https://doi.org/10.1080/01621459.2017.1356319},
}

@InProceedings{Zhang2015Multi,
  author    = {Zhang, Yu},
  title     = {Multi-task Learning and Algorithmic Stability},
  booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year      = {2015},
  series    = {AAAI'15},
  pages     = {3181--3187},
  publisher = {AAAI Press},
  acmid     = {2888159},
  file      = {:Zhang2015Multi.pdf:PDF},
  isbn      = {0-262-51129-0},
  location  = {Austin, Texas},
  numpages  = {7},
  timestamp = {2018.04.13},
  url       = {http://dl.acm.org/citation.cfm?id=2888116.2888159},
}

@Article{Zhang2017Distributed,
  author    = {Chi Zhang and Peilin Zhao and Shuji Hao and Yeng Chai Soh and Bu Sung Lee and Chunyan Miao and Steven C. H. Hoi},
  title     = {Distributed multi-task classification: a decentralized online learning approach},
  journal   = {Machine Learning},
  year      = {2017},
  volume    = {107},
  number    = {4},
  pages     = {727--747},
  month     = {nov},
  doi       = {10.1007/s10994-017-5676-y},
  file      = {:Zhang2017Distributed.pdf:PDF},
  publisher = {Springer Nature},
  timestamp = {2018.04.13},
}

@Article{Li2017Visualizing,
  author      = {Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
  title       = {Visualizing the Loss Landscape of Neural Nets},
  abstract    = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comp arisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  date        = {2017-12-28},
  eprint      = {1712.09913v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Li2017Visualizing.pdf:PDF},
  keywords    = {cs.LG, cs.CV, stat.ML},
}

@Article{Wong2017Provable,
  author      = {Eric Wong and J. Zico Kolter},
  title       = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
  abstract    = {We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$). Code for all experiments in the paper is available at https://github.com/locuslab/convex_adversarial.},
  date        = {2017-11-02},
  eprint      = {1711.00851v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Wong2017Provable.pdf:PDF},
  keywords    = {cs.LG, cs.AI, math.OC},
  timestamp   = {2018.04.17},
}

@Article{Yang2018Estimating,
  author      = {Jilei Yang and Jie Peng},
  title       = {Estimating Time-Varying Graphical Models},
  journal     = {arxiv: 1804.03811},
  year        = {2018},
  abstract    = {In this paper, we study time-varying graphical models based on data measured over a temporal grid. Such models are motivated by the needs to describe and understand evolving interacting relationships among a set of random variables in many real applications, for instance the study of how stocks interact with each other and how such interactions change over time. We propose a new model, LOcal Group Graphical Lasso Estimation (loggle), under the assumption that the graph topology changes gradually over time. Specifically, loggle uses a novel local group-lasso type penalty to efficiently incorporate information from neighboring time points and to impose structural smoothness of the graphs. We implement an ADMM based algorithm to fit the loggle model. This algorithm utilizes blockwise fast computation and pseudo-likelihood approximation to improve computational efficiency. An R package loggle has also been developed. We evaluate the performance of loggle by simulation experiments. We also apply loggle to S&P 500 stock price data and demonstrate that loggle is able to reveal the interacting relationships among stocks and among industrial sectors in a time period that covers the recent global financial crisis.},
  date        = {2018-04-11},
  eprint      = {1804.03811v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Yang2018Estimating.pdf:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Hallac2017Network,
  author      = {David Hallac and Youngsuk Park and Stephen Boyd and Jure Leskovec},
  title       = {Network Inference via the Time-Varying Graphical Lasso},
  abstract    = {Many important problems can be modeled as a system of interconnected entities, where each entity is recording time-dependent observations or measurements. In order to spot trends, detect anomalies, and interpret the temporal dynamics of such data, it is essential to understand the relationships between the different entities and how these relationships evolve over time. In this paper, we introduce the time-varying graphical lasso (TVGL), a method of inferring time-varying networks from raw time series data. We cast the problem in terms of estimating a sparse time-varying inverse covariance matrix, which reveals a dynamic network of interdependencies between the entities. Since dynamic network inference is a computationally expensive task, we derive a scalable message-passing algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve this problem in an efficient way. We also discuss several extensions, including a streaming algorithm to update the model and incorporate new observations in real time. Finally, we evaluate our TVGL algorithm on both real and synthetic datasets, obtaining interpretable results and outperforming state-of-the-art baselines in terms of both accuracy and scalability.},
  date        = {2017-03-06},
  eprint      = {1703.01958v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Hallac2017Network.pdf:PDF},
  keywords    = {cs.LG, cs.SI, math.OC},
}

@Article{Wit2015Inferring,
  author    = {Ernst C Wit and Antonino Abbruzzo},
  title     = {Inferring slowly-changing dynamic gene-regulatory networks},
  journal   = {{BMC} Bioinformatics},
  year      = {2015},
  volume    = {16},
  number    = {Suppl 6},
  pages     = {S5},
  doi       = {10.1186/1471-2105-16-s6-s5},
  file      = {:Wit2015Inferring.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Liu2017inexact,
  author      = {Xuanqing Liu and Cho-Jui Hsieh and Jason D. Lee and Yuekai Sun},
  title       = {An inexact subsampled proximal Newton-type method for large-scale machine learning},
  journal     = {arxiv: 1708.08552},
  year        = {2017},
  abstract    = {We propose a fast proximal Newton-type algorithm for minimizing regularized finite sums that returns an $\epsilon$-suboptimal point in $\tilde{\mathcal{O}}(d(n + \sqrt{\kappa d})\log(\frac{1}{\epsilon}))$ FLOPS, where $n$ is number of samples, $d$ is feature dimension, and $\kappa$ is the condition number. As long as $n > d$, the proposed method is more efficient than state-of-the-art accelerated stochastic first-order methods for non-smooth regularizers which requires $\tilde{\mathcal{O}}(d(n + \sqrt{\kappa n})\log(\frac{1}{\epsilon}))$ FLOPS. The key idea is to form the subsampled Newton subproblem in a way that preserves the finite sum structure of the objective, thereby allowing us to leverage recent developments in stochastic first-order methods to solve the subproblem. Experimental results verify that the proposed algorithm outperforms previous algorithms for $\ell_1$-regularized logistic regression on real datasets.},
  date        = {2017-08-28},
  eprint      = {1708.08552v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Liu2017inexact.pdf:PDF},
  keywords    = {cs.LG, cs.NA, stat.ML},
}

@Article{Lee2016LLORMA:,
  author  = {Joonseok Lee and Seungyeon Kim and Guy Lebanon and Yoram Singer and Samy Bengio},
  title   = {LLORMA: Local Low-Rank Matrix Approximation},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {15},
  pages   = {1-24},
  file    = {:Lee2016LLORMA_.pdf:PDF},
  url     = {http://jmlr.org/papers/v17/14-301.html},
}

@InProceedings{Sinha2018Certifiable,
  author    = {Aman Sinha and Hongseok Namkoong and John Duchi},
  title     = {Certifiable Distributional Robustness with Principled Adversarial Training},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  file      = {:Sinha2018Certifiable.pdf:PDF},
  url       = {https://openreview.net/forum?id=Hk6kPgZA-},
}

@Article{Hamilton2017Representation,
  author      = {William L. Hamilton and Rex Ying and Jure Leskovec},
  title       = {Representation Learning on Graphs: Methods and Applications},
  journal     = {arxiv: 1709.05584},
  year        = {2017},
  abstract    = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
  date        = {2017-09-17},
  eprint      = {1709.05584v3},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  file        = {:Hamilton2017Representation.pdf:PDF},
  keywords    = {cs.SI, cs.LG},
  timestamp   = {2018.04.27},
}

@Article{Jankova2016Semi,
  author    = {Jankov\'{a}, Jana and van de Geer, Sara},
  title     = {Semiparametric efficiency bounds for high-dimensional models},
  journal   = {Ann. Statist.},
  year      = {2018},
  volume    = {46},
  number    = {5},
  pages     = {2336--2359},
  issn      = {0090-5364},
  doi       = {10.1214/17-AOS1622},
  file      = {:Jankova2016Semi.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  mrclass   = {62J07 (62F12 62H99)},
  mrnumber  = {3845020},
  timestamp = {2019.05.02},
  url       = {https://doi.org/10.1214/17-AOS1622},
}

@InCollection{Jankova2018Inference,
  author    = {Jankov\'{a}, Jana and van de Geer, Sara},
  title     = {Inference in high-dimensional graphical models},
  booktitle = {Handbook of graphical models},
  publisher = {CRC Press, Boca Raton, FL},
  year      = {2019},
  series    = {Chapman \& Hall/CRC Handb. Mod. Stat. Methods},
  pages     = {325--349},
  file      = {:Jankova2018Inference.pdf:PDF},
  mrclass   = {62H99 (62H12 62J07)},
  mrnumber  = {3888003},
  timestamp = {2019.05.02},
}

@Article{Jankova2017Honest,
  author     = jjankova #{ and } # svdgeer,
  title      = {Honest confidence regions and optimality in high-dimensional precision matrix estimation},
  journal    = {TEST},
  year       = {2017},
  volume     = {26},
  number     = {1},
  pages      = {143--162},
  issn       = {1133-0686},
  doi        = {10.1007/s11749-016-0503-5},
  file       = {:Jankova2017Honest.pdf:PDF},
  fjournal   = {TEST},
  mrclass    = {62J07 (62F12 62H12)},
  mrnumber   = {3613609},
  mrreviewer = {Hung Hung},
  timestamp  = {2018.04.27},
  url        = {https://doi.org/10.1007/s11749-016-0503-5},
}

@Article{Jankova2016Confidence,
  author      = jjankova #{ and } # svdgeer,
  title       = {Confidence regions for high-dimensional generalized linear models under sparsity},
  year        = {2016},
  abstract    = {We study asymptotically normal estimation and confidence regions for low-dimensional parameters in high-dimensional sparse models. Our approach is based on the $\ell_1$-penalized M-estimator which is used for construction of a bias corrected estimator. We show that the proposed estimator is asymptotically normal, under a sparsity assumption on the high-dimensional parameter, smoothness conditions on the expected loss and an entropy condition. This leads to uniformly valid confidence regions and hypothesis testing for low-dimensional parameters. The present approach is different in that it allows for treatment of loss functions that we not sufficiently differentiable, such as quantile loss, Huber loss or hinge loss functions. We also provide new results for estimation of the inverse Fisher information matrix, which is necessary for the construction of the proposed estimator. We formulate our results for general models under high-level conditions, but investigate these conditions in detail for generalized linear models and provide mild sufficient conditions. As particular examples, we investigate the case of quantile loss and Huber loss in linear regression and demonstrate the performance of the estimators in a simulation study and on real datasets from genome-wide association studies. We further investigate the case of logistic regression and illustrate the performance of the estimator on simulated and real data.},
  date        = {2016-10-05},
  eprint      = {1610.01353v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Jankova2016Confidence.pdf:PDF},
  keywords    = {stat.ME, math.ST, stat.TH},
  timestamp   = {2018.04.27},
}

@InProceedings{Yu2018Graphical,
  author    = {Shiqing Yu and Mathias Drton and Ali Shojaie},
  title     = {Graphical Models for Non-Negative Data Using Generalized Score Matching},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  year      = {2018},
  editor    = {Amos Storkey and Fernando Perez-Cruz},
  volume    = {84},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1781--1790},
  address   = {Playa Blanca, Lanzarote, Canary Islands},
  month     = {09--11 Apr},
  publisher = {PMLR},
  abstract  = {A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. In contrast, the score matching method of Hyvrinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over $\mathbb{R}^m$. Hyvrinen (2007) extended the approach to distributions supported on the non-negative orthant $\mathbb{R}_+^m$. In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. We also generalize the regularized score matching method of Lin et al. (2016) for non-negative Gaussian graphical models, with improved theoretical guarantees.},
  file      = {:Yu2018Graphical.pdf:PDF;yu18b.pdf:http\://proceedings.mlr.press/v84/yu18b/yu18b.pdf:PDF},
  timestamp = {2018.04.30},
  url       = {http://proceedings.mlr.press/v84/yu18b.html},
}

@InProceedings{Suggala2017Expxorcist,
  author    = {Arun Sai Suggala and Mladen Kolar and Pradeep Ravikumar},
  title     = {{The Expxorcist}: Nonparametric Graphical Models Via Conditional Exponential Densities},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}},
  year      = {2017},
  editor    = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
  pages     = {4449--4459},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/SuggalaKR17},
  timestamp = {2019.05.02},
  url       = {http://papers.nips.cc/paper/7031-the-expxorcist-nonparametric-graphical-models-via-conditional-exponential-densities},
}

@TechReport{Babichev2016Slice,
  author      = {Babichev, Dmitry and Bach, Francis},
  title       = {Slice inverse regression with score functions},
  institution = {INRIA},
  year        = {2016},
  file        = {:Babichev2016Slice.pdf:PDF},
  timestamp   = {2018.05.07},
}

@Article{Ma2015Varying,
  author    = {Ma, Shujie and Song, Peter X.-K.},
  title     = {Varying index coefficient models},
  journal   = {J. Amer. Statist. Assoc.},
  year      = {2015},
  volume    = {110},
  number    = {509},
  pages     = {341--356},
  issn      = {0162-1459},
  doi       = {10.1080/01621459.2014.903185},
  file      = {:Ma2015Varying.pdf:PDF},
  fjournal  = {Journal of the American Statistical Association},
  mrclass   = {62G08 (62P10)},
  mrnumber  = {3338507},
  timestamp = {2018.05.07},
  url       = {https://doi.org/10.1080/01621459.2014.903185},
}

@Article{Guo2016Generalized,
  author     = {Guo, Chaohui and Yang, Hu and Lv, Jing},
  title      = {Generalized varying index coefficient models},
  journal    = {J. Comput. Appl. Math.},
  year       = {2016},
  volume     = {300},
  pages      = {1--17},
  issn       = {0377-0427},
  doi        = {10.1016/j.cam.2015.11.025},
  file       = {:Guo2016Generalized.pdf:PDF},
  fjournal   = {Journal of Computational and Applied Mathematics},
  mrclass    = {62G05 (62J12)},
  mrnumber   = {3460280},
  mrreviewer = {Peng Lai},
  timestamp  = {2018.05.07},
  url        = {https://doi.org/10.1016/j.cam.2015.11.025},
}

@Article{Yang2017Steins,
  author      = {Zhuoran Yang and Krishnakumar Balasubramanian and Han Liu},
  title       = {On Stein's Identity and Near-Optimal Estimation in High-dimensional Index Models},
  journal     = {arXiv: 1709.08795},
  year        = {2017},
  abstract    = {We consider estimating the parametric components of semi-parametric multiple index models in a high-dimensional non-Gaussian setting. Our estimators leverage the score function based second-order Stein's lemma and do not require Gaussian or elliptical symmetry assumptions made in the literature. Moreover, to handle score functions and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. We show that our estimator achieves near- optimal statistical rate of convergence in several settings. We supplement our theoretical results via simulation experiments that confirm the theory.},
  date        = {2017-09-26},
  eprint      = {1709.08795v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Yang2017Steins.pdf:PDF},
  keywords    = {math.ST, stat.ME, stat.ML, stat.TH},
  timestamp   = {2018.05.07},
}

@Article{Fan2012Variance,
  author     = {Fan, Jianqing and Guo, Shaojun and Hao, Ning},
  title      = {Variance estimation using refitted cross-validation in ultrahigh dimensional regression},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2012},
  volume     = {74},
  number     = {1},
  pages      = {37--65},
  issn       = {1369-7412},
  doi        = {10.1111/j.1467-9868.2011.01005.x},
  file       = {:Fan2012Variance.pdf:PDF},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62G05 (62F07 62J07)},
  mrnumber   = {2885839},
  mrreviewer = {Alex Karagrigoriou},
  timestamp  = {2018.05.09},
  url        = {https://doi.org/10.1111/j.1467-9868.2011.01005.x},
}

@Article{Chiou2016Multivariate,
  author    = {Chiou, Jeng-Min and Yang, Ya-Fang and Chen, Yu-Ting},
  title     = {Multivariate functional linear regression and prediction},
  journal   = {J. Multivariate Anal.},
  year      = {2016},
  volume    = {146},
  pages     = {301--312},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2015.10.003},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62M20 (62H25 62J05 62M09)},
  mrnumber  = {3477667},
  timestamp = {2018.05.09},
  url       = {https://doi.org/10.1016/j.jmva.2015.10.003},
}

@Article{Brunel2016Non,
  author     = {Brunel, \'Elodie and Mas, Andr\'e and Roche, Angelina},
  title      = {Non-asymptotic adaptive prediction in functional linear models},
  journal    = {J. Multivariate Anal.},
  year       = {2016},
  volume     = {143},
  pages      = {208--232},
  issn       = {0047-259X},
  doi        = {10.1016/j.jmva.2015.09.008},
  file       = {:Brunel2016Non.pdf:PDF},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {62J05 (62G08 62H25 62J07)},
  mrnumber   = {3431429},
  mrreviewer = {Jialiang Li},
  timestamp  = {2018.05.09},
  url        = {https://doi.org/10.1016/j.jmva.2015.09.008},
}

@Unpublished{Roche2018VARIABLE,
  author      = {Roche, Angelina},
  title       = {{VARIABLE SELECTION AND ESTIMATION IN MULTIVARIATE FUNCTIONAL LINEAR REGRESSION VIA THE LASSO}},
  note        = {working paper or preprint},
  month       = Mar,
  year        = {2018},
  file        = {:Roche2018VARIABLE.pdf:PDF;lassov2.pdf:https\://hal.archives-ouvertes.fr/hal-01725351/file/lassov2.pdf:PDF},
  hal_id      = {hal-01725351},
  hal_version = {v1},
  timestamp   = {2018.05.10},
  url         = {https://hal.archives-ouvertes.fr/hal-01725351},
}

@Article{Chagny2016Adaptive,
  author     = {Chagny, Ga\"elle and Roche, Angelina},
  title      = {Adaptive estimation in the functional nonparametric regression model},
  journal    = {J. Multivariate Anal.},
  year       = {2016},
  volume     = {146},
  pages      = {105--118},
  issn       = {0047-259X},
  doi        = {10.1016/j.jmva.2015.07.001},
  file       = {:Chagny2016Adaptive.pdf:PDF},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {62G08 (62H12)},
  mrnumber   = {3477653},
  mrreviewer = {Pauliina Ilmonen},
  timestamp  = {2018.05.09},
  url        = {https://doi.org/10.1016/j.jmva.2015.07.001},
}

@Article{Qi2018Function,
  author    = {Qi, Xin and Luo, Ruiyan},
  title     = {Function-on-function regression with thousands of predictive curves},
  journal   = {J. Multivariate Anal.},
  year      = {2018},
  volume    = {163},
  pages     = {51--66},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2017.10.002},
  file      = {:Qi2018Function.pdf:PDF},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62G08 (62G20 62J05)},
  mrnumber  = {3732340},
  timestamp = {2018.05.09},
  url       = {https://doi.org/10.1016/j.jmva.2017.10.002},
}

@Article{Smaga2018note,
  author    = {{\L}ukasz Smaga and Hidetoshi Matsui},
  title     = {A note on variable selection in functional regression via random subspace method},
  journal   = {Statistical Methods {\&} Applications},
  year      = {2018},
  month     = {jan},
  doi       = {10.1007/s10260-018-0421-7},
  file      = {:Smaga2018note.pdf:PDF},
  publisher = {Springer Nature},
  timestamp = {2018.05.09},
}

@Article{Goia2016introduction,
  author    = {Goia, Aldo and Vieu, Philippe},
  title     = {An introduction to recent advances in high/infinite dimensional statistics [{E}ditorial]},
  journal   = {J. Multivariate Anal.},
  year      = {2016},
  volume    = {146},
  pages     = {1--6},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2015.12.001},
  file      = {:Goia2016introduction.pdf:PDF},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62-06 (62Fxx 62Gxx 62Hxx 62Jxx)},
  mrnumber  = {3477644},
  timestamp = {2018.05.09},
  url       = {https://doi.org/10.1016/j.jmva.2015.12.001},
}

@Article{Fraiman2016Feature,
  author     = {Fraiman, Ricardo and Gimenez, Yanina and Svarc, Marcela},
  title      = {Feature selection for functional data},
  journal    = {J. Multivariate Anal.},
  year       = {2016},
  volume     = {146},
  pages      = {191--208},
  issn       = {0047-259X},
  doi        = {10.1016/j.jmva.2015.09.006},
  file       = {:Fraiman2016Feature.pdf:PDF},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {62H30 (62H25 62J05)},
  mrnumber   = {3477659},
  mrreviewer = {Jos\'e R. Berrendero},
  timestamp  = {2018.05.09},
  url        = {https://doi.org/10.1016/j.jmva.2015.09.006},
}

@Article{Luo2017Function,
  author    = {Luo, Ruiyan and Qi, Xin},
  title     = {Function-on-function linear regression by signal compression},
  journal   = {J. Amer. Statist. Assoc.},
  year      = {2017},
  volume    = {112},
  number    = {518},
  pages     = {690--705},
  issn      = {0162-1459},
  doi       = {10.1080/01621459.2016.1164053},
  file      = {:Luo2017Function.pdf:PDF},
  fjournal  = {Journal of the American Statistical Association},
  mrclass   = {62J05 (62H25)},
  mrnumber  = {3671763},
  timestamp = {2018.05.10},
  url       = {https://doi.org/10.1080/01621459.2016.1164053},
}

@Article{Luo2016Functional,
  author    = {Luo, Ruiyan and Qi, Xin and Wang, Yanhong},
  title     = {Functional wavelet regression for linear function-on-function models},
  journal   = {Electron. J. Stat.},
  year      = {2016},
  volume    = {10},
  number    = {2},
  pages     = {3179--3216},
  issn      = {1935-7524},
  doi       = {10.1214/16-EJS1204},
  file      = {:Luo2016Functional.pdf:PDF},
  fjournal  = {Electronic Journal of Statistics},
  mrclass   = {62J05 (62G08 62J07 94A08)},
  mrnumber  = {3571966},
  timestamp = {2018.05.10},
  url       = {https://doi.org/10.1214/16-EJS1204},
}

@Article{Ivanescu2015Penalized,
  author    = {Ivanescu, Andrada E. and Staicu, Ana-Maria and Scheipl, Fabian and Greven, Sonja},
  title     = {Penalized function-on-function regression},
  journal   = {Comput. Statist.},
  year      = {2015},
  volume    = {30},
  number    = {2},
  pages     = {539--568},
  issn      = {0943-4062},
  doi       = {10.1007/s00180-014-0548-4},
  file      = {:Ivanescu2015Penalized.pdf:PDF},
  fjournal  = {Computational Statistics},
  mrclass   = {Expansion},
  mrnumber  = {3357075},
  timestamp = {2018.05.10},
  url       = {https://doi.org/10.1007/s00180-014-0548-4},
}

@Article{Scheipl2015Functional,
  author    = {Scheipl, Fabian and Staicu, Ana-Maria and Greven, Sonja},
  title     = {Functional additive mixed models},
  journal   = {J. Comput. Graph. Statist.},
  year      = {2015},
  volume    = {24},
  number    = {2},
  pages     = {477--501},
  issn      = {1061-8600},
  doi       = {10.1080/10618600.2014.901914},
  file      = {:Scheipl2015Functional.pdf:PDF},
  fjournal  = {Journal of Computational and Graphical Statistics},
  mrclass   = {62J05},
  mrnumber  = {3357391},
  timestamp = {2018.05.10},
  url       = {https://doi.org/10.1080/10618600.2014.901914},
}

@Article{Lv2016Robust,
  author    = {Lv, Jing and Yang, Hu and Guo, Chaohui},
  title     = {Robust estimation for varying index coefficient models},
  journal   = {Comput. Statist.},
  year      = {2016},
  volume    = {31},
  number    = {3},
  pages     = {1131--1167},
  issn      = {0943-4062},
  doi       = {10.1007/s00180-015-0595-5},
  file      = {:Lv2016Robust.pdf:PDF},
  fjournal  = {Computational Statistics},
  mrclass   = {Expansion},
  mrnumber  = {3528649},
  timestamp = {2018.05.10},
  url       = {https://doi.org/10.1007/s00180-015-0595-5},
}

@InProceedings{Inouye2016Square,
  author    = {David Inouye and Pradeep Ravikumar and Inderjit Dhillon},
  title     = {Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2445--2453},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {We develop Square Root Graphical Models (SQR), a novel class of parametric graphical models that provides multivariate generalizations of univariate exponential family distributions. Previous multivariate graphical models [Yang et al. 2015] did not allow positive dependencies for the exponential and Poisson generalizations. However, in many real-world datasets, variables clearly have positive dependencies. For example, the airport delay time in New Yorkmodeled as an exponential distributionis positively related to the delay time in Boston. With this motivation, we give an example of our model class derived from the univariate exponential distribution that allows for almost arbitrary positive and negative dependencies with only a mild condition on the parameter matrixa condition akin to the positive definiteness of the Gaussian covariance matrix. Our Poisson generalization allows for both positive and negative dependencies without any constraints on the parameter values. We also develop parameter estimation methods using node-wise regressions with \ell_1 regularization and likelihood approximation methods using sampling. Finally, we demonstrate our exponential generalization on a synthetic dataset and a real-world dataset of airport delay times.},
  file      = {:Inouye2016Square.pdf:PDF;inouye16.pdf:http\://proceedings.mlr.press/v48/inouye16.pdf:PDF},
  timestamp = {2018.05.14},
  url       = {http://proceedings.mlr.press/v48/inouye16.html},
}

@Article{Lederer2016Graphical,
  author      = {Johannes Lederer},
  title       = {Graphical Models for Discrete and Continuous Data},
  year        = {2016},
  abstract    = {We introduce a general framework for undirected graphical models. It generalizes Gaussian graphical models to a wide range of continuous, discrete, and combinations of different types of data. We also show that the models in the framework, called exponential trace models, are amenable to efficient estimation and inference based on maximum likelihood. As a consequence, we expect applications to a large variety of multivariate data that have correlated coordinates.},
  date        = {2016-09-18},
  eprint      = {1609.05551v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Lederer2016Graphical.pdf:PDF},
  keywords    = {math.ST, stat.OT, stat.TH},
  timestamp   = {2018.05.11},
}

@Article{Bu2017Integrating,
  author      = {Yunqi Bu and Johannes Lederer},
  title       = {Integrating Additional Knowledge Into Estimation of Graphical Models},
  journal     = {arxiv: 1704.02739},
  year        = {2017},
  abstract    = {In applications of graphical models, we typically have more information than just the samples themselves. A prime example is the estimation of brain connectivity networks based on fMRI data, where in addition to the samples themselves, the spatial positions of the measurements are readily available. With particular regard for this application, we are thus interested in ways to incorporate additional knowledge most effectively into graph estimation. Our approach to this is to make neighborhood selection receptive to additional knowledge by strengthening the role of the tuning parameters. We demonstrate that this concept (i) can improve reproducibility, (ii) is computationally convenient and efficient, and (iii) carries a lucid Bayesian interpretation. We specifically show that the approach provides effective estimations of brain connectivity graphs from fMRI data. However, providing a general scheme for the inclusion of additional knowledge, our concept is expected to have applications in a wide range of domains.},
  date        = {2017-04-10},
  eprint      = {1704.02739v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Bu2017Integrating.pdf:PDF},
  keywords    = {stat.ML, stat.AP, stat.ME},
  timestamp   = {2018.05.11},
}

@Article{Inouye2016Generalized,
  author      = {David I. Inouye and Pradeep Ravikumar and Inderjit S. Dhillon},
  title       = {Generalized Root Models: Beyond Pairwise Graphical Models for Univariate Exponential Families},
  journal     = {arxiv: 1606.00813},
  year        = {2016},
  abstract    = {We present a novel k-way high-dimensional graphical model called the Generalized Root Model (GRM) that explicitly models dependencies between variable sets of size k > 2---where k = 2 is the standard pairwise graphical model. This model is based on taking the k-th root of the original sufficient statistics of any univariate exponential family with positive sufficient statistics, including the Poisson and exponential distributions. As in the recent work with square root graphical (SQR) models [Inouye et al. 2016]---which was restricted to pairwise dependencies---we give the conditions of the parameters that are needed for normalization using the radial conditionals similar to the pairwise case [Inouye et al. 2016]. In particular, we show that the Poisson GRM has no restrictions on the parameters and the exponential GRM only has a restriction akin to negative definiteness. We develop a simple but general learning algorithm based on L1-regularized node-wise regressions. We also present a general way of numerically approximating the log partition function and associated derivatives of the GRM univariate node conditionals---in contrast to [Inouye et al. 2016], which only provided algorithm for estimating the exponential SQR. To illustrate GRM, we model word counts with a Poisson GRM and show the associated k-sized variable sets. We finish by discussing methods for reducing the parameter space in various situations.},
  date        = {2016-06-02},
  eprint      = {1606.00813v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Inouye2016Generalized.pdf:PDF},
  keywords    = {stat.ML},
  timestamp   = {2018.05.11},
}

@Article{Glasser2017Using,
  author    = {Matthew F Glasser and Timothy S Coalson and Janine D Bijsterbosch and Samuel J Harrison and Michael P Harms and Alan Anticevic and David C Van Essen and Stephen M Smith},
  title     = {Using Temporal {ICA} to Selectively Remove Global Noise While Preserving Global Signal in Functional {MRI} Data},
  year      = {2017},
  month     = {sep},
  doi       = {10.1101/193862},
  file      = {:Glasser2017Using.pdf:PDF},
  publisher = {Cold Spring Harbor Laboratory},
  timestamp = {2018.05.11},
}

@Article{Kadri2016Operator,
  author     = {Kadri, Hachem and Duflos, Emmanuel and Preux, Philippe and Canu, St\'ephane and Rakotomamonjy, Alain and Audiffren, Julien},
  title      = {Operator-valued kernels for learning from functional response data},
  journal    = {J. Mach. Learn. Res.},
  year       = {2016},
  volume     = {17},
  pages      = {Paper No. 20, 54},
  issn       = {1532-4435},
  file       = {:Kadri2016Operator.pdf:PDF},
  fjournal   = {Journal of Machine Learning Research (JMLR)},
  mrclass    = {62H30 (62G08 68Q32)},
  mrnumber   = {3491114},
  mrreviewer = {Antonio Cuevas},
  timestamp  = {2018.05.14},
}

@Article{Szabo2016Learning,
  author    = {Zolt{{\'a}}n Szab{{\'o}} and Bharath K. Sriperumbudur and Barnab{{\'a}}s P{{\'o}}czos and Arthur Gretton},
  title     = {Learning Theory for Distribution Regression},
  journal   = {Journal of Machine Learning Research},
  year      = {2016},
  volume    = {17},
  number    = {152},
  pages     = {1-40},
  file      = {:Szabo2016Learning.pdf:PDF},
  timestamp = {2018.05.14},
  url       = {http://jmlr.org/papers/v17/14-510.html},
}

@InProceedings{Brault2016Random,
  author    = {Romain Brault and Markus Heinonen and Florence Buc},
  title     = {Random Fourier Features For Operator-Valued Kernels},
  booktitle = {Proceedings of The 8th Asian Conference on Machine Learning},
  year      = {2016},
  editor    = {Robert J. Durrant and Kee-Eung Kim},
  volume    = {63},
  series    = {Proceedings of Machine Learning Research},
  pages     = {110--125},
  address   = {The University of Waikato, Hamilton, New Zealand},
  month     = {16--18 Nov},
  publisher = {PMLR},
  abstract  = {Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operator-valued kernels. We propose a general principle for Operator-valued Random Fourier Feature construction relying on a generalization of Bochners theorem for translation-invariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets.},
  file      = {:Brault2016Random.pdf:PDF;Brault39.pdf:http\://proceedings.mlr.press/v63/Brault39.pdf:PDF},
  timestamp = {2018.05.16},
  url       = {http://proceedings.mlr.press/v63/Brault39.html},
}

@Article{Minh2016Operator,
  author      = {Ha Quang Minh},
  title       = {Operator-Valued Bochner Theorem, Fourier Feature Maps for Operator-Valued Kernels, and Vector-Valued Learning},
  year        = {2016},
  abstract    = {This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.},
  date        = {2016-08-19},
  eprint      = {1608.05639v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Minh2016Operator.pdf:PDF},
  keywords    = {cs.LG},
  timestamp   = {2018.05.14},
}

@Article{Berrendero2018RKHS,
  author    = {Jos{\'{e}} R. Berrendero and Beatriz Bueno-Larraz and Antonio Cuevas},
  title     = {An {RKHS} model for variable selection in functional linear regression},
  journal   = {Journal of Multivariate Analysis},
  year      = {2018},
  month     = {apr},
  doi       = {10.1016/j.jmva.2018.04.008},
  publisher = {Elsevier {BV}},
  timestamp = {2018.05.14},
}

@Book{Lukic1996Stochastic,
  title     = {Stochastic processes having sample paths in reproducing kernel {H}ilbert spaces with an application to white noise analysis},
  publisher = {ProQuest LLC, Ann Arbor, MI},
  year      = {1996},
  author    = {Luki\'c, Milan Nikola},
  isbn      = {978-0591-24091-7},
  note      = {Thesis (Ph.D.)--The University of Wisconsin - Milwaukee},
  file      = {:Lukic1996Stochastic.pdf:PDF},
  mrclass   = {Thesis},
  mrnumber  = {2695316},
  pages     = {209},
  timestamp = {2018.05.14},
  url       = {http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqdiss&rft_dat=xri:pqdiss:9716177},
}

@Article{Lukic2001Stochastic,
  author    = {Luki\'c, Milan N. and Beder, Jay H.},
  title     = {Stochastic processes with sample paths in reproducing kernel {H}ilbert spaces},
  journal   = {Trans. Amer. Math. Soc.},
  year      = {2001},
  volume    = {353},
  number    = {10},
  pages     = {3945--3969},
  issn      = {0002-9947},
  doi       = {10.1090/S0002-9947-01-02852-5},
  file      = {:Lukic2001Stochastic.pdf:PDF},
  fjournal  = {Transactions of the American Mathematical Society},
  mrclass   = {60G12 (28C20 46E22 46N30 60B11 60G15 60G17)},
  mrnumber  = {1837215},
  timestamp = {2018.05.14},
  url       = {https://doi.org/10.1090/S0002-9947-01-02852-5},
}

@Article{Manton2014primer,
  author     = {Manton, Jonathan H. and Amblard, Pierre-Olivier},
  title      = {A primer on reproducing kernel {H}ilbert spaces},
  journal    = {Found. Trends Signal Process.},
  year       = {2014},
  volume     = {8},
  number     = {1-2, \copyright{}2015},
  pages      = {1--126},
  issn       = {1932-8346},
  file       = {:Manton2014primer.pdf:PDF},
  fjournal   = {Foundations and Trends${}^\circledR$ in Signal Processing},
  mrclass    = {94A12 (42B35)},
  mrnumber   = {3438277},
  mrreviewer = {Joseph D. Lakey},
  timestamp  = {2018.05.14},
}

@Article{Steinwart2014Convergence,
  author      = {Ingo Steinwart},
  title       = {Convergence Types and Rates in Generic Karhunen-Love Expansions with Applications to Sample Path Properties},
  year        = {2014},
  abstract    = {We establish a Karhunen-Lo`eve expansion for generic centered, second order stochastic processes, which does not rely on topological assumptions. We further investigate in which norms the expansion converges and derive exact average rates of convergence for these norms. For Gaussian processes we additionally prove certain sharpness results in terms of the norm. Moreover, we show that the generic Karhunen-Lo`eve expansion can in some situations be used to construct reproducing kernel Hilbert spaces (RKHSs) containing the paths of a version of the process. As applications of the general theory, we compare the smoothness of the paths with the smoothness of the functions contained in the RKHS of the covariance function and discuss some small ball probabilities. Key tools for our results are a recently shown generalization of Mercer's theorem, spectral properties of the covariance integral operator, interpolation spaces of the real method, and for the smoothness results, entropy numbers of embeddings between classical function spaces.},
  date        = {2014-03-05},
  eprint      = {1403.1040v3},
  eprintclass = {math.PR},
  eprinttype  = {arXiv},
  file        = {:Steinwart2014Convergence.pdf:PDF},
  keywords    = {math.PR},
  timestamp   = {2018.05.14},
}

@Book{Gualtierotti2015Detection,
  title      = {Detection of random signals in dependent {G}aussian noise},
  publisher  = {Springer, Cham},
  year       = {2015},
  author     = {Gualtierotti, Antonio F.},
  isbn       = {978-3-319-22314-8; 978-3-319-22315-5},
  doi        = {10.1007/978-3-319-22315-5},
  mrclass    = {94-02 (60-02 60B11 60G25 60G35 60H30 62M07 94A12)},
  mrnumber   = {3445605},
  mrreviewer = {Paul F. Bracken},
  pages      = {xxxiv+1176},
  timestamp  = {2018.05.14},
  url        = {https://doi.org/10.1007/978-3-319-22315-5},
}

@Article{Owhadi2017Separability,
  author     = {Owhadi, Houman and Scovel, Clint},
  title      = {Separability of reproducing kernel spaces},
  journal    = {Proc. Amer. Math. Soc.},
  year       = {2017},
  volume     = {145},
  number     = {5},
  pages      = {2131--2138},
  issn       = {0002-9939},
  doi        = {10.1090/proc/13354},
  file       = {:Owhadi2017Separability.pdf:PDF},
  fjournal   = {Proceedings of the American Mathematical Society},
  mrclass    = {46E22 (46E15)},
  mrnumber   = {3611326},
  mrreviewer = {George Chailos},
  timestamp  = {2018.05.14},
  url        = {https://doi.org/10.1090/proc/13354},
}

@Article{Ma2016Joint,
  author    = {Jing Ma and George Michailidis},
  title     = {Joint Structural Estimation of Multiple Graphical Models},
  journal   = {Journal of Machine Learning Research},
  year      = {2016},
  volume    = {17},
  number    = {166},
  pages     = {1-48},
  file      = {:Ma2016Joint.pdf:PDF},
  timestamp = {2018.05.16},
  url       = {http://jmlr.org/papers/v17/15-656.html},
}

@Article{Majumdar2018Joint,
  author      = {Subhabrata Majumdar and George Michailidis},
  title       = {Joint Estimation and Inference for Data Integration Problems based on Multiple Multi-layered Gaussian Graphical Models},
  journal     = {arxiv: 1803.03348},
  year        = {2018},
  abstract    = {The rapid development of high-throughput technologies has enabled the generation of data from biological or disease processes that span multiple layers, like genomic, proteomic or metabolomic data, and further pertain to multiple sources, like disease subtypes or experimental conditions. In this work, we propose a general statistical framework based on Gaussian graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across different layers containing data on molecular compartments) integration of information in such datasets. We start with decomposing the multi-layer problem into a series of two-layer problems. For each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. We use a combination of neighborhood selection and group-penalized regression to obtain sparse estimates of all model parameters. Following this, we develop a debiasing technique and asymptotic distributions of inter-layer directed edge weights that utilize already computed neighborhood selection coefficients for nodes in the upper layer. Subsequently, we establish global and simultaneous testing procedures for these edge weights. Performance of the proposed methodology is evaluated on synthetic data.},
  date        = {2018-03-09},
  eprint      = {1803.03348v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Majumdar2018Joint.pdf:PDF},
  keywords    = {stat.ML, math.ST, stat.ME, stat.TH},
  timestamp   = {2018.08.21},
}

@Article{Plan2016generalized,
  author     = {Plan, Yaniv and Vershynin, Roman},
  title      = {The generalized {L}asso with non-linear observations},
  journal    = {IEEE Trans. Inform. Theory},
  year       = {2016},
  volume     = {62},
  number     = {3},
  pages      = {1528--1537},
  issn       = {0018-9448},
  doi        = {10.1109/TIT.2016.2517008},
  file       = {:Plan2016generalized.pdf:PDF},
  fjournal   = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass    = {94A12},
  mrnumber   = {3472264},
  mrreviewer = {Mikhail A. Lifshits},
  timestamp  = {2018.05.18},
  url        = {https://doi.org/10.1109/TIT.2016.2517008},
}

@InCollection{Thrampoulidis2015LASSO,
  author    = {Chrtistos Thrampoulidis and Ehsan Abbasi and Babak Hassibi},
  title     = {LASSO with Non-linear Measurements is Equivalent to One With Linear Measurements},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {3420--3428},
  file      = {:Thrampoulidis2015LASSO.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5739-lasso-with-non-linear-measurements-is-equivalent-to-one-with-linear-measurements.pdf},
}

@Article{Dobra2004Sparse,
  author   = {Dobra, Adrian and Hans, Chris and Jones, Beatrix and Nevins, Joseph R. and Yao, Guang and West, Mike},
  title    = {Sparse graphical models for exploring gene expression data},
  journal  = {J. Multivariate Anal.},
  year     = {2004},
  volume   = {90},
  number   = {1},
  pages    = {196--212},
  issn     = {0047-259X},
  doi      = {10.1016/j.jmva.2004.02.009},
  fjournal = {Journal of Multivariate Analysis},
  mrclass  = {62P10 (62-09 62D05 62F15)},
  mrnumber = {2064941},
  url      = {https://doi.org/10.1016/j.jmva.2004.02.009},
}

@InProceedings{Hartemink2001Using,
  author    = {Alexander J. Hartemink and David K. Gifford and Tommi S. Jaakkola and Richard A. Young},
  title     = {Using Graphical Models and Genomic Expression Data to Statistically Validate Models of Genetic Regulatory Networks},
  booktitle = {Proceedings of the 6th Pacific Symposium on Biocomputing, {PSB} 2001, Hawaii, USA, January 3-7, 2001},
  year      = {2001},
  editor    = {Russ B. Altman and A. Keith Dunker and Lawrence Hunter and Teri E. Klein},
  pages     = {422--433},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/psb/HarteminkGJY01},
  timestamp = {Mon, 22 Jun 2015 17:24:46 +0200},
  url       = {http://psb.stanford.edu/psb-online/proceedings/psb01/hartemink.pdf},
}

@Article{Liu2014Direct,
  author   = {Liu, Song and Quinn, John A. and Gutmann, Michael U. and Suzuki, Taiji and Sugiyama, Masashi},
  title    = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  journal  = {Neural Comput.},
  year     = {2014},
  volume   = {26},
  number   = {6},
  pages    = {1169--1197},
  issn     = {0899-7667},
  doi      = {10.1162/NECO_a_00589},
  fjournal = {Neural Computation},
  mrclass  = {62H12 (62G05 62J07 62M02 90C90)},
  mrnumber = {3234638},
  url      = {https://doi.org/10.1162/NECO_a_00589},
}

@Article{Liu2013Change,
  author    = {Song Liu and Makoto Yamada and Nigel Collier and Masashi Sugiyama},
  title     = {Change-point detection in time-series data by relative density-ratio estimation},
  journal   = {Neural Networks},
  year      = {2013},
  volume    = {43},
  pages     = {72--83},
  month     = {jul},
  doi       = {10.1016/j.neunet.2013.01.012},
  publisher = {Elsevier {BV}},
}

@Book{MacKay2003Information,
  title      = {Information theory, inference and learning algorithms},
  publisher  = {Cambridge University Press, New York},
  year       = {2003},
  author     = {MacKay, David J. C.},
  isbn       = {0-521-64298-1},
  mrclass    = {94-01 (68P30 68T05 92C20 94A15)},
  mrnumber   = {2012999},
  mrreviewer = {Robert M. Baer},
  pages      = {xii+628},
}

@Article{Shimodaira2000Improving,
  author   = {Shimodaira, Hidetoshi},
  title    = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
  journal  = {J. Statist. Plann. Inference},
  year     = {2000},
  volume   = {90},
  number   = {2},
  pages    = {227--244},
  issn     = {0378-3758},
  doi      = {10.1016/S0378-3758(00)00115-4},
  fjournal = {Journal of Statistical Planning and Inference},
  mrclass  = {62B10 (62F15)},
  mrnumber = {1795598},
  url      = {https://doi.org/10.1016/S0378-3758(00)00115-4},
}

@Article{Sugiyama2011Least,
  author    = {Masashi Sugiyama and Taiji Suzuki and Yuta Itoh and Takafumi Kanamori and Manabu Kimura},
  title     = {Least-squares two-sample test},
  journal   = {Neural Networks},
  year      = {2011},
  volume    = {24},
  number    = {7},
  pages     = {735--751},
  month     = {sep},
  doi       = {10.1016/j.neunet.2011.04.003},
  publisher = {Elsevier {BV}},
}

@Book{Sugiyama2012Density,
  title      = {Density ratio estimation in machine learning},
  publisher  = {Cambridge University Press, Cambridge},
  year       = {2012},
  author     = {Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  isbn       = {978-0-521-19017-6},
  note       = {With a foreword by Thomas G. Dietterich},
  doi        = {10.1017/CBO9781139035613},
  mrclass    = {62G05 (62G20 62H30 68-02 68T05)},
  mrnumber   = {2895762},
  mrreviewer = {Pierre Alquier},
  pages      = {xii+329},
  url        = {https://doi.org/10.1017/CBO9781139035613},
}

@Article{Sugiyama2007Covariate,
  author     = {Sugiyama, Masashi and Krauledat, Matthias and M\"{u}ller, Klaus-Robert},
  title      = {Covariate Shift Adaptation by Importance Weighted Cross Validation},
  journal    = {J. Mach. Learn. Res.},
  year       = {2007},
  volume     = {8},
  pages      = {985--1005},
  month      = dec,
  issn       = {1532-4435},
  acmid      = {1390324},
  issue_date = {12/1/2007},
  numpages   = {21},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=1314498.1390324},
}

@Book{Vapnik1998Statistical,
  title      = {Statistical learning theory},
  publisher  = {John Wiley \& Sons, Inc., New York},
  year       = {1998},
  author     = {Vapnik, Vladimir N.},
  series     = {Adaptive and Learning Systems for Signal Processing, Communications, and Control},
  isbn       = {0-471-03003-1},
  note       = {A Wiley-Interscience Publication},
  mrclass    = {62G07 (68T05)},
  mrnumber   = {1641250},
  mrreviewer = {Ilya S. Molchanov},
  pages      = {xxvi+736},
}

@Article{Janzamin2015Beating,
  author      = {Majid Janzamin and Hanie Sedghi and Anima Anandkumar},
  title       = {Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods},
  journal     = {arxiv: 1506.08473},
  year        = {2015},
  abstract    = {Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for learnability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we propose a computationally efficient method with guaranteed risk bounds for training neural networks with one hidden layer.},
  date        = {2015-06-28},
  eprint      = {1506.08473v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.08473v3:PDF},
  keywords    = {cs.LG, cs.NE, stat.ML},
  timestamp   = {2018.05.24},
}

@Article{Yang2017Misspecified,
  author      = {Zhuoran Yang and Lin F. Yang and Ethan X. Fang and Tuo Zhao and Zhaoran Wang and Matey Neykov},
  title       = {Misspecified Nonconvex Statistical Optimization for Phase Retrieval},
  journal     = {arxiv: 1712.06245},
  year        = {2017},
  abstract    = {Existing nonconvex statistical optimization theory and methods crucially rely on the correct specification of the underlying "true" statistical models. To address this issue, we take a first step towards taming model misspecification by studying the high-dimensional sparse phase retrieval problem with misspecified link functions. In particular, we propose a simple variant of the thresholded Wirtinger flow algorithm that, given a proper initialization, linearly converges to an estimator with optimal statistical accuracy for a broad family of unknown link functions. We further provide extensive numerical experiments to support our theoretical findings.},
  date        = {2017-12-18},
  eprint      = {1712.06245v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Yang2017Misspecified.pdf:PDF},
  keywords    = {stat.ML, cs.LG, math.OC},
}

@Article{Jain2017Non,
  author    = {Prateek Jain and Purushottam Kar},
  title     = {Non-convex Optimization for Machine Learning},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  year      = {2017},
  volume    = {10},
  number    = {3-4},
  pages     = {142--336},
  doi       = {10.1561/2200000058},
  file      = {:Jain2017Non.pdf:PDF},
  publisher = {Now Publishers},
  timestamp = {2018.07.16},
}

@Article{Lin2016Penalized,
  author    = {Lin, Jiahe and Basu, Sumanta and Banerjee, Moulinath and Michailidis, George},
  title     = {Penalized maximum likelihood estimation of multi-layered {G}aussian graphical models},
  journal   = {J. Mach. Learn. Res.},
  year      = {2016},
  volume    = {17},
  pages     = {Paper No. 146, 51},
  issn      = {1532-4435},
  file      = {:Lin2016Penalized.pdf:PDF},
  fjournal  = {Journal of Machine Learning Research (JMLR)},
  mrclass   = {62H12 (05C90 62J07)},
  mrnumber  = {3555037},
  timestamp = {2018.08.21},
}

@Article{Li2007Finding,
  author    = {Ker-Chau Li and Aarno Palotie and Shinsheng Yuan and Denis Bronnikov and Daniel Chen and Xuelian Wei and Oi-Wa Choi and Janna Saarela and Leena Peltonen},
  title     = {Finding disease candidate genes by liquid association},
  journal   = {Genome Biology},
  year      = {2007},
  volume    = {8},
  number    = {10},
  pages     = {R205},
  doi       = {10.1186/gb-2007-8-10-r205},
  publisher = {Springer Nature},
  timestamp = {2018.08.21},
}

@Article{Chernozhukov2015Comparison,
  author     = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
  title      = {Comparison and anti-concentration bounds for maxima of {G}aussian random vectors},
  journal    = {Probab. Theory Related Fields},
  year       = {2015},
  volume     = {162},
  number     = {1-2},
  pages      = {47--70},
  issn       = {0178-8051},
  doi        = {10.1007/s00440-014-0565-9},
  file       = {:Chernozhukov2015Comparison.pdf:PDF},
  fjournal   = {Probability Theory and Related Fields},
  mrclass    = {60G15 (60E15 60G70 62E20)},
  mrnumber   = {3350040},
  mrreviewer = {Eugen P\u alt\u anea},
  timestamp  = {2018.08.23},
  url        = {https://doi.org/10.1007/s00440-014-0565-9},
}

@Article{Chen2018Gaussian,
  author    = {Chen, Xiaohui},
  title     = {Gaussian and bootstrap approximations for high-dimensional {U}-statistics and their applications},
  journal   = {Ann. Statist.},
  year      = {2018},
  volume    = {46},
  number    = {2},
  pages     = {642--678},
  issn      = {0090-5364},
  doi       = {10.1214/17-AOS1563},
  file      = {:Chen2018Gaussian.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  mrclass   = {62E17 (62F40)},
  mrnumber  = {3782380},
  timestamp = {2018.08.23},
  url       = {https://doi.org/10.1214/17-AOS1563},
}

@Article{Chen2017Jackknife,
  author      = {Xiaohui Chen and Kengo Kato},
  title       = {Jackknife multiplier bootstrap: finite sample approximations to the $U$-process supremum with applications},
  journal     = {arxiv: 1708.02705},
  year        = {2017},
  abstract    = {This paper is concerned with finite sample approximations to the supremum of a non-degenerate $U$-process of a general order indexed by a function class. We are primarily interested in situations where the function class as well as the underlying distribution change with the sample size, and the $U$-process itself is not weakly convergent as a process. Such situations arise in a variety of modern statistical problems. We first consider Gaussian approximations, namely, approximate the $U$-process supremum by the supremum of a Gaussian process, and derive coupling and Kolmogorov distance bounds. Such Gaussian approximations are, however, not often directly usable in statistical problems since the covariance function of the approximating Gaussian process is unknown. This motivates us to study bootstrap-type approximations to the $U$-process supremum. We propose a novel jackknife multiplier bootstrap (JMB) tailored to the $U$-process, and derive coupling and Kolmogorov distance bounds for the proposed JMB method. All these results are non-asymptotic, and established under fairly general conditions on function classes and underlying distributions. Key technical tools in the proofs are new local maximal inequalities for $U$-processes, which may be useful in other contexts. We also discuss applications of the general approximation results to testing for qualitative features of nonparametric functions based on generalized local $U$-processes.},
  date        = {2017-08-09},
  eprint      = {http://arxiv.org/abs/1708.02705v3},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Chen2017Jackknife.pdf:PDF},
  keywords    = {math.ST, math.PR, stat.ME, stat.TH, 62E17, 62F40, 60F17, 62G10},
  timestamp   = {2018.08.23},
}

@Article{Chen2017Randomized,
  author      = {Xiaohui Chen and Kengo Kato},
  title       = {Randomized incomplete $U$-statistics in high dimensions},
  journal     = {arXiv: 1712.00771},
  year        = {2017},
  abstract    = {This paper studies inference for the mean vector of a high-dimensional $U$-statistic. In the era of Big Data, the dimension $d$ of the $U$-statistic and the sample size $n$ of the observations tend to be both large, and the computation of the $U$-statistic is prohibitively demanding. Data-dependent inferential procedures such as the empirical bootstrap for $U$-statistics is even more computationally expensive. To overcome such computational bottleneck, incomplete $U$-statistics obtained by sampling fewer terms of the $U$-statistic are attractive alternatives. In this paper, we introduce randomized incomplete $U$-statistics with sparse weights whose computational cost can be made independent of the order of the $U$-statistic. We derive non-asymptotic Gaussian approximation error bounds for the randomized incomplete $U$-statistics in high dimensions, namely in cases where the dimension $d$ is possibly much larger than the sample size $n$, for both non-degenerate and degenerate kernels. In addition, we propose generic bootstrap methods for the incomplete $U$-statistics that are computationally much less-demanding than existing bootstrap methods, and establish finite sample validity of the proposed bootstrap methods. Our methods are illustrated on the application to nonparametric testing for the pairwise independence of a high-dimensional random vector under weaker assumptions than those appearing in the literature.},
  date        = {2017-12-03},
  eprint      = {http://arxiv.org/abs/1712.00771v3},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Chen2017Randomized.pdf:PDF},
  keywords    = {math.ST, math.PR, stat.CO, stat.ME, stat.TH, 62E17, 62F40, 62H15},
  timestamp   = {2018.08.23},
}

@Article{Belloni2018High,
  author      = {Alexandre Belloni and Victor Chernozhukov and Denis Chetverikov and Christian Hansen and Kengo Kato},
  title       = {High-Dimensional Econometrics and Regularized {GMM}},
  journal     = {arxiv: 1806.01888},
  year        = {2018},
  abstract    = {This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. We first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. Within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. We also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. We then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. The presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results.},
  date        = {2018-06-05},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Belloni2018High.pdf:PDF},
  keywords    = {math.ST, econ.EM, stat.TH},
  timestamp   = {2018.08.23},
}

@Article{Lopes2018Bootstrapping,
  author      = {Miles E. Lopes and Zhenhua Lin and Hans-Georg Mueller},
  title       = {Bootstrapping Max Statistics in High Dimensions: Near-Parametric Rates Under Weak Variance Decay and Application to Functional Data Analysis},
  journal     = {arxiv: 1807.04429},
  year        = {2018},
  abstract    = {In recent years, bootstrap methods have drawn attention for their ability to approximate the laws of "max statistics" in high-dimensional problems. A leading example of such a statistic is the coordinate-wise maximum of a sample average of $n$ random vectors in $\mathbb{R}^p$. Existing results for this statistic show that the bootstrap can work when $n\ll p$, and rates of approximation (in Kolmogorov distance) have been obtained with only logarithmic dependence in $p$. Nevertheless, one of the challenging aspects of this setting is that established rates tend to scale like $n^{-1/6}$ as a function of $n$. The main purpose of this paper is to demonstrate that improvement in rate is possible when extra model structure is available. Specifically, we show that if the coordinate-wise variances of the observations exhibit decay, then a nearly $n^{-1/2}$ rate can be achieved, independent of $p$. Furthermore, a surprising aspect of this dimension-free rate is that it holds even when the decay is very weak. As a numerical illustration, we show how these ideas can be used in the context of functional data analysis to construct simultaneous confidence intervals for the Fourier coefficients of a mean function.},
  date        = {2018-07-12},
  eprint      = {http://arxiv.org/abs/1807.04429v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Lopes2018Bootstrapping.pdf:PDF},
  keywords    = {math.ST, stat.ME, stat.TH},
  timestamp   = {2018.08.23},
}

@Article{Ballout2017Structure,
  author      = {Nadim Ballout and Vivian Viallon},
  title       = {Structure estimation of binary graphical models on stratified data: application to the description of injury tables for victims of road accidents},
  journal     = {arxiv: 1709.10298},
  year        = {2017},
  abstract    = {Graphical models are used in many applications such as medical diagnostic, computer security, etc. More and more often, the estimation of such models has to be performed on several predefined strata of the whole population. For instance, in epidemiology and clinical research, strata are often defined according to age, gender, treatment or disease type, etc. In this article, we propose new approaches aimed at estimating binary graphical models on such strata. Our approaches are obtained by combining well-known methods when estimating one single binary graphical model, with penalties encouraging structured sparsity, and which have recently been shown appropriate when dealing with stratified data. Empirical comparions on synthetic data highlight that our approaches generally outperform the competitors we considered. An application is provided where we study associations among injuries suffered by victims of road accidents according to road user type.},
  date        = {2017-09-29},
  eprint      = {http://arxiv.org/abs/1709.10298v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Ballout2017Structure.pdf:PDF},
  keywords    = {stat.ME, stat.AP},
  timestamp   = {2018.08.23},
}

@Article{Chen2017pseudolikelihood,
  author    = {Y. Chen and J. Ning and Y. Ning and K.-Y. Liang and K. Bandeen-Roche},
  title     = {On pseudolikelihood inference for semiparametric models with boundary problems},
  journal   = {Biometrika},
  year      = {2017},
  volume    = {104},
  number    = {1},
  pages     = {165--179},
  month     = {feb},
  doi       = {10.1093/biomet/asw072},
  file      = {:Chen2017pseudolikelihood.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
  timestamp = {2018.09.11},
}

@Article{Hansen2014Integrated,
  author    = {Bruce E. Hansen},
  title     = {The Integrated Mean Squared Error of Series Regression and a Rosenthal Hilbert-Space Inequality},
  journal   = {Econometric Theory},
  year      = {2014},
  volume    = {31},
  number    = {02},
  pages     = {337--361},
  month     = {aug},
  doi       = {10.1017/s0266466614000322},
  file      = {:Hansen2014Integrated.pdf:PDF},
  publisher = {Cambridge University Press ({CUP})},
}

@InCollection{Pena2017Sharp,
  author    = {Victor H. de la Pe{\~{n}}a and Rustam Ibragimov},
  title     = {Sharp Probability Inequalities for Random Polynomials, Generalized Sample Cross-Moments, and Studentized Processes},
  booktitle = {Inequalities and Extremal Problems in Probability and Statistics},
  publisher = {Elsevier},
  year      = {2017},
  pages     = {159--187},
  doi       = {10.1016/b978-0-12-809818-9.00006-9},
  file      = {:Pena2017Sharp.pdf:PDF},
}

@Article{Freyberger2018Practical,
  author  = {Joachim Freyberger and Matthew A. Masten},
  title   = {A Practical Guide to Compact Infinite Dimensional Parameter Spaces},
  journal = {Econometric Reviews (to appear)},
  year    = {2018},
  file    = {:Freyberger2018Practical.pdf:PDF;:Freyberger2018Practical_supp.pdf:PDF},
}

@Article{Santos2012Inference,
  author    = {Andres Santos},
  title     = {Inference in Nonparametric Instrumental Variables With Partial Identification},
  journal   = {Econometrica},
  year      = {2012},
  volume    = {80},
  number    = {1},
  pages     = {213--275},
  doi       = {10.3982/ecta7493},
  file      = {:Santos2012Inference.pdf:PDF},
  publisher = {The Econometric Society},
}

@Article{Chen2015Optimal,
  author    = {Xiaohong Chen and Timothy M. Christensen},
  title     = {Optimal uniform convergence rates and asymptotic normality for series estimators under weak dependence and weak conditions},
  journal   = {Journal of Econometrics},
  year      = {2015},
  volume    = {188},
  number    = {2},
  pages     = {447--465},
  month     = {oct},
  doi       = {10.1016/j.jeconom.2015.03.010},
  file      = {:Chen2015Optimal.pdf:pdf},
  publisher = {Elsevier {BV}},
}

@Article{Gallant1987Semi,
  author     = {Gallant, A. Ronald and Nychka, Douglas W.},
  title      = {Semi-nonparametric maximum likelihood estimation},
  journal    = {Econometrica},
  year       = {1987},
  volume     = {55},
  number     = {2},
  pages      = {363--390},
  issn       = {0012-9682},
  doi        = {10.2307/1913241},
  file       = {:Gallant1987Semi.pdf:pdf},
  fjournal   = {Econometrica. Journal of the Econometric Society},
  mrclass    = {62P20 (62G05 62J02)},
  mrnumber   = {882100},
  mrreviewer = {M. G. Akritas},
  url        = {https://doi.org/10.2307/1913241},
}

@Article{Belloni2015Some,
  author    = {Alexandre Belloni and Victor Chernozhukov and Denis Chetverikov and Kengo Kato},
  title     = {Some new asymptotic theory for least squares series: Pointwise and uniform results},
  journal   = {Journal of Econometrics},
  year      = {2015},
  volume    = {186},
  number    = {2},
  pages     = {345--366},
  month     = {jun},
  doi       = {10.1016/j.jeconom.2015.02.014},
  file      = {:Belloni2015Some.pdf:pdf},
  publisher = {Elsevier {BV}},
}

@TechReport{Hansen2016Unified,
  author      = {Bruce E. Hansen},
  title       = {A Unified Asymptotic Distribution Theory for Parametric and Non-Parametric Least Squares},
  institution = {University of Wisconsin},
  year        = {2016},
  file        = {:Hansen2016Unified.pdf:pdf},
}

@Article{Han2017Estimation,
  author      = {Yanjun Han and Jiantao Jiao and Rajarshi Mukherjee and Tsachy Weissman},
  title       = {On Estimation of $L_{r}$-Norms in Gaussian White Noise Models},
  journal     = {arXiv: 1710.03863},
  year        = {2017},
  abstract    = {We provide a complete picture of asymptotically minimax estimation of $L_r$-norms (for any $r\ge 1$) of the mean in Gaussian white noise model over Nikolskii-Besov spaces. In this regard, we complement the work of Lepski, Nemirovski and Spokoiny (1999), who considered the cases of $r=1$ (with poly-logarithmic gap between upper and lower bounds) and $r$ even (with asymptotically sharp upper and lower bounds) over H\"{o}lder spaces. We additionally consider the case of asymptotically adaptive minimax estimation and demonstrate a difference between even and non-even $r$ in terms of an investigator's ability to produce asymptotically adaptive minimax estimators without paying a penalty.},
  date        = {2017-10-11},
  eprint      = {http://arxiv.org/abs/1710.03863v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Han2017Estimation.pdf:PDF},
  keywords    = {math.ST, cs.LG, stat.TH},
}

@TechReport{Dong2018Series,
  author      = {Chaohua Dong and Jiti Gao and Bin Peng},
  title       = {{Series estimation for single-index models under constraints}},
  institution = {Monash University, Department of Econometrics and Business Statistics},
  year        = {2018},
  type        = {Monash Econometrics and Business Statistics Working Papers},
  number      = {5/18},
  abstract    = {This paper discusses a semiparametric single-index model. The link function is allowed to be unbounded and has unbounded support that fill the gap in the literature. The link function is treated as a point in an infinitely many dimensional function space which enables us to derive the estimates for the index parameter and the link function simultaneously. This approach is different from the profile method commonly used in the literature. The estimator is derived from an optimization with the constraint of an identification condition for the index parameter, which solves an important problem in the literature of single-index models. In addition, making use of a property of Hermite orthogonal polynomials, an explicit estimator for the index parameter is obtained. Asymptotic properties of the two estimators of the index parameter are established. Their efficiency is discussed in some special cases as well. The finite sample properties of the two estimators are demonstrated through an extensive Monte Carlo study and an empirical example.},
  file        = {:Dong2018Series.pdf:pdf},
  keywords    = {asymptotic theory; closed-form estimation; cross-sectional model; Hermite series expansion.},
  url         = {https://ideas.repec.org/p/msh/ebswps/2018-5.html},
}

@Article{Serdyukova2018Lower,
  author    = {Nora Serdyukova},
  title     = {Lower bounds in estimation at a point under multi-index constraint},
  journal   = {Statistics {\&} Probability Letters},
  year      = {2018},
  volume    = {138},
  pages     = {183--189},
  month     = {jul},
  doi       = {10.1016/j.spl.2018.02.062},
  file      = {:Serdyukova2018Lower.pdf:pdf},
  publisher = {Elsevier {BV}},
}

@Unpublished{Comte2018Regression,
  author              = {Comte, Fabienne and Genon-Catalot, V},
  title               = {{Regression function estimation on non compact support as a partly inverse problem}},
  note                = {working paper or preprint},
  month               = Apr,
  year                = {2018},
  file                = {:Comte2018Regression.pdf:PDF;NewNonCompactReg.pdf:https\://hal.archives-ouvertes.fr/hal-01690856/file/NewNonCompactReg.pdf:PDF},
  hal_id              = {hal-01690856},
  hal_local_reference = {MAP5 2018-01},
  hal_version         = {v2},
  url                 = {https://hal.archives-ouvertes.fr/hal-01690856},
}

@Article{Belomestny2017Sobolev,
  author    = {Denis Belomestny and Fabienne Comte and Valentine Genon-Catalot},
  title     = {Sobolev-Hermite versus Sobolev nonparametric density estimation on {\textdollar}{\textdollar}$\lbrace${\textbackslash}mathbb $\lbrace$R$\rbrace$$\rbrace${\textdollar}{\textdollar} R},
  journal   = {Annals of the Institute of Statistical Mathematics},
  year      = {2017},
  month     = {oct},
  doi       = {10.1007/s10463-017-0624-y},
  file      = {:Belomestny2017Sobolev.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Comte2015Adaptive,
  author   = {Comte, Fabienne and Genon-Catalot, Valentine},
  title    = {Adaptive {L}aguerre density estimation for mixed {P}oisson models},
  journal  = {Electron. J. Stat.},
  year     = {2015},
  volume   = {9},
  number   = {1},
  pages    = {1113--1149},
  issn     = {1935-7524},
  doi      = {10.1214/15-EJS1028},
  file     = {:Comte2015Adaptive.pdf:pdf},
  fjournal = {Electronic Journal of Statistics},
  mrclass  = {62G07 (62C20 62G09)},
  mrnumber = {3352069},
  url      = {https://doi.org/10.1214/15-EJS1028},
}

@Article{Comte2017Laplace,
  author   = {Comte, Fabienne and Cuenod, Charles-A. and Pensky, Marianna and Rozenholc, Yves},
  title    = {Laplace deconvolution on the basis of time domain data and its application to dynamic contrast-enhanced imaging},
  journal  = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year     = {2017},
  volume   = {79},
  number   = {1},
  pages    = {69--94},
  issn     = {1369-7412},
  doi      = {10.1111/rssb.12159},
  file     = {:Comte2017Laplace.pdf:pdf},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62G05 (62H35 62M15)},
  mrnumber = {3597965},
  url      = {https://doi.org/10.1111/rssb.12159},
}

@Article{Mabon2017Adaptive,
  author    = {Gwennalle Mabon},
  title     = {Adaptive Deconvolution on the Non-negative Real Line},
  journal   = {Scandinavian Journal of Statistics},
  year      = {2017},
  volume    = {44},
  number    = {3},
  pages     = {707--740},
  month     = {mar},
  doi       = {10.1111/sjos.12272},
  file      = {:Mabon2017Adaptive.pdf:PDF},
  publisher = {Wiley},
}

@Article{Comte2018Laguerre,
  author    = {F. Comte and V. Genon-Catalot},
  title     = {Laguerre and Hermite bases for inverse problems},
  journal   = {Journal of the Korean Statistical Society},
  year      = {2018},
  volume    = {47},
  number    = {3},
  pages     = {273--296},
  month     = {sep},
  doi       = {10.1016/j.jkss.2018.03.001},
  file      = {:Comte2018Laguerre.pdf:pdf},
  publisher = {Elsevier {BV}},
}

@Article{Belomestny2016Nonparametric,
  author   = {Belomestny, Denis and Comte, Fabienne and Genon-Catalot, Valentine},
  title    = {Nonparametric {L}aguerre estimation in the multiplicative censoring model},
  journal  = {Electron. J. Stat.},
  year     = {2016},
  volume   = {10},
  number   = {2},
  pages    = {3114--3152},
  issn     = {1935-7524},
  doi      = {10.1214/16-EJS1203},
  file     = {:Belomestny2016Nonparametric:PDF;:Belomestny2016Nonparametric_corr.pdf:pdf},
  fjournal = {Electronic Journal of Statistics},
  mrclass  = {62G07 (62N01)},
  mrnumber = {3571964},
  url      = {https://doi.org/10.1214/16-EJS1203},
}

@InProceedings{Yu2018Learning,
  author    = {Yu, Ming and Gupta, Varun and Kolar, Mladen},
  title     = {Learning Influence-Receptivity Network Structure with Guarantee},
  booktitle = {Proceedings of Machine Learning Research},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1476--1485},
  month     = {16--18 Apr},
  publisher = {PMLR},
  abstract  = {Traditional works on community detection from observations of information cascade assume that a single adjacency matrix parametrizes all the observed cascades.  However, in reality the connection structure usually does not stay the same across cascades.  For example, different people have different topics of interest, therefore the connection structure depends on the information/topic content of the cascade.  In this paper we consider the case where we observe a sequence of noisy adjacency matrices triggered by information/events with different topic distributions.  We propose a novel latent model using the intuition that a connection is more likely to exist between two nodes if they are interested in similar topics, which are common with the information/event.  Specifically, we endow each node with two node-topic vectors: an influence vector that measures how influential/authoritative they are on each topic; and a receptivity vector that measures how receptive/susceptible they are to each topic.  We show how these two node-topic structures can be estimated from observed adjacency matrices with theoretical guarantee on estimation error, in cases where the topic distributions of the information/events are known, as well as when they are unknown. Experiments on synthetic and real data demonstrate the effectiveness of our model and superior performance compared to state-of-the-art methods.},
  file      = {yu19c.pdf:http\://proceedings.mlr.press/v89/yu19c/yu19c.pdf:PDF},
  timestamp = {2019.05.16},
  url       = {http://proceedings.mlr.press/v89/yu19c.html},
}

@Article{Drton2016Computation,
  author      = {Mathias Drton and Christopher Fox and Y. Samuel Wang},
  title       = {Computation of maximum likelihood estimates in cyclic structural equation models},
  abstract    = {Software for computation of maximum likelihood estimates in linear structural equation models typically employs general techniques from non-linear optimization, such as quasi-Newton methods. In practice, careful tuning of initial values is often required to avoid convergence issues. As an alternative approach, we propose a block-coordinate descent method that cycles through the considered variables, updating only the parameters related to a given variable in each step. We show that the resulting block update problems can be solved in closed form even when the structural equation model comprises feedback cycles. Furthermore, we give a characterization of the models for which the block-coordinate descent algorithm is well-defined, meaning that for generic data and starting values all block optimization problems admit a unique solution. For the characterization, we represent each model by its mixed graph (also known as path diagram), which leads to criteria that can be checked in time that is polynomial in the number of considered variables.},
  date        = {2016-10-11},
  eprint      = {http://arxiv.org/abs/1610.03434v1},
  eprintclass = {stat.CO},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1610.03434v1:PDF},
  keywords    = {stat.CO},
}

@InProceedings{Zhu2018Distributed,
  author    = {Yuancheng Zhu and John Lafferty},
  title     = {Distributed Nonparametric Regression under Communication Constraints},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  year      = {2018},
  editor    = {Jennifer G. Dy and Andreas Krause},
  volume    = {80},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {6004--6012},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/ZhuL18},
  file      = {:Zhu2018Distributed.pdf:PDF},
  timestamp = {2018.09.20},
  url       = {http://proceedings.mlr.press/v80/zhu18a.html},
}

@Article{Xia1999single,
  author   = {Xia, Yingcun and Li, W. K.},
  title    = {On single-index coefficient regression models},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {1999},
  volume   = {94},
  number   = {448},
  pages    = {1275--1285},
  issn     = {0162-1459},
  doi      = {10.2307/2669941},
  file     = {:Xia1999single.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62M10 (62F12)},
  mrnumber = {1731489},
  url      = {https://doi.org/10.2307/2669941},
}

@Article{Xue2012Empirical,
  author    = {Xue, Liugen and Wang, Qihua},
  title     = {Empirical likelihood for single-index varying-coefficient models},
  journal   = {Bernoulli},
  year      = {2012},
  volume    = {18},
  number    = {3},
  pages     = {836--856},
  issn      = {1350-7265},
  doi       = {10.3150/11-BEJ365},
  file      = {:Xue2012Empirical.pdf:PDF},
  fjournal  = {Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability},
  mrclass   = {62J02 (62F12 62G05 62G20)},
  mrnumber  = {2948904},
  timestamp = {2018.10.04},
  url       = {https://doi.org/10.3150/11-BEJ365},
}

@InCollection{Stein2004Use,
  author    = {Stein, Charles and Diaconis, Persi and Holmes, Susan and Reinert, Gesine},
  title     = {Use of exchangeable pairs in the analysis of simulations},
  booktitle = {Stein's method: expository lectures and applications},
  publisher = {Inst. Math. Statist., Beachwood, OH},
  year      = {2004},
  volume    = {46},
  series    = {IMS Lecture Notes Monogr. Ser.},
  pages     = {1--26},
  doi       = {10.1214/lnms/1196283797},
  file      = {:Stein2004Use.pdf:PDF},
  mrclass   = {65C05 (62M05)},
  mrnumber  = {2118600},
  url       = {https://doi.org/10.1214/lnms/1196283797},
}

@Article{Carroll1997Generalized,
  author   = {Carroll, R. J. and Fan, Jianqing and Gijbels, Ir\`ene and Wand, M. P.},
  title    = {Generalized partially linear single-index models},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {1997},
  volume   = {92},
  number   = {438},
  pages    = {477--489},
  issn     = {0162-1459},
  doi      = {10.2307/2965697},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62J12},
  mrnumber = {1467842},
  url      = {https://doi.org/10.2307/2965697},
}

@Article{Chen1991Estimation,
  author   = {Chen, Hung},
  title    = {Estimation of a projection-pursuit type regression model},
  journal  = {Ann. Statist.},
  year     = {1991},
  volume   = {19},
  number   = {1},
  pages    = {142--157},
  issn     = {0090-5364},
  doi      = {10.1214/aos/1176347974},
  fjournal = {The Annals of Statistics},
  mrclass  = {62J02 (62G05 62G20 62H40)},
  mrnumber = {1091843},
  url      = {https://doi.org/10.1214/aos/1176347974},
}

@Book{Chen2011Normal,
  title      = {Normal approximation by {S}tein's method},
  publisher  = {Springer, Heidelberg},
  year       = {2011},
  author     = {Chen, Louis H. Y. and Goldstein, Larry and Shao, Qi-Man},
  series     = {Probability and its Applications (New York)},
  isbn       = {978-3-642-15006-7},
  doi        = {10.1007/978-3-642-15007-4},
  mrclass    = {60F05 (60C05 60F10 60H07 62E20)},
  mrnumber   = {2732624},
  mrreviewer = {Anthony R\'{e}veillac},
  pages      = {xii+405},
  url        = {https://doi.org/10.1007/978-3-642-15007-4},
}

@Article{Chen2016Generalized,
  author     = {Chen, Yining and Samworth, Richard J.},
  title      = {Generalized additive and index models with shape constraints},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2016},
  volume     = {78},
  number     = {4},
  pages      = {729--754},
  issn       = {1369-7412},
  doi        = {10.1111/rssb.12137},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62G20 (62F30 62J12)},
  mrnumber   = {3534348},
  mrreviewer = {Gengsheng Qin},
  url        = {https://doi.org/10.1111/rssb.12137},
}

@InProceedings{Chwialkowski2016Kernel,
  author    = {Kacper Chwialkowski and Heiko Strathmann and Arthur Gretton},
  title     = {A Kernel Test of Goodness of Fit},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2606--2615},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Steins method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
  file      = {:Chwialkowski2016Kernel.pdf:PDF;chwialkowski16.pdf:http\://proceedings.mlr.press/v48/chwialkowski16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/chwialkowski16.html},
}

@Article{Fan2003Adaptive,
  author   = {Fan, Jianqing and Yao, Qiwei and Cai, Zongwu},
  title    = {Adaptive varying-coefficient linear models},
  journal  = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  year     = {2003},
  volume   = {65},
  number   = {1},
  pages    = {57--80},
  issn     = {1369-7412},
  doi      = {10.1111/1467-9868.00372},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62J02 (62B10 62G05)},
  mrnumber = {1959093},
  url      = {https://doi.org/10.1111/1467-9868.00372},
}

@Article{Goldstein2016Structured,
  author      = {Larry Goldstein and Stanislav Minsker and Xiaohan Wei},
  title       = {Structured signal recovery from non-linear and heavy-tailed measurements},
  journal     = {ArXiv e-prints: 1609.01025},
  year        = {2016},
  abstract    = {We study high-dimensional signal recovery from non-linear measurements with design vectors having elliptically symmetric distribution. Special attention is devoted to the situation when the unknown signal belongs to a set of low statistical complexity, while both the measurements and the design vectors are heavy-tailed. We propose and analyze a new estimator that adapts to the structure of the problem, while being robust both to the possible model misspecification characterized by arbitrary non-linearity of the measurements as well as to data corruption modeled by the heavy-tailed distributions. Moreover, this estimator has low computational complexity. Our results are expressed in the form of exponential concentration inequalities for the error of the proposed estimator. On the technical side, our proofs rely on the generic chaining methods, and illustrate the power of this approach for statistical applications. Theory is supported by numerical experiments demonstrating that our estimator outperforms existing alternatives when data is heavy-tailed.},
  date        = {2016-09-05},
  eprint      = {http://arxiv.org/abs/1609.01025v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1609.01025v2:PDF},
  keywords    = {math.ST, stat.TH, 94A12, 62G35},
}

@Article{Jiang2014Variable,
  author   = {Jiang, Bo and Liu, Jun S.},
  title    = {Variable selection for general index models via sliced inverse regression},
  journal  = {Ann. Statist.},
  year     = {2014},
  volume   = {42},
  number   = {5},
  pages    = {1751--1786},
  issn     = {0090-5364},
  doi      = {10.1214/14-AOS1233},
  fjournal = {The Annals of Statistics},
  mrclass  = {62J02 (62H25 62P10)},
  mrnumber = {3262467},
  url      = {https://doi.org/10.1214/14-AOS1233},
}

@Article{Li1991Sliced,
  author   = {Li, Ker-Chau},
  title    = {Sliced inverse regression for dimension reduction},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {1991},
  volume   = {86},
  number   = {414},
  pages    = {316--342},
  issn     = {0162-1459},
  note     = {With discussion and a rejoinder by the author},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62H25 (62J02)},
  mrnumber = {1137117},
  url      = {http://links.jstor.org/sici?sici=0162-1459(199106)86:414<316:SIRFDR>2.0.CO;2-V&origin=MSN},
}

@Article{Li1992principal,
  author     = {Li, Ker-Chau},
  title      = {On principal {H}essian directions for data visualization and dimension reduction: another application of {S}tein's lemma},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {1992},
  volume     = {87},
  number     = {420},
  pages      = {1025--1039},
  issn       = {0162-1459},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62H40},
  mrnumber   = {1209564},
  mrreviewer = {Yasunori Fujikoshi},
  url        = {http://links.jstor.org/sici?sici=0162-1459(199212)87:420<1025:OPHDFD>2.0.CO;2-J&origin=MSN},
}

@Article{Lin2017optimality,
  author      = {Qian Lin and Xinran Li and Dongming Huang and Jun S. Liu},
  title       = {On the optimality of sliced inverse regression in high dimensions},
  journal     = {ArXiv e-prints: 1701.06009},
  year        = {2017},
  abstract    = {The central subspace of a pair of random variables $(y,x) \in \mathbb{R}^{p+1}$ is the minimal subspace $\mathcal{S}$ such that $y \perp \hspace{-2mm} \perp x\mid P_{\mathcal{S}}x$. In this paper, we consider the minimax rate of estimating the central space of the multiple index models $y=f(\beta_{1}^{\tau}x,\beta_{2}^{\tau}x,...,\beta_{d}^{\tau}x,\epsilon)$ with at most $s$ active predictors where $x \sim N(0,I_{p})$. We first introduce a large class of models depending on the smallest non-zero eigenvalue $\lambda$ of $var(\mathbb{E}[x|y])$, over which we show that an aggregated estimator based on the SIR procedure converges at rate $d\wedge((sd+s\log(ep/s))/(n\lambda))$. We then show that this rate is optimal in two scenarios: the single index models; and the multiple index models with fixed central dimension $d$ and fixed $\lambda$. By assuming a technical conjecture, we can show that this rate is also optimal for multiple index models with bounded dimension of the central space. We believe that these (conditional) optimal rate results bring us meaningful insights of general SDR problems in high dimensions.},
  date        = {2017-01-21},
  eprint      = {http://arxiv.org/abs/1701.06009v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1701.06009v2:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Lin2018consistency,
  author   = {Lin, Qian and Zhao, Zhigen and Liu, Jun S.},
  title    = {On consistency and sparsity for sliced inverse regression in high dimensions},
  journal  = {Ann. Statist.},
  year     = {2018},
  volume   = {46},
  number   = {2},
  pages    = {580--610},
  issn     = {0090-5364},
  doi      = {10.1214/17-AOS1561},
  fjournal = {The Annals of Statistics},
  mrclass  = {62J02 (62H25)},
  mrnumber = {3782378},
  url      = {https://doi.org/10.1214/17-AOS1561},
}

@InProceedings{Liu2018Action,
  author    = {Hao Liu and Yihao Feng and Yi Mao and Dengyong Zhou and Jian Peng and Qiang Liu},
  title     = {Action-dependent Control Variates for Policy Optimization via Stein Identity},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  url       = {https://openreview.net/forum?id=H1mCp-ZRZ},
}

@InCollection{Liu2016Stein,
  author    = {Liu, Qiang and Wang, Dilin},
  title     = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {2378--2386},
  url       = {http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.pdf},
}

@Article{Minsker2018Sub,
  author   = {Minsker, Stanislav},
  title    = {Sub-{G}aussian estimators of the mean of a random matrix with heavy-tailed entries},
  journal  = {Ann. Statist.},
  year     = {2018},
  volume   = {46},
  number   = {6A},
  pages    = {2871--2903},
  issn     = {0090-5364},
  doi      = {10.1214/17-AOS1642},
  fjournal = {The Annals of Statistics},
  mrclass  = {60B20 (62G35 62H12)},
  mrnumber = {3851758},
  url      = {https://doi.org/10.1214/17-AOS1642},
}

@Article{Neykov2016L1,
  author  = {Matey Neykov and Jun S. Liu and Tianxi Cai},
  title   = {L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {87},
  pages   = {1-37},
  url     = {http://jmlr.org/papers/v17/16-006.html},
}

@Article{Plan2017High,
  author   = {Plan, Y. and Vershynin, R. and Yudovina, E.},
  title    = {High-dimensional estimation with geometric constraints},
  journal  = {Inf. Inference},
  year     = {2017},
  volume   = {6},
  number   = {1},
  pages    = {1--40},
  issn     = {2049-8764},
  fjournal = {Information and Inference. A Journal of the IMA},
  mrclass  = {94A12},
  mrnumber = {3636866},
}

@InProceedings{Sedghi2016Provable,
  author    = {Hanie Sedghi and Majid Janzamin and Anima Anandkumar},
  title     = {Provable Tensor Methods for Learning Mixtures of Generalized Linear Models},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  year      = {2016},
  editor    = {Arthur Gretton and Christian C. Robert},
  volume    = {51},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1223--1231},
  address   = {Cadiz, Spain},
  month     = {09--11 May},
  publisher = {PMLR},
  abstract  = {We consider the problem of learning mixtures of generalized linear models (GLM) which arise in   classification and regression problems.   Typical learning  approaches such as expectation maximization (EM) or variational Bayes  can get stuck in spurious local optima. In contrast,  we present a tensor decomposition method which is guaranteed to correctly recover the parameters. The key insight is to employ certain feature transformations of the input, which depend on the input generative model. Specifically, we employ score function tensors of the input  and compute their cross-correlation with the response variable.   We establish that the decomposition of this tensor consistently recovers the parameters, under mild non-degeneracy conditions.  We demonstrate that the computational and  sample complexity of our method is a low order polynomial of the input and the latent  dimensions.},
  file      = {sedghi16.pdf:http\://proceedings.mlr.press/v51/sedghi16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v51/sedghi16.html},
}

@Article{Stein1972bound,
  author     = {Stein, Charles},
  title      = {A bound for the error in the normal approximation to the distribution of a sum of dependent random variables},
  year       = {1972},
  pages      = {583--602},
  booktitle  = {Proceedings of the {S}ixth {B}erkeley {S}ymposium on {M}athematical {S}tatistics and {P}robability ({U}niv. {C}alifornia, {B}erkeley, {C}alif., 1970/1971), {V}ol. {II}: {P}robability theory},
  mrclass    = {60F05},
  mrnumber   = {0402873},
  mrreviewer = {A. K. AleSkeviciene},
  publisher  = {Univ. California Press, Berkeley, Calif.},
}

@Article{Wang2015Estimation,
  author   = {Wang, Tao and Zhang, Jun and Liang, Hua and Zhu, Lixing},
  title    = {Estimation of a groupwise additive multiple-index model and its applications},
  journal  = {Statist. Sinica},
  year     = {2015},
  volume   = {25},
  number   = {2},
  pages    = {551--566},
  issn     = {1017-0405},
  fjournal = {Statistica Sinica},
  mrclass  = {62J12 (62G05 62J05)},
  mrnumber = {3379088},
}

@InProceedings{Yang2017High,
  author    = {Zhuoran Yang and Krishnakumar Balasubramanian and Han Liu},
  title     = {High-dimensional Non-{G}aussian Single Index Models via Thresholded Score Function Estimation},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3851--3860},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
  abstract  = {We consider estimating the parametric component of single index models in high dimensions. Compared with existing work, we do not require the covariate to be normally distributed. Utilizing Steins Lemma, we propose estimators based on the score function of the covariate. Moreover, to handle score function and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators. Extensive numerical experiments are provided to back up our theory.},
  file      = {yang17a.pdf:http\://proceedings.mlr.press/v70/yang17a/yang17a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v70/yang17a.html},
}

@Article{Yuan2011identifiability,
  author   = {Yuan, Ming},
  title    = {On the identifiability of additive index models},
  journal  = {Statist. Sinica},
  year     = {2011},
  volume   = {21},
  number   = {4},
  pages    = {1901--1911},
  issn     = {1017-0405},
  doi      = {10.5705/ss.2008.117},
  fjournal = {Statistica Sinica},
  mrclass  = {62J12 (62J07 62P20)},
  mrnumber = {2896004},
  url      = {https://doi.org/10.5705/ss.2008.117},
}

@Article{Zhang2018High,
  author      = {Jia Zhang and Xin Chen and Wang Zhou},
  title       = {High Dimensional Elliptical Sliced Inverse Regression in non-Gaussian Distributions},
  journal     = {ArXiv e-prints 1801.01950},
  year        = {2017},
  abstract    = {Sliced inverse regression (SIR) is the most widely-used sufficient dimension reduction method due to its simplicity, generality and computational efficiency. However, when the distribution of the covariates deviates from the multivariate normal distribution, the estimation efficiency of SIR is rather low. In this paper, we propose a robust alternative to SIR - called elliptical sliced inverse regression (ESIR) for analysing high dimensional, elliptically distributed data. There are wide range of applications of the elliptically distributed data, especially in finance and economics where the distribution of the data is often heavy-tailed. To tackle the heavy-tailed elliptically distributed covariates, we novelly utilize the multivariate Kendall's tau matrix in a framework of so-called generalized eigenvector problem for sufficient dimension reduction. Methodologically, we present a practical algorithm for our method. Theoretically, we investigate the asymptotic behavior of the ESIR estimator and obtain the corresponding convergence rate under high dimensional setting. Quantities of simulation results show that ESIR significantly improves the estimation efficiency in heavy-tailed scenarios. A stock exchange data analysis also demonstrates the effectiveness of our method. Moreover, ESIR can be easily extended to most other sufficient dimension reduction methods.},
  date        = {2018-01-06},
  eprint      = {http://arxiv.org/abs/1801.01950v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1801.01950v1:PDF},
  keywords    = {stat.ME, 62H86},
}

@Article{Zhu2006sliced,
  author     = {Zhu, Lixing and Miao, Baiqi and Peng, Heng},
  title      = {On sliced inverse regression with high-dimensional covariates},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {2006},
  volume     = {101},
  number     = {474},
  pages      = {630--643},
  issn       = {0162-1459},
  doi        = {10.1198/016214505000001285},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62H12 (62G05 62G08)},
  mrnumber   = {2281245},
  mrreviewer = {P. Laurie Davies},
  url        = {https://doi.org/10.1198/016214505000001285},
}

@InProceedings{Belkin2018Understand,
  author    = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  title     = {To Understand Deep Learning We Need to Understand Kernel Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  pages     = {541--549},
  address   = {Stockholmsmssan, Stockholm Sweden},
  month     = {10--15 Jul},
  publisher = {PMLR},
  abstract  = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable shallow kernel methods are better understood.},
  file      = {:Belkin2018Understand.pdf:PDF;belkin18a.pdf:http\://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf:PDF},
  timestamp = {2018.10.18},
  url       = {http://proceedings.mlr.press/v80/belkin18a.html},
}

@Article{Belkin2018Overfitting,
  author      = {Mikhail Belkin and Daniel Hsu and Partha Mitra},
  title       = {Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate},
  journal     = {ArXiv e-prints: 1806.05161},
  year        = {2018},
  abstract    = {Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for "overfitted" / interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is robust even when the data contain large amounts of label noise. Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. These schemes have an inductive bias that benefits from higher dimension, a kind of "blessing of dimensionality". Finally, connections to kernel machines, random forests, and adversarial examples in the interpolated regime are discussed.},
  date        = {2018-06-13},
  eprint      = {http://arxiv.org/abs/1806.05161v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Belkin2018Overfitting.pdf:PDF},
  keywords    = {stat.ML, cond-mat.stat-mech, cs.LG},
}

@Article{Belkin2018Does,
  author      = {Mikhail Belkin and Alexander Rakhlin and Alexandre B. Tsybakov},
  title       = {Does data interpolation contradict statistical optimality?},
  journal     = {ArXiv e-prints: 1806.09471},
  year        = {2018},
  abstract    = {We show that learning methods interpolating the training data can achieve optimal rates for the problems of nonparametric regression and prediction with square loss.},
  date        = {2018-06-25},
  eprint      = {http://arxiv.org/abs/1806.09471v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Belkin2018Does.pdf:PDF},
  keywords    = {stat.ML, cs.LG, math.ST, stat.TH},
}

@Article{Radchenko2015High,
  author    = {Radchenko, Peter},
  title     = {High dimensional single index models},
  journal   = {J. Multivariate Anal.},
  year      = {2015},
  volume    = {139},
  pages     = {266--282},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2015.02.007},
  file      = {:Radchenko2015High.pdf:PDF},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62J02 (62F12 62J07 62P25)},
  mrnumber  = {3349492},
  timestamp = {2018.10.18},
  url       = {https://doi.org/10.1016/j.jmva.2015.02.007},
}

@Article{Lian2015Variance,
  author    = {Lian, Heng and Liang, Hua and Carroll, Raymond J.},
  title     = {Variance function partially linear single-index models},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2015},
  volume    = {77},
  number    = {1},
  pages     = {171--194},
  issn      = {1369-7412},
  doi       = {10.1111/rssb.12066},
  file      = {:Lian2015Variance.pdf:PDF},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62J02 (62G08 62G20 62P12)},
  mrnumber  = {3299404},
  timestamp = {2018.10.18},
  url       = {https://doi.org/10.1111/rssb.12066},
}

@Article{Fang2015Variance,
  author    = {Fang, Yixin and Lian, Heng and Liang, Hua and Ruppert, David},
  title     = {Variance function additive partial linear models},
  journal   = {Electron. J. Stat.},
  year      = {2015},
  volume    = {9},
  number    = {2},
  pages     = {2793--2827},
  issn      = {1935-7524},
  doi       = {10.1214/15-EJS1080},
  file      = {:Fang2015Variance.pdf:PDF},
  fjournal  = {Electronic Journal of Statistics},
  mrclass   = {62G08 (62F12 62G20 62J02)},
  mrnumber  = {3439185},
  timestamp = {2018.10.18},
  url       = {https://doi.org/10.1214/15-EJS1080},
}

@InProceedings{Mei2015Signal,
  author    = {Jonathan Mei and Jose M.F. Moura},
  title     = {Signal processing on graphs: Estimating the structure of a graph},
  booktitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  year      = {2015},
  month     = {apr},
  publisher = {{IEEE}},
  doi       = {10.1109/icassp.2015.7179022},
  timestamp = {2018.10.18},
}

@Article{Mei2017Signal,
  author    = {Jonathan Mei and Jose M. F. Moura},
  title     = {Signal Processing on Graphs: Causal Modeling of $\less$italic$\greater$Un$\less$/italic$\greater$structured Data},
  journal   = {{IEEE} Transactions on Signal Processing},
  year      = {2017},
  volume    = {65},
  number    = {8},
  pages     = {2077--2092},
  month     = {apr},
  doi       = {10.1109/tsp.2016.2634543},
  file      = {:Mei2017Signal.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2018.10.18},
}

@Article{Mei2018SILVar,
  author    = {Jonathan Mei and Jose M. F. Moura},
  title     = {{SILVar}: Single Index Latent Variable Models},
  journal   = {{IEEE} Transactions on Signal Processing},
  year      = {2018},
  volume    = {66},
  number    = {11},
  pages     = {2790--2803},
  month     = {jun},
  doi       = {10.1109/tsp.2018.2818075},
  file      = {:Mei2018SILVar.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2018.10.18},
}

@Article{Ma2017Exploration,
  author      = {Zhuang Ma and Zongming Ma},
  title       = {Exploration of Large Networks with Covariates via Fast and Universal Latent Space Model Fitting},
  journal     = {ArXiv e-prints: 1705.02372},
  year        = {2017},
  abstract    = {Latent space models are effective tools for statistical modeling and exploration of network data. These models can effectively model real world network characteristics such as degree heterogeneity, transitivity, homophily, etc. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing likelihood for a special class of inner-product models while working simultaneously for a wide range of different latent space models, such as distance models, which allow latent vectors to affect edge formation in flexible ways. These fitting methods, especially the one based on projected gradient descent, are fast and scalable to large networks. We obtain their rates of convergence for both inner-product models and beyond. The effectiveness of the modeling approach and fitting algorithms is demonstrated on five real world network datasets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning.},
  date        = {2017-05-05},
  eprint      = {http://arxiv.org/abs/1705.02372v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Ma2017Exploration.pdf:PDF},
  keywords    = {stat.ME, cs.SI, stat.ML},
  timestamp   = {2018.10.18},
}

@Article{Wei2018Margin,
  author      = {Colin Wei and Jason D. Lee and Qiang Liu and Tengyu Ma},
  title       = {On the Margin Theory of Feedforward Neural Networks},
  journal     = {ArXiv e-prints: 1810.05369},
  year        = {2018},
  abstract    = {Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks. Towards explaining this phenomenon, we adopt a margin-based perspective. We establish: 1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, 2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for two-layer networks. In particular, an infinite-size neural network enjoys the best generalization guarantees. The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees. We validate this gap between the two approaches empirically. Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization. We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.},
  date        = {2018-10-12},
  eprint      = {http://arxiv.org/abs/1810.05369v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Wei2018Margin.pdf:PDF},
  keywords    = {stat.ML, cs.LG},
  timestamp   = {2018.10.18},
}

@Book{Fan1996Local,
  title      = {Local polynomial modelling and its applications},
  publisher  = {Chapman \& Hall, London},
  year       = {1996},
  author     = {Fan, J. and Gijbels, I.},
  volume     = {66},
  series     = {Monographs on Statistics and Applied Probability},
  isbn       = {0-412-98321-4},
  mrclass    = {62G05 (62J02)},
  mrnumber   = {1383587},
  mrreviewer = {Theo Gasser},
  pages      = {xvi+341},
  timestamp  = {2018.10.19},
}

@Article{Gine2001consistency,
  author     = {Gin\'{e}, Evarist and Guillou, Armelle},
  title      = {On consistency of kernel density estimators for randomly censored data: rates holding uniformly over adaptive intervals},
  journal    = {Ann. Inst. H. Poincar\'{e} Probab. Statist.},
  year       = {2001},
  volume     = {37},
  number     = {4},
  pages      = {503--522},
  issn       = {0246-0203},
  doi        = {10.1016/S0246-0203(01)01081-0},
  file       = {:Gine2001consistency.pdf:PDF},
  fjournal   = {Annales de l'Institut Henri Poincar\'{e}. Probabilit\'{e}s et Statistiques},
  mrclass    = {62G07 (60F15 62G20 62G30 62N01)},
  mrnumber   = {1876841},
  mrreviewer = {Lian Fen Qian},
  url        = {https://doi.org/10.1016/S0246-0203(01)01081-0},
}

@Article{Gine2002Rates,
  author     = {Gin\'{e}, Evarist and Guillou, Armelle},
  title      = {Rates of strong uniform consistency for multivariate kernel density estimators},
  journal    = {Ann. Inst. H. Poincar\'{e} Probab. Statist.},
  year       = {2002},
  volume     = {38},
  number     = {6},
  pages      = {907--921},
  issn       = {0246-0203},
  note       = {En l'honneur de J. Bretagnolle, D. Dacunha-Castelle, I. Ibragimov},
  doi        = {10.1016/S0246-0203(02)01128-7},
  file       = {:Gine2002Rates.pdf:PDF},
  fjournal   = {Annales de l'Institut Henri Poincar\'{e}. Probabilit\'{e}s et Statistiques},
  mrclass    = {60G07 (60F15 62G20 62G30)},
  mrnumber   = {1955344},
  mrreviewer = {Jos\'{e} Rafael Le\'{o}n},
  url        = {https://doi.org/10.1016/S0246-0203(02)01128-7},
}

@Article{Dirksen2015Tail,
  author    = {Dirksen, Sjoerd},
  title     = {Tail bounds via generic chaining},
  journal   = {Electron. J. Probab.},
  year      = {2015},
  volume    = {20},
  pages     = {no. 53, 29},
  issn      = {1083-6489},
  doi       = {10.1214/EJP.v20-3760},
  file      = {:Dirksen2015Tail.pdf:},
  fjournal  = {Electronic Journal of Probability},
  mrclass   = {60E15 (60F10 60G50)},
  mrnumber  = {3354613},
  timestamp = {2018.10.31},
  url       = {https://doi.org/10.1214/EJP.v20-3760},
}

@Article{Fang2018Empirical,
  author    = {Fang, Jianglin and Liu, Wanrong and Lu, Xuewen},
  title     = {Empirical likelihood for heteroscedastic partially linear single-index models with growing dimensional data},
  journal   = {Metrika},
  year      = {2018},
  volume    = {81},
  number    = {3},
  pages     = {255--281},
  issn      = {0026-1335},
  doi       = {10.1007/s00184-018-0642-7},
  file      = {:Fang2018Empirical.pdf:PDF},
  fjournal  = {Metrika. International Journal for Theoretical and Applied Statistics},
  mrclass   = {62F10 (62F12 62G05)},
  mrnumber  = {3770900},
  timestamp = {2018.11.02},
  url       = {https://doi.org/10.1007/s00184-018-0642-7},
}

@Article{Yang2017Empirical,
  author    = {Yang, Yiping and Chen, Lifang and Zhao, Peixin},
  title     = {Empirical likelihood inference in partially linear single-index models with endogenous covariates},
  journal   = {Comm. Statist. Theory Methods},
  year      = {2017},
  volume    = {46},
  number    = {7},
  pages     = {3297--3307},
  issn      = {0361-0926},
  doi       = {10.1080/03610926.2015.1060341},
  file      = {:Yang2017Empirical.pdf:PDF},
  fjournal  = {Communications in Statistics. Theory and Methods},
  mrclass   = {62G05 (62G20)},
  mrnumber  = {3589099},
  timestamp = {2018.11.02},
  url       = {https://doi.org/10.1080/03610926.2015.1060341},
}

@Article{Yu2014Empirical,
  author    = {Yu, Zhuoxi and He, Bing and Chen, Min},
  title     = {Empirical likelihood for generalized partially linear single-index models},
  journal   = {Comm. Statist. Theory Methods},
  year      = {2014},
  volume    = {43},
  number    = {19},
  pages     = {4156--4163},
  issn      = {0361-0926},
  doi       = {10.1080/03610926.2012.719989},
  file      = {:Yu2014Empirical.pdf:PDF},
  fjournal  = {Communications in Statistics. Theory and Methods},
  mrclass   = {62G10 (62G20 62J12)},
  mrnumber  = {3264935},
  timestamp = {2018.11.02},
  url       = {https://doi.org/10.1080/03610926.2012.719989},
}

@Article{Xia2002Single,
  author    = {Xia, Yingcun and Tong, Howell and Li, W. K.},
  title     = {Single-index volatility models and estimation},
  journal   = {Statist. Sinica},
  year      = {2002},
  volume    = {12},
  number    = {3},
  pages     = {785--799},
  issn      = {1017-0405},
  file      = {:Xia2002Single.pdf:PDF},
  fjournal  = {Statistica Sinica},
  mrclass   = {62G05 (62M10)},
  mrnumber  = {1929964},
  timestamp = {2018.11.07},
}

@Article{Haerdle1993Optimal,
  author    = {H\"{a}rdle, Wolfgang and Hall, Peter and Ichimura, Hidehiko},
  title     = {Optimal smoothing in single-index models},
  journal   = {Ann. Statist.},
  year      = {1993},
  volume    = {21},
  number    = {1},
  pages     = {157--178},
  issn      = {0090-5364},
  doi       = {10.1214/aos/1176349020},
  file      = {:Haerdle1993Optimal.pdf:},
  fjournal  = {The Annals of Statistics},
  mrclass   = {62H05 (62G05)},
  mrnumber  = {1212171},
  timestamp = {2018.11.07},
  url       = {https://doi.org/10.1214/aos/1176349020},
}

@Article{Klein2005Concentration,
  author     = {Klein, T. and Rio, E.},
  title      = {Concentration around the mean for maxima of empirical processes},
  journal    = {Ann. Probab.},
  year       = {2005},
  volume     = {33},
  number     = {3},
  pages      = {1060--1077},
  issn       = {0091-1798},
  doi        = {10.1214/009117905000000044},
  file       = {:Klein2005Concentration.pdf:PDF},
  fjournal   = {The Annals of Probability},
  mrclass    = {60E15 (60F10)},
  mrnumber   = {2135312},
  mrreviewer = {Peter Eichelsbacher},
  timestamp  = {2018.11.14},
  url        = {https://doi.org/10.1214/009117905000000044},
}

@Article{Gine2009Uniform,
  author     = {Gin\'{e}, Evarist and Nickl, Richard},
  title      = {Uniform limit theorems for wavelet density estimators},
  journal    = {Ann. Probab.},
  year       = {2009},
  volume     = {37},
  number     = {4},
  pages      = {1605--1646},
  issn       = {0091-1798},
  doi        = {10.1214/08-AOP447},
  file       = {:Gine2009Uniform.pdf:PDF},
  fjournal   = {The Annals of Probability},
  mrclass    = {62G07 (60F05 60F17)},
  mrnumber   = {2546757},
  mrreviewer = {Paul Doukhan},
  timestamp  = {2018.11.14},
  url        = {https://doi.org/10.1214/08-AOP447},
}

@Article{Gine2006Concentration,
  author     = {Gin\'{e}, Evarist and Koltchinskii, Vladimir},
  title      = {Concentration inequalities and asymptotic results for ratio type empirical processes},
  journal    = {Ann. Probab.},
  year       = {2006},
  volume     = {34},
  number     = {3},
  pages      = {1143--1216},
  issn       = {0091-1798},
  doi        = {10.1214/009117906000000070},
  file       = {:Gine2006Concentration.pdf:PDF},
  fjournal   = {The Annals of Probability},
  mrclass    = {60E15 (60F15 60F17 62G08 68T10)},
  mrnumber   = {2243881},
  mrreviewer = {Przemys\l aw Matu\l a},
  timestamp  = {2018.11.15},
  url        = {https://doi.org/10.1214/009117906000000070},
}

@Article{Csoergo1986Weighted,
  author     = {Cs\"{o}rg\H{o}, Mikl\'{o}s and Cs\"{o}rg\H{o}, S\'{a}ndor and Horv\'{a}th, Lajos and Mason, David M.},
  title      = {Weighted empirical and quantile processes},
  journal    = {Ann. Probab.},
  year       = {1986},
  volume     = {14},
  number     = {1},
  pages      = {31--85},
  issn       = {0091-1798},
  file       = {:Csoergo1986Weighted.pdf:PDF},
  fjournal   = {The Annals of Probability},
  mrclass    = {60F05 (60F17 62G30)},
  mrnumber   = {815960},
  mrreviewer = {Gutti J. Babu},
  timestamp  = {2018.11.15},
  url        = {http://links.jstor.org/sici?sici=0091-1798(198601)14:1<31:WEAQP>2.0.CO;2-S&origin=MSN},
}

@Article{Csaki1977law,
  author     = {Cs\'{a}ki, E.},
  title      = {The law of the iterated logarithm for normalized empirical distribution function},
  journal    = {Z. Wahrscheinlichkeitstheorie und Verw. Gebiete},
  year       = {1977},
  volume     = {38},
  number     = {2},
  pages      = {147--167},
  doi        = {10.1007/BF00533305},
  file       = {:Csaki1977law.pdf:PDF},
  mrclass    = {60F15 (62E20 62G99)},
  mrnumber   = {0431350},
  mrreviewer = {Jon A. Wellner},
  timestamp  = {2018.11.15},
  url        = {https://doi.org/10.1007/BF00533305},
}

@Article{Jaeschke1979asymptotic,
  author     = {Jaeschke, D.},
  title      = {The asymptotic distribution of the supremum of the standardized empirical distribution function on subintervals},
  journal    = {Ann. Statist.},
  year       = {1979},
  volume     = {7},
  number     = {1},
  pages      = {108--115},
  issn       = {0090-5364},
  file       = {:Jaeschke1979asymptotic.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62E20 (60F05 62G99)},
  mrnumber   = {515687},
  mrreviewer = {S\'{a}ndor Cs\"{o}rg\H{o}},
  timestamp  = {2018.11.15},
  url        = {http://links.jstor.org/sici?sici=0090-5364(197901)7:1<108:TADOTS>2.0.CO;2-P&origin=MSN},
}

@Article{Eicker1979asymptotic,
  author     = {Eicker, F.},
  title      = {The asymptotic distribution of the suprema of the standardized empirical processes},
  journal    = {Ann. Statist.},
  year       = {1979},
  volume     = {7},
  number     = {1},
  pages      = {116--138},
  issn       = {0090-5364},
  file       = {:Eicker1979asymptotic.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62E20 (60F05 62G99)},
  mrnumber   = {515688},
  mrreviewer = {S\'{a}ndor Cs\"{o}rg\H{o}},
  timestamp  = {2018.11.15},
  url        = {http://links.jstor.org/sici?sici=0090-5364(197901)7:1<116:TADOTS>2.0.CO;2-O&origin=MSN},
}

@Article{Shorack1991Embedding,
  author     = {Shorack, Galen R.},
  title      = {Embedding the finite sampling process at a rate},
  journal    = {Ann. Probab.},
  year       = {1991},
  volume     = {19},
  number     = {2},
  pages      = {826--842},
  issn       = {0091-1798},
  file       = {:Shorack1991Embedding.pdf:PDF},
  fjournal   = {The Annals of Probability},
  mrclass    = {60F17 (60G17 60G42)},
  mrnumber   = {1106288},
  mrreviewer = {Lajos Horv\'{a}th},
  timestamp  = {2018.11.15},
  url        = {http://links.jstor.org/sici?sici=0091-1798(199104)19:2<826:ETFSPA>2.0.CO;2-1&origin=MSN},
}

@InCollection{Mason2001exponential,
  author     = {Mason, David M.},
  title      = {An exponential inequality for a weighted approximation to the uniform empirical process with applications},
  booktitle  = {State of the art in probability and statistics ({L}eiden, 1999)},
  publisher  = {Inst. Math. Statist., Beachwood, OH},
  year       = {2001},
  volume     = {36},
  series     = {IMS Lecture Notes Monogr. Ser.},
  pages      = {477--498},
  doi        = {10.1214/lnms/1215090084},
  file       = {:Mason2001exponential.pdf:PDF},
  mrclass    = {60F17 (60F15)},
  mrnumber   = {1836576},
  mrreviewer = {Miguel A. Arcones},
  timestamp  = {2018.11.15},
  url        = {https://doi.org/10.1214/lnms/1215090084},
}

@Article{Ferger2018supremum,
  author    = {Ferger, Dietmar},
  title     = {On the supremum of a {B}rownian bridge standardized by its maximizing point with applications to statistics},
  journal   = {Statist. Probab. Lett.},
  year      = {2018},
  volume    = {134},
  pages     = {63--69},
  issn      = {0167-7152},
  doi       = {10.1016/j.spl.2017.10.008},
  file      = {:Ferger2018supremum.pdf:PDF},
  fjournal  = {Statistics \& Probability Letters},
  mrclass   = {60J65 (60F17 62G10)},
  mrnumber  = {3758582},
  timestamp = {2018.11.15},
  url       = {https://doi.org/10.1016/j.spl.2017.10.008},
}

@Article{Monti2017Learning,
  author    = {Monti, Ricardo Pio and Anagnostopoulos, Christoforos and Montana, Giovanni},
  title     = {Learning population and subject-specific brain connectivity networks via mixed neighborhood selection},
  journal   = {Ann. Appl. Stat.},
  year      = {2017},
  volume    = {11},
  number    = {4},
  pages     = {2142--2164},
  issn      = {1932-6157},
  doi       = {10.1214/17-AOAS1067},
  file      = {:Monti2017Learning.pdf:PDF},
  fjournal  = {The Annals of Applied Statistics},
  mrclass   = {62P10 (62H35 62H99)},
  mrnumber  = {3743291},
  timestamp = {2018.11.20},
  url       = {https://doi.org/10.1214/17-AOAS1067},
}

@Article{Mammen1993Bootstrap,
  author     = {Mammen, Enno},
  title      = {Bootstrap and wild bootstrap for high-dimensional linear models},
  journal    = {Ann. Statist.},
  year       = {1993},
  volume     = {21},
  number     = {1},
  pages      = {255--285},
  issn       = {0090-5364},
  doi        = {10.1214/aos/1176349025},
  file       = {:Mammen1993Bootstrap.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62G09 (62F12 62J05)},
  mrnumber   = {1212176},
  mrreviewer = {Arup Bose},
  timestamp  = {2018.11.20},
  url        = {https://doi.org/10.1214/aos/1176349025},
}

@Article{Stoker1993Smoothing,
  author    = {Stoker, Thomas M.},
  title     = {Smoothing bias in density derivative estimation},
  journal   = {J. Amer. Statist. Assoc.},
  year      = {1993},
  volume    = {88},
  number    = {423},
  pages     = {855--863},
  issn      = {0162-1459},
  file      = {:Stoker1993Smoothing.pdf:PDF},
  fjournal  = {Journal of the American Statistical Association},
  mrclass   = {62G05},
  mrnumber  = {1242936},
  timestamp = {2018.11.30},
  url       = {http://links.jstor.org/sici?sici=0162-1459(199309)88:423<855:SBIDDE>2.0.CO;2-Y&origin=MSN},
}

@Article{Hansen2008Uniform,
  author     = {Hansen, Bruce E.},
  title      = {Uniform convergence rates for kernel estimation with dependent data},
  journal    = {Econometric Theory},
  year       = {2008},
  volume     = {24},
  number     = {3},
  pages      = {726--748},
  issn       = {0266-4666},
  doi        = {10.1017/S0266466608080304},
  file       = {:Hansen2008Uniform.pdf:PDF},
  fjournal   = {Econometric Theory},
  mrclass    = {62G07 (62G05)},
  mrnumber   = {2409261},
  mrreviewer = {Tiee-Jian Wu},
  timestamp  = {2018.11.30},
  url        = {https://doi.org/10.1017/S0266466608080304},
}

@Article{Cattaneo2013Optimal,
  author     = {Cattaneo, Matias D. and Farrell, Max H.},
  title      = {Optimal convergence rates, {B}ahadur representation, and asymptotic normality of partitioning estimators},
  journal    = {J. Econometrics},
  year       = {2013},
  volume     = {174},
  number     = {2},
  pages      = {127--143},
  issn       = {0304-4076},
  doi        = {10.1016/j.jeconom.2013.02.002},
  file       = {:Cattaneo2013Optimal.pdf:PDF;:Cattaneo2013Optimal_supp.pdf:PDF},
  fjournal   = {Journal of Econometrics},
  mrclass    = {62G08 (62G20)},
  mrnumber   = {3045024},
  mrreviewer = {Ross S. McVinish},
  timestamp  = {2018.11.30},
  url        = {https://doi.org/10.1016/j.jeconom.2013.02.002},
}

@Article{Kohler2006Rates,
  author    = {Kohler, Michael and Krzy\.{z}ak, Adam and Walk, Harro},
  title     = {Rates of convergence for partitioning and nearest neighbor regression estimates with unbounded data},
  journal   = {J. Multivariate Anal.},
  year      = {2006},
  volume    = {97},
  number    = {2},
  pages     = {311--323},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2005.03.006},
  file      = {:Kohler2006Rates.pdf:PDF},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62G07 (62H12)},
  mrnumber  = {2234025},
  timestamp = {2018.11.30},
  url       = {https://doi.org/10.1016/j.jmva.2005.03.006},
}

@Article{Dai2016Optimal,
  author    = {Dai, Wenlin and Tong, Tiejun and Genton, Marc G.},
  title     = {Optimal estimation of derivatives in nonparametric regression},
  journal   = {J. Mach. Learn. Res.},
  year      = {2016},
  volume    = {17},
  pages     = {Paper No. 164, 25},
  issn      = {1532-4435},
  fjournal  = {Journal of Machine Learning Research (JMLR)},
  mrclass   = {62G08 (62G05 62G20)},
  mrnumber  = {3555052},
  timestamp = {2018.11.30},
}

@Article{Sasaki2017Mode,
  author    = {Sasaki, Hiroaki and Kanamori, Takafumi and Hyv\"{a}rinen, Aapo and Niu, Gang and Sugiyama, Masashi},
  title     = {Mode-seeking clustering and density ridge estimation via direct estimation of density-derivative-ratios},
  journal   = {J. Mach. Learn. Res.},
  year      = {2017},
  volume    = {18},
  pages     = {Paper No. 180, 47},
  issn      = {1532-4435},
  fjournal  = {Journal of Machine Learning Research (JMLR)},
  mrclass   = {62G07 (62H30 62J07)},
  mrnumber  = {3827068},
  timestamp = {2018.11.30},
}

@Article{Sasaki2016Direct,
  author    = {Hiroaki Sasaki and Yung-Kyun Noh and Gang Niu and Masashi Sugiyama},
  title     = {Direct Density Derivative Estimation},
  journal   = {Neural Computation},
  year      = {2016},
  volume    = {28},
  number    = {6},
  pages     = {1101--1140},
  month     = {jun},
  doi       = {10.1162/neco_a_00835},
  file      = {:Sasaki2016Direct.pdf:PDF},
  publisher = {{MIT} Press - Journals},
  timestamp = {2018.11.30},
}

@Article{Gross2016Data,
  author     = {Gross, Samuel M. and Tibshirani, Robert},
  title      = {Data shared lasso: a novel tool to discover uplift},
  journal    = {Comput. Statist. Data Anal.},
  year       = {2016},
  volume     = {101},
  pages      = {226--235},
  issn       = {0167-9473},
  doi        = {10.1016/j.csda.2016.02.015},
  file       = {:Gross2016Data.pdf:PDF},
  fjournal   = {Computational Statistics \& Data Analysis},
  mrclass    = {62J05 (62J07)},
  mrnumber   = {3504847},
  mrreviewer = {Pierre Alquier},
  url        = {https://doi.org/10.1016/j.csda.2016.02.015},
}

@Article{Engle1987Estimating,
  author    = {Robert F. Engle and David M. Lilien and Russell P. Robins},
  title     = {{Estimating Time Varying Risk Premia in the Term Structure: The {A}rch-{M} Model}},
  journal   = {Econometrica},
  year      = {1987},
  volume    = {55},
  number    = {2},
  pages     = {391},
  month     = {mar},
  doi       = {10.2307/1913242},
  publisher = {{JSTOR}},
  timestamp = {2019.02.26},
}

@Book{Vaart1998Asymptotic,
  title      = {Asymptotic statistics},
  publisher  = {Cambridge University Press, Cambridge},
  year       = {1998},
  author     = {van der Vaart, A. W.},
  volume     = {3},
  series     = {Cambridge Series in Statistical and Probabilistic Mathematics},
  isbn       = {0-521-49603-9; 0-521-78450-6},
  doi        = {10.1017/CBO9780511802256},
  mrclass    = {62-02 (62E20 62F05 62F12 62G07 62G09 62G20)},
  mrnumber   = {1652247},
  mrreviewer = {Nancy Reid},
  pages      = {xvi+443},
  timestamp  = {2019.03.22},
  url        = {https://doi.org/10.1017/CBO9780511802256},
}

@Article{Pilavakis2019Testing,
  author      = {Dimitrios Pilavakis and Efstathios Paparoditis and Theofanis Sapatinas},
  title       = {Testing Equality of Autocovariance Operators for Functional Time Series},
  journal     = {ArXiv e-prints 1901.08535},
  year        = {2019},
  abstract    = {We consider strictly stationary stochastic processes of Hilbert space-valued random variables and focus on tests of the equality of the lag-zero autocovariance operators of several independent functional time series. A moving block bootstrap-based testing procedure is proposed which generates pseudo random elements that satisfy the null hypothesis of interest. It is based on directly bootstrapping the time series of tensor products which overcomes some common difficulties associated with applications of the bootstrap to related testing problems. The suggested methodology can be potentially applied to a broad range of test statistics of the hypotheses of interest. As an example, we establish validity for approximating the distribution under the null of a fully functional test statistic based on the Hilbert-Schmidt distance of the corresponding sample lag-zero autocovariance operators, and show consistency under the alternative. As a prerequisite, we prove a central limit theorem for the moving block bootstrap procedure applied to the sample autocovariance operator which is of interest on its own. The finite sample size and power performance of the suggested moving block bootstrap-based testing procedure is illustrated through simulations and an application to a real-life dataset is discussed.},
  date        = {2019-01-24},
  eprint      = {http://arxiv.org/abs/1901.08535v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {:Pilavakis2019Testing.pdf:PDF},
  keywords    = {math.ST, stat.TH},
  timestamp   = {2019.03.28},
}

@Article{Kokoszka2017Dynamic,
  author    = {Piotr Kokoszka and Hong Miao and Matthew Reimherr and Bahaeddine Taoufik},
  title     = {Dynamic Functional Regression with Application to the Cross-section of Returns},
  journal   = {Journal of Financial Econometrics},
  year      = {2017},
  volume    = {16},
  number    = {3},
  pages     = {461--485},
  month     = {aug},
  doi       = {10.1093/jjfinec/nbx027},
  file      = {:Kokoszka2017Dynamic.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
  timestamp = {2019.03.28},
}

@Article{Fremdt2013Testing,
  author     = {Fremdt, Stefan and Steinebach, Josef G. and Horv\'{a}th, Lajos and Kokoszka, Piotr},
  title      = {Testing the equality of covariance operators in functional samples},
  journal    = {Scand. J. Stat.},
  year       = {2013},
  volume     = {40},
  number     = {1},
  pages      = {138--152},
  issn       = {0303-6898},
  doi        = {10.1111/j.1467-9469.2012.00796.x},
  fjournal   = {Scandinavian Journal of Statistics. Theory and Applications},
  mrclass    = {62G10 (62G05 62G20)},
  mrnumber   = {3024036},
  mrreviewer = {Philippe Vieu},
  timestamp  = {2019.03.28},
  url        = {https://doi.org/10.1111/j.1467-9469.2012.00796.x},
}

@Article{Ou-Yang2018Joint,
  author    = {Le Ou-Yang and Xiao-Fei Zhang and Xing-Ming Zhao and Debby D. Wang and Fu Lee Wang and Baiying Lei and Hong Yan},
  title     = {Joint Learning of Multiple Differential Networks With Latent Variables},
  journal   = {{IEEE} Transactions on Cybernetics},
  year      = {2018},
  pages     = {1--13},
  doi       = {10.1109/tcyb.2018.2845838},
  file      = {:Ou-Yang2018Joint.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Li2018nonparametric,
  author   = {Li, Bing and Solea, Eftychia},
  title    = {A nonparametric graphical model for functional data with application to brain networks based on f{MRI}},
  journal  = {J. Amer. Statist. Assoc.},
  year     = {2018},
  volume   = {113},
  number   = {524},
  pages    = {1637--1655},
  issn     = {0162-1459},
  doi      = {10.1080/01621459.2017.1356726},
  file     = {:Li2018nonparametric.pdf:PDF},
  fjournal = {Journal of the American Statistical Association},
  mrclass  = {62G05 (62H99)},
  mrnumber = {3902235},
  url      = {https://doi.org/10.1080/01621459.2017.1356726},
}

@Article{Li2010dimension,
  author     = {Li, Bing and Kim, Min Kyung and Altman, Naomi},
  title      = {On dimension folding of matrix- or array-valued statistical objects},
  journal    = {Ann. Statist.},
  year       = {2010},
  volume     = {38},
  number     = {2},
  pages      = {1094--1121},
  issn       = {0090-5364},
  doi        = {10.1214/09-AOS737},
  file       = {:Li2010dimension.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H12 (62G08)},
  mrnumber   = {2604706},
  mrreviewer = {Dennis Kristensen},
  url        = {https://doi.org/10.1214/09-AOS737},
}

@Article{Zhu2018Multiple,
  author   = {Zhu, Yunzhang and Li, Lexin},
  title    = {Multiple matrix {G}aussian graphs estimation},
  journal  = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year     = {2018},
  volume   = {80},
  number   = {5},
  pages    = {927--950},
  issn     = {1369-7412},
  doi      = {10.1111/rssb.12278},
  file     = {:Zhu2018Multiple.pdf:PDF},
  fjournal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass  = {62H12 (62H99 62P10)},
  mrnumber = {3874304},
  url      = {https://doi.org/10.1111/rssb.12278},
}

@Article{Li2018Linear,
  author   = {Li, Bing},
  title    = {Linear operator-based statistical analysis: a useful paradigm for big data},
  journal  = {Canad. J. Statist.},
  year     = {2018},
  volume   = {46},
  number   = {1},
  pages    = {79--103},
  issn     = {0319-5724},
  doi      = {10.1002/cjs.11329},
  file     = {:Li2018Linear.pdf:PDF},
  fjournal = {The Canadian Journal of Statistics. La Revue Canadienne de Statistique},
  mrclass  = {62H99 (62G99)},
  mrnumber = {3767167},
  url      = {https://doi.org/10.1002/cjs.11329},
}

@Article{Lee2016additive,
  author   = {Lee, Kuang-Yao and Li, Bing and Zhao, Hongyu},
  title    = {On an additive partial correlation operator and nonparametric estimation of graphical models},
  journal  = {Biometrika},
  year     = {2016},
  volume   = {103},
  number   = {3},
  pages    = {513--530},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/asw028},
  file     = {:Lee2016additive.pdf:PDF;:Lee2016additive_supp.pdf:PDF},
  fjournal = {Biometrika},
  mrclass  = {62G05 (62H99)},
  mrnumber = {3551781},
  url      = {https://doi.org/10.1093/biomet/asw028},
}

@Article{Yu2018Generalized,
  author      = {Shiqing Yu and Mathias Drton and Ali Shojaie},
  title       = {Generalized Score Matching for Non-Negative Data},
  journal     = {Arxiv 1812.10551},
  year        = {2018},
  abstract    = {A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. The score matching method of Hyv\"arinen [2005] avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over $\mathbb{R}^m$. Hyv\"arinen [2007] extended the approach to distributions supported on the non-negative orthant, $\mathbb{R}_+^m$. In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. As an example, we consider a general class of pairwise interaction models. Addressing an overlooked inexistence problem, we generalize the regularized score matching method of Lin et al. [2016] and improve its theoretical guarantees for non-negative Gaussian graphical models.},
  date        = {2018-12-26},
  eprint      = {http://arxiv.org/abs/1812.10551v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Yu2018Generalized.pdf:PDF},
  keywords    = {stat.ML, cs.LG, stat.ME, stat.OT},
  timestamp   = {2019.05.02},
}

@Article{Chernozhukov2015Valid,
  author    = {Victor Chernozhukov and Christian Hansen and Martin Spindler},
  title     = {Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach},
  journal   = {Annual Review of Economics},
  year      = {2015},
  volume    = {7},
  number    = {1},
  pages     = {649--688},
  month     = {aug},
  doi       = {10.1146/annurev-economics-012315-015826},
  publisher = {Annual Reviews},
}

@Article{Roy2017Change,
  author     = {Roy, Sandipan and Atchad\'{e}, Yves and Michailidis, George},
  title      = {Change point estimation in high dimensional {M}arkov random-field models},
  journal    = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year       = {2017},
  volume     = {79},
  number     = {4},
  pages      = {1187--1206},
  issn       = {1369-7412},
  doi        = {10.1111/rssb.12205},
  fjournal   = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass    = {62H12 (62E20 62M40)},
  mrnumber   = {3689314},
  mrreviewer = {Salim Bouzebda},
  url        = {https://doi.org/10.1111/rssb.12205},
}

@Article{Tzourio-Mazoyer2002Automated,
  author    = {N. Tzourio-Mazoyer and B. Landeau and D. Papathanassiou and F. Crivello and O. Etard and N. Delcroix and B. Mazoyer and M. Joliot},
  title     = {Automated Anatomical Labeling of Activations in {SPM} Using a Macroscopic Anatomical Parcellation of the {MNI} {MRI} Single-Subject Brain},
  journal   = {{NeuroImage}},
  year      = {2002},
  volume    = {15},
  number    = {1},
  pages     = {273--289},
  month     = jan,
  doi       = {10.1006/nimg.2001.0978},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1006/nimg.2001.0978},
}

@Article{Botev2017normal,
  author    = {Botev, Z. I.},
  title     = {The normal law under linear restrictions: simulation and estimation via minimax tilting},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2017},
  volume    = {79},
  number    = {1},
  pages     = {125--148},
  issn      = {1369-7412},
  doi       = {10.1111/rssb.12162},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62F30 (60-08 62H12)},
  mrnumber  = {3597967},
  timestamp = {2019.05.02},
  url       = {https://doi.org/10.1111/rssb.12162},
}

@Article{Peters2016Causal,
  author    = {Peters, Jonas and B\"{u}hlmann, Peter and Meinshausen, Nicolai},
  title     = {Causal inference by using invariant prediction: identification and confidence intervals},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2016},
  volume    = {78},
  number    = {5},
  pages     = {947--1012},
  issn      = {1369-7412},
  note      = {With comments and a rejoinder},
  doi       = {10.1111/rssb.12167},
  file      = {:Peters2016Causal.pdf:PDF},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62H99 (62G10 62J02 62J05)},
  mrnumber  = {3557186},
  timestamp = {2019.05.07},
  url       = {https://doi.org/10.1111/rssb.12167},
}

@InProceedings{Yu2018Provable,
  author    = {Ming Yu and Zhuoran Yang and Tuo Zhao and Mladen Kolar and Zhaoran Wang},
  title     = {Provable Gaussian Embedding with One Observation},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al, Canada.},
  year      = {2018},
  editor    = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  pages     = {6765--6775},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/YuYZKW18},
  timestamp = {Sun, 16 Dec 2018 17:30:05 +0100},
  url       = {http://papers.nips.cc/paper/7910-provable-gaussian-embedding-with-one-observation},
}

@Article{Virta2017Independent,
  author   = {Virta, Joni and Li, Bing and Nordhausen, Klaus and Oja, Hannu},
  title    = {Independent component analysis for tensor-valued data},
  journal  = {J. Multivariate Anal.},
  year     = {2017},
  volume   = {162},
  pages    = {172--192},
  issn     = {0047-259X},
  doi      = {10.1016/j.jmva.2017.09.008},
  fjournal = {Journal of Multivariate Analysis},
  mrclass  = {62H12 (62E20 62H10 62H25)},
  mrnumber = {3719342},
  url      = {https://doi.org/10.1016/j.jmva.2017.09.008},
}

@Article{Virta2018JADE,
  author   = {Virta, Joni and Li, Bing and Nordhausen, Klaus and Oja, Hannu},
  title    = {J{ADE} for tensor-valued observations},
  journal  = {J. Comput. Graph. Statist.},
  year     = {2018},
  volume   = {27},
  number   = {3},
  pages    = {628--637},
  issn     = {1061-8600},
  doi      = {10.1080/10618600.2017.1407324},
  fjournal = {Journal of Computational and Graphical Statistics},
  mrclass  = {62F12 (62H25)},
  mrnumber = {3863763},
  url      = {https://doi.org/10.1080/10618600.2017.1407324},
}

@Article{Safayani2018EM,
  author    = {Mehran Safayani and Seyed Hashem Ahmadi and Homayun Afrabandpey and Abdolreza Mirzaei},
  title     = {An {EM} based probabilistic two-dimensional {CCA} with application to face recognition},
  journal   = {Appl. Intell.},
  year      = {2018},
  volume    = {48},
  number    = {3},
  pages     = {755--770},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/apin/SafayaniAAM18},
  doi       = {10.1007/s10489-017-1012-2},
  timestamp = {Fri, 09 Feb 2018 16:09:28 +0100},
  url       = {https://doi.org/10.1007/s10489-017-1012-2},
}

@Article{Luo2015Tensor,
  author    = {Yong Luo and Dacheng Tao and Kotagiri Ramamohanarao and Chao Xu and Yonggang Wen},
  title     = {Tensor Canonical Correlation Analysis for Multi-View Dimension Reduction},
  journal   = {{IEEE} Trans. Knowl. Data Eng.},
  year      = {2015},
  volume    = {27},
  number    = {11},
  pages     = {3111--3124},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tkde/LuoTR0W15},
  doi       = {10.1109/TKDE.2015.2445757},
  timestamp = {2019.05.16},
  url       = {https://doi.org/10.1109/TKDE.2015.2445757},
}

@Article{Fu1998Penalized,
  author    = {Fu, Wenjiang J.},
  title     = {Penalized regressions: the bridge versus the lasso},
  journal   = {J. Comput. Graph. Statist.},
  year      = {1998},
  volume    = {7},
  number    = {3},
  pages     = {397--416},
  issn      = {1061-8600},
  doi       = {10.2307/1390712},
  fjournal  = {Journal of Computational and Graphical Statistics},
  mrclass   = {62J07},
  mrnumber  = {1646710},
  timestamp = {2019.05.17},
  url       = {https://doi.org/10.2307/1390712},
}

@Article{Hayden2006Patterns,
  author    = {Elizabeth P. Hayden and Ryan E. Wiegand and Eric T. Meyer and Lance O. Bauer and Sean J. O\'Connor and John I. Nurnberger and David B. Chorlian and Bernice Porjesz and Henri Begleiter},
  title     = {Patterns of Regional Brain Activity in Alcohol-Dependent Subjects},
  journal   = {Alcoholism: Clinical and Experimental Research},
  year      = {2006},
  volume    = {30},
  number    = {12},
  pages     = {1986--1991},
  month     = {oct},
  doi       = {10.1111/j.1530-0277.2006.00244.x},
  publisher = {Wiley},
  timestamp = {2019.05.21},
}

@Article{Ingber1998Statistical,
  author    = {Ingber, L.},
  title     = {Statistical mechanics of neocortical interactions: training and testing canonical momenta indicators of {EEG}},
  journal   = {Math. Comput. Modelling},
  year      = {1998},
  volume    = {27},
  number    = {3},
  pages     = {33--64},
  issn      = {0895-7177},
  doi       = {10.1016/S0895-7177(97)00265-3},
  fjournal  = {Mathematical and Computer Modelling},
  mrclass   = {82B80 (92C20 92C50)},
  mrnumber  = {1609439},
  timestamp = {2019.05.21},
  url       = {https://doi.org/10.1016/S0895-7177(97)00265-3},
}

@Article{Knyazev2007Motivation,
  author    = {Gennady G. Knyazev},
  title     = {Motivation, emotion, and their inhibitory control mirrored in brain oscillations},
  journal   = {Neuroscience {\&} Biobehavioral Reviews},
  year      = {2007},
  volume    = {31},
  number    = {3},
  pages     = {377--395},
  month     = {jan},
  doi       = {10.1016/j.neubiorev.2006.10.004},
  publisher = {Elsevier {BV}},
  timestamp = {2019.05.21},
}

@Article{Ingber1997Statistical,
  author    = {Lester Ingber},
  title     = {Statistical mechanics of neocortical interactions: Canonical momenta indicators of electroencephalography},
  journal   = {Physical Review E},
  year      = {1997},
  volume    = {55},
  number    = {4},
  pages     = {4578--4593},
  month     = {apr},
  doi       = {10.1103/physreve.55.4578},
  publisher = {American Physical Society ({APS})},
  timestamp = {2019.05.21},
}

@Article{Newman2003structure,
  author     = {Newman, M. E. J.},
  title      = {The structure and function of complex networks},
  journal    = {SIAM Rev.},
  year       = {2003},
  volume     = {45},
  number     = {2},
  pages      = {167--256},
  issn       = {0036-1445},
  doi        = {10.1137/S003614450342480},
  fjournal   = {SIAM Review},
  mrclass    = {05C90 (68M10 90B10 90B18 91D30 92B20 94C15)},
  mrnumber   = {2010377},
  mrreviewer = {Jerrold W. Grossman},
  timestamp  = {2019.05.21},
  url        = {https://doi.org/10.1137/S003614450342480},
}

@Article{Parikh2014Proximal,
  author    = {Neal Parikh and Stephen P. Boyd},
  title     = {Proximal Algorithms},
  journal   = {Foundations and Trends in Optimization},
  year      = {2014},
  volume    = {1},
  number    = {3},
  pages     = {127--239},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/ftopt/ParikhB14},
  doi       = {10.1561/2400000003},
  file      = {:Parikh2014Proximal.pdf:PDF},
  timestamp = {2019.05.21},
  url       = {https://doi.org/10.1561/2400000003},
}

@InProceedings{Wang2018Direct,
  author    = {Yuhao Wang and Chandler Squires and Anastasiya Belyaeva and Caroline Uhler},
  title     = {Direct Estimation of Differences in Causal Graphs},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al, Canada.},
  year      = {2018},
  editor    = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  pages     = {3774--3785},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/WangSBU18},
  timestamp = {2019.05.21},
  url       = {http://papers.nips.cc/paper/7634-direct-estimation-of-differences-in-causal-graphs},
}

@Article{Zhang2018Dynamic,
  author        = {Chen Zhang and Hao Yan and Seungho Lee and Jianjun Shi},
  title         = {Dynamic Multivariate Functional Data Modeling via Sparse Subspace Learning},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1804.03797},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-03797},
  eprint        = {1804.03797},
  timestamp     = {2019.05.21},
  url           = {http://arxiv.org/abs/1804.03797},
}

@Article{Zhang1995Event,
  author    = {Xiao Lei Zhang and Henri Begleiter and Bernice Porjesz and Wenyu Wang and Ann Litke},
  title     = {Event related potentials during object recognition tasks},
  journal   = {Brain Research Bulletin},
  year      = {1995},
  volume    = {38},
  number    = {6},
  pages     = {531--538},
  month     = {jan},
  doi       = {10.1016/0361-9230(95)02023-5},
  publisher = {Elsevier {BV}},
  timestamp = {2019.05.21},
}

@Article{Yuan2017Differential,
  author    = {Yuan, Huili and Xi, Ruibin and Chen, Chong and Deng, Minghua},
  title     = {Differential network analysis via lasso penalized {D}-trace loss},
  journal   = {Biometrika},
  year      = {2017},
  volume    = {104},
  number    = {4},
  pages     = {755--770},
  issn      = {0006-3444},
  doi       = {10.1093/biomet/asx049},
  file      = {:Yuan2017Differential.pdf:PDF},
  fjournal  = {Biometrika},
  mrclass   = {62J07 (62H99 62P10)},
  mrnumber  = {3737302},
  timestamp = {2019.05.22},
  url       = {https://doi.org/10.1093/biomet/asx049},
}

@Article{Absil2005Convergence,
  author     = {Absil, P.-A. and Mahony, R. and Andrews, B.},
  title      = {Convergence of the iterates of descent methods for analytic cost functions},
  journal    = {SIAM J. Optim.},
  year       = {2005},
  volume     = {16},
  number     = {2},
  pages      = {531--547},
  issn       = {1052-6234},
  doi        = {10.1137/040605266},
  file       = {:Absil2005Convergence.pdf:PDF},
  fjournal   = {SIAM Journal on Optimization},
  mrclass    = {90C26 (37N40 65K10 90C52)},
  mrnumber   = {2197994},
  mrreviewer = {Oliver Stein},
  timestamp  = {2019.05.29},
  url        = {https://doi.org/10.1137/040605266},
}

@Book{Absil2008Optimization,
  title      = {Optimization algorithms on matrix manifolds},
  publisher  = {Princeton University Press, Princeton, NJ},
  year       = {2008},
  author     = {Absil, P.-A. and Mahony, R. and Sepulchre, R.},
  isbn       = {978-0-691-13298-3},
  note       = {With a foreword by Paul Van Dooren},
  doi        = {10.1515/9781400830244},
  mrclass    = {90-02 (58E17 90C30 90C52)},
  mrnumber   = {2364186},
  mrreviewer = {Anders Linn\'{e}r},
  pages      = {xvi+224},
  timestamp  = {2019.05.29},
  url        = {https://doi.org/10.1515/9781400830244},
}

@InProceedings{Allen-Zhu2017Doubly,
  author    = {Zeyuan Allen{-}Zhu and Yuanzhi Li},
  title     = {Doubly Accelerated Methods for Faster {CCA} and Generalized Eigendecomposition},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {98--106},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/Allen-ZhuL17},
  file      = {:Allen-Zhu2017Doubly.pdf:PDF},
  timestamp = {2019.05.29},
  url       = {http://proceedings.mlr.press/v70/allen-zhu17b.html},
}

@Article{Cai2019Differential,
  author   = {Cai, T. T. and Li, H. and Ma, J. and Xia, Y.},
  title    = {Differential {M}arkov random field analysis with an application to detecting differential microbial community networks},
  journal  = {Biometrika},
  year     = {2019},
  volume   = {106},
  number   = {2},
  pages    = {401--416},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/asz012},
  file     = {:Cai2019Differential.pdf:PDF},
  fjournal = {Biometrika},
  mrclass  = {92D40 (62H15 62H99 92B15)},
  mrnumber = {3949311},
  url      = {https://doi.org/10.1093/biomet/asz012},
}

@Article{Li2019Likelihood,
  author    = {Chunlin Li and Xiaotong Shen and Wei Pan},
  title     = {Likelihood ratio tests for a large directed acyclic graph},
  journal   = {Journal of the American Statistical Association},
  year      = {2019},
  volume    = {0},
  number    = {ja},
  pages     = {1-36},
  doi       = {10.1080/01621459.2019.1623042},
  eprint    = {https://doi.org/10.1080/01621459.2019.1623042},
  file      = {:Li2019Likelihood.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1080/01621459.2019.1623042
    
},
}

@Article{Zheng2019Model,
  author    = {Chao Zheng and Davide Ferrari and Yuhong Yang},
  title     = {Model Selection confidence sets by likelihood ratio testing},
  journal   = {Statistica Sinica},
  year      = {2019},
  doi       = {10.5705/ss.202017.0006},
  file      = {:Zheng2019Model.pdf:PDF},
  publisher = {Institute of Statistical Science},
}

@Article{Jiang2013Central,
  author     = {Jiang, Tiefeng and Yang, Fan},
  title      = {Central limit theorems for classical likelihood ratio tests for high-dimensional normal distributions},
  journal    = {Ann. Statist.},
  year       = {2013},
  volume     = {41},
  number     = {4},
  pages      = {2029--2074},
  issn       = {0090-5364},
  doi        = {10.1214/13-AOS1134},
  file       = {:Jiang2013Central.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62H15 (60F05 62-07 62H10)},
  mrnumber   = {3127857},
  mrreviewer = {Wenxin Zhou},
  url        = {https://doi.org/10.1214/13-AOS1134},
}

@Article{Efron1979Bootstrap,
  author     = {Efron, B.},
  title      = {Bootstrap methods: another look at the jackknife},
  journal    = {Ann. Statist.},
  year       = {1979},
  volume     = {7},
  number     = {1},
  pages      = {1--26},
  issn       = {0090-5364},
  file       = {:Absil2005Convergence.pdf:PDF},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62E15 (62G05 62H30 62J05)},
  mrnumber   = {515681},
  mrreviewer = {B. Ya. Levit},
  url        = {http://links.jstor.org/sici?sici=0090-5364(197901)7:1<1:BMALAT>2.0.CO;2-6&origin=MSN},
}

@InProceedings{Sen2017Model,
  author    = {Rajat Sen and Ananda Theertha Suresh and Karthikeyan Shanmugam and Alexandros G. Dimakis and Sanjay Shakkottai},
  title     = {Model-Powered Conditional Independence Test},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}},
  year      = {2017},
  editor    = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
  pages     = {2955--2965},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/SenSSDS17},
  file      = {:Sen2017Model.pdf:PDF},
  timestamp = {Mon, 27 Nov 2017 12:38:48 +0100},
  url       = {http://papers.nips.cc/paper/6888-model-powered-conditional-independence-test},
}

@Article{Shah2018Hardness,
  author      = {Rajen D. Shah and Jonas Peters},
  title       = {The Hardness of Conditional Independence Testing and the Generalised Covariance Measure},
  journal     = {arxiv},
  year        = {2018},
  abstract    = {It is a common saying that testing for conditional independence, i.e., testing whether X is independent of Y, given Z, is a hard statistical problem if Z is a continuous random variable. In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem setting may be judged easily. To address this need, we propose in the case where X and Y are univariate to nonlinearly regress X on Z, and Y on Z and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means X given Z, and Y given Z, at a slow rate. We extend the methodology to handle settings where X and Y may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code will be available as an R package.},
  date        = {2018-04-19},
  eprint      = {http://arxiv.org/abs/1804.07203v2},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  keywords    = {math.ST, stat.TH},
}

@InProceedings{Arora2012Stochastic,
  author    = {Raman Arora and Andrew Cotter and Karen Livescu and Nathan Srebro},
  title     = {Stochastic optimization for {PCA} and {PLS}},
  booktitle = {50th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2012, Allerton Park {\&} Retreat Center, Monticello, IL, USA, October 1-5, 2012},
  year      = {2012},
  pages     = {861--868},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/allerton/AroraCLS12},
  doi       = {10.1109/Allerton.2012.6483308},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1109/Allerton.2012.6483308},
}

@InProceedings{Andrew2013Deep,
  author    = {Galen Andrew and Raman Arora and Jeff A. Bilmes and Karen Livescu},
  title     = {Deep Canonical Correlation Analysis},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013},
  year      = {2013},
  volume    = {28},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {1247--1255},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/AndrewABL13},
  file      = {:Andrew2013Deep.pdf:PDF},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v28/andrew13.html},
}

@InProceedings{Bhatia2018Gen,
  author    = {Kush Bhatia and Aldo Pacchiano and Nicolas Flammarion and Peter L. Bartlett and Michael I. Jordan},
  title     = {Gen-Oja: Simple {\&} Efficient Algorithm for Streaming Generalized Eigenvector Computation},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al, Canada.},
  year      = {2018},
  editor    = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  pages     = {7016--7025},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/BhatiaPFBJ18},
  timestamp = {2019.06.03},
  url       = {http://papers.nips.cc/paper/7933-gen-oja-simple-efficient-algorithm-for-streaming-generalized-eigenvector-computation},
}

@Article{Novembre2008Genes,
  author    = {John Novembre and Toby Johnson and Katarzyna Bryc and Zolt{\'{a}}n Kutalik and Adam R. Boyko and Adam Auton and Amit Indap and Karen S. King and Sven Bergmann and Matthew R. Nelson and Matthew Stephens and Carlos D. Bustamante},
  title     = {Genes mirror geography within Europe},
  journal   = {Nature},
  year      = {2008},
  volume    = {456},
  number    = {7218},
  pages     = {98--101},
  month     = {aug},
  doi       = {10.1038/nature07331},
  publisher = {Springer Science and Business Media {LLC}},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1038/nature07331},
}

@Article{Brown2018Expression,
  author    = {Brielin C. Brown and Nicolas L. Bray and Lior Pachter},
  title     = {Expression reflects population structure},
  journal   = {{PLOS} Genetics},
  year      = {2018},
  volume    = {14},
  number    = {12},
  pages     = {e1007841},
  month     = {dec},
  doi       = {10.1371/journal.pgen.1007841},
  editor    = {Anna Di Rienzo},
  publisher = {Public Library of Science ({PLoS})},
  timestamp = {2019.06.03},
}

@Article{DeLathauwer2000multilinear,
  author     = {De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
  title      = {A multilinear singular value decomposition},
  journal    = {SIAM J. Matrix Anal. Appl.},
  year       = {2000},
  volume     = {21},
  number     = {4},
  pages      = {1253--1278},
  issn       = {0895-4798},
  doi        = {10.1137/S0895479896305696},
  fjournal   = {SIAM Journal on Matrix Analysis and Applications},
  mrclass    = {15A69 (15A18 65F30)},
  mrnumber   = {1780272},
  mrreviewer = {Ilse C. F. Ipsen},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.1137/S0895479896305696},
}

@Article{DeLathauwer2000best,
  author    = {De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
  title     = {On the best rank-1 and rank-{$(R_1,R_2,\cdots,R_N)$} approximation of higher-order tensors},
  journal   = {SIAM J. Matrix Anal. Appl.},
  year      = {2000},
  volume    = {21},
  number    = {4},
  pages     = {1324--1342},
  issn      = {0895-4798},
  doi       = {10.1137/S0895479898346995},
  fjournal  = {SIAM Journal on Matrix Analysis and Applications},
  mrclass   = {15A69 (65F30)},
  mrnumber  = {1780276},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1137/S0895479898346995},
}

@Article{Miles1960Generalized,
  author     = {Miles, Jr., E. P.},
  title      = {Generalized {F}ibonacci numbers and associated matrices},
  journal    = {Amer. Math. Monthly},
  year       = {1960},
  volume     = {67},
  pages      = {745--752},
  issn       = {0002-9890},
  doi        = {10.2307/2308649},
  fjournal   = {The American Mathematical Monthly},
  mrclass    = {10.07},
  mrnumber   = {0123521},
  mrreviewer = {T. Estermann},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.2307/2308649},
}

@Article{Fu2017Scalable,
  author    = {Xiao Fu and Kejun Huang and Mingyi Hong and Nicholas D. Sidiropoulos and Anthony Man{-}Cho So},
  title     = {Scalable and Flexible Multiview {MAX-VAR} Canonical Correlation Analysis},
  journal   = {{IEEE} Trans. Signal Processing},
  year      = {2017},
  volume    = {65},
  number    = {16},
  pages     = {4150--4165},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tsp/FuHHSS17},
  doi       = {10.1109/TSP.2017.2698365},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1109/TSP.2017.2698365},
}

@InProceedings{Ge2016Efficient,
  author    = {Rong Ge and Chi Jin and Sham M. Kakade and Praneeth Netrapalli and Aaron Sidford},
  title     = {Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  year      = {2016},
  editor    = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {2741--2750},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/GeJKNS16},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v48/geb16.html},
}

@Article{Guan2018Convergence,
  author     = {Guan, Yu and Chu, Moody T. and Chu, Delin},
  title      = {Convergence analysis of an {SVD}-based algorithm for the best rank-1 tensor approximation},
  journal    = {Linear Algebra Appl.},
  year       = {2018},
  volume     = {555},
  pages      = {53--69},
  issn       = {0024-3795},
  doi        = {10.1016/j.laa.2018.06.006},
  fjournal   = {Linear Algebra and its Applications},
  mrclass    = {65F15 (15A69)},
  mrnumber   = {3834191},
  mrreviewer = {H. T. Lau},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.1016/j.laa.2018.06.006},
}

@InProceedings{Haeffele2014Structured,
  author    = {Benjamin D. Haeffele and Eric Young and Ren{\'{e}} Vidal},
  title     = {Structured Low-Rank Matrix Factorization: Optimality, Algorithm, and Applications to Image Processing},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning, {ICML} 2014, Beijing, China, 21-26 June 2014},
  year      = {2014},
  volume    = {32},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {2007--2015},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/HaeffeleYV14},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v32/haeffele14.html},
}

@Article{Hardoon2011Sparse,
  author    = {David R. Hardoon and John Shawe{-}Taylor},
  title     = {Sparse canonical correlation analysis},
  journal   = {Machine Learning},
  year      = {2011},
  volume    = {83},
  number    = {3},
  pages     = {331--353},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/ml/HardoonS11},
  doi       = {10.1007/s10994-010-5222-7},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1007/s10994-010-5222-7},
}

@Article{Hardoon2004Canonical,
  author    = {David R. Hardoon and S{\'{a}}ndor Szedm{\'{a}}k and John Shawe{-}Taylor},
  title     = {Canonical Correlation Analysis: An Overview with Application to Learning Methods},
  journal   = {Neural Computation},
  year      = {2004},
  volume    = {16},
  number    = {12},
  pages     = {2639--2664},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/neco/HardoonSS04},
  doi       = {10.1162/0899766042321814},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1162/0899766042321814},
}

@Article{Hotelling1936Relations,
  author    = {Harold Hotelling},
  title     = {Relations Between Two Sets of Variates},
  journal   = {Biometrika},
  year      = {1936},
  volume    = {28},
  number    = {3/4},
  pages     = {321},
  month     = {dec},
  doi       = {10.2307/2333955},
  publisher = {{JSTOR}},
  timestamp = {2019.06.03},
}

@Article{Hu2018Convergence,
  author    = {Hu, Shenglong and Li, Guoyin},
  title     = {Convergence rate analysis for the higher order power method in best rank one approximations of tensors},
  journal   = {Numer. Math.},
  year      = {2018},
  volume    = {140},
  number    = {4},
  pages     = {993--1031},
  issn      = {0029-599X},
  doi       = {10.1007/s00211-018-0981-3},
  fjournal  = {Numerische Mathematik},
  mrclass   = {65F15 (15A18 15A69)},
  mrnumber  = {3864708},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1007/s00211-018-0981-3},
}

@Article{Jendoubi2019whitening,
  author    = {Takoua Jendoubi and Korbinian Strimmer},
  title     = {A whitening approach to probabilistic canonical correlation analysis for omics data integration},
  journal   = {{BMC} Bioinformatics},
  year      = {2019},
  volume    = {20},
  number    = {1},
  month     = {jan},
  doi       = {10.1186/s12859-018-2572-9},
  publisher = {Springer Nature},
  timestamp = {2019.06.03},
}

@InProceedings{Jin2015Cross,
  author    = {Cheng Jin and Wenhui Mao and Ruiqi Zhang and Yuejie Zhang and Xiangyang Xue},
  title     = {Cross-Modal Image Clustering via Canonical Correlation Analysis},
  booktitle = {Proceedings of the Twenty-Ninth {AAAI} Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, {USA.}},
  year      = {2015},
  editor    = {Blai Bonet and Sven Koenig},
  pages     = {151--159},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/JinMZZX15},
  timestamp = {2019.06.03},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9580},
}

@Article{Kalman1982Generalized,
  author     = {Kalman, Dan},
  title      = {Generalized {F}ibonacci numbers by matrix methods},
  journal    = {Fibonacci Quart.},
  year       = {1982},
  volume     = {20},
  number     = {1},
  pages      = {73--76},
  issn       = {0015-0517},
  fjournal   = {The Fibonacci Quarterly. Official Organ of the Fibonacci Association},
  mrclass    = {10A35 (05A99 15A99)},
  mrnumber   = {660765},
  mrreviewer = {Neville Robbins},
  timestamp  = {2019.06.03},
}

@InProceedings{Kim2007Tensor,
  author    = {Tae{-}Kyun Kim and Shu{-}Fai Wong and Roberto Cipolla},
  title     = {Tensor Canonical Correlation Analysis for Action Classification},
  booktitle = {2007 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition {(CVPR} 2007), 18-23 June 2007, Minneapolis, Minnesota, {USA}},
  year      = {2007},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/cvpr/KimWC07},
  doi       = {10.1109/CVPR.2007.383137},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1109/CVPR.2007.383137},
}

@Article{Kolda2009Tensor,
  author     = {Kolda, Tamara G. and Bader, Brett W.},
  title      = {Tensor decompositions and applications},
  journal    = {SIAM Rev.},
  year       = {2009},
  volume     = {51},
  number     = {3},
  pages      = {455--500},
  issn       = {0036-1445},
  doi        = {10.1137/07070111X},
  fjournal   = {SIAM Review},
  mrclass    = {15A72 (15A69 65F99)},
  mrnumber   = {2535056},
  mrreviewer = {Maria Chiara Brambilla},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.1137/07070111X},
}

@Article{Kruger2003Canonical,
  author    = {Uwe Kruger and S. Joe Qin},
  title     = {Canonical Correlation Partial Least Squares},
  journal   = {{IFAC} Proceedings Volumes},
  year      = {2003},
  volume    = {36},
  number    = {16},
  pages     = {1603--1608},
  month     = {sep},
  doi       = {10.1016/s1474-6670(17)34989-3},
  publisher = {Elsevier {BV}},
  timestamp = {2019.06.03},
}

@Article{Li2018Calculus,
  author    = {Li, Guoyin and Pong, Ting Kei},
  title     = {Calculus of the exponent of {K}urdyka-\L ojasiewicz inequality and its applications to linear convergence of first-order methods},
  journal   = {Found. Comput. Math.},
  year      = {2018},
  volume    = {18},
  number    = {5},
  pages     = {1199--1232},
  issn      = {1615-3375},
  doi       = {10.1007/s10208-017-9366-8},
  fjournal  = {Foundations of Computational Mathematics. The Journal of the Society for the Foundations of Computational Mathematics},
  mrclass   = {90C26 (90C25)},
  mrnumber  = {3857908},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1007/s10208-017-9366-8},
}

@InProceedings{Li2019Nonconvex,
  author    = {Li, Yuanxin and Ma, Cong and Chen, Yuxin and Chi, Yuejie},
  title     = {Nonconvex Matrix Factorization from Rank-One Measurements},
  booktitle = {Proceedings of Machine Learning Research},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1496--1505},
  month     = {16--18 Apr},
  publisher = {PMLR},
  abstract  = {We consider the problem of recovering low-rank matrices from random rank-one measurements, which spans numerous applications including phase retrieval, quantum state tomography, and learning shallow neural networks with quadratic activations, among others. Our approach is to directly estimate the low-rank factor by minimizing a nonconvex least-squares loss function via vanilla gradient descent, following a tailored spectral initialization. When the true rank is small, this algorithm is guaranteed to converge to the ground truth (up to global ambiguity) with near-optimal sample and computational complexities with respect to the problem size. To the best of our knowledge, this is the first theoretical guarantee that achieves near optimality in both metrics. In particular, the key enabler of near-optimal computational guarantees is an implicit regularization phenomenon: without explicit regularization, both spectral initialization and the gradient descent iterates automatically stay within a region incoherent with the measurement vectors. This feature allows one to employ much more aggressive step sizes compared with the ones suggested in prior literature, without the need of sample splitting.},
  file      = {li19e.pdf:http\://proceedings.mlr.press/v89/li19e/li19e.pdf:PDF},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v89/li19e.html},
}

@Article{Li2015convergence,
  author    = {Li, Zhening and Uschmajew, Andr\'{e} and Zhang, Shuzhong},
  title     = {On convergence of the maximum block improvement method},
  journal   = {SIAM J. Optim.},
  year      = {2015},
  volume    = {25},
  number    = {1},
  pages     = {210--233},
  issn      = {1052-6234},
  doi       = {10.1137/130939110},
  fjournal  = {SIAM Journal on Optimization},
  mrclass   = {90C26 (15A69 41A25 49M27)},
  mrnumber  = {3300411},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1137/130939110},
}

@InProceedings{Liu2016Quadratic,
  author    = {Huikang Liu and Weijie Wu and Anthony Man{-}Cho So},
  title     = {Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  year      = {2016},
  editor    = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {1158--1167},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/LiuWS16},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v48/liue16.html},
}

@Article{Liu2018Quadratic,
  author    = {Huikang Liu and Anthony Man-Cho So and Weijie Wu},
  title     = {Quadratic optimization with orthogonality constraint: explicit {\L}ojasiewicz exponent and linear convergence of retraction-based line-search and stochastic variance-reduced gradient methods},
  journal   = {Mathematical Programming},
  year      = {2018},
  month     = {jun},
  doi       = {10.1007/s10107-018-1285-1},
  publisher = {Springer Science and Business Media {LLC}},
  timestamp = {2019.06.03},
}

@InProceedings{Lopez-Paz2014Randomized,
  author    = {David Lopez{-}Paz and Suvrit Sra and Alexander J. Smola and Zoubin Ghahramani and Bernhard Sch{\"{o}}lkopf},
  title     = {Randomized Nonlinear Component Analysis},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning, {ICML} 2014, Beijing, China, 21-26 June 2014},
  year      = {2014},
  volume    = {32},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {1359--1367},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/Lopez-PazSSGS14},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v32/lopez-paz14.html},
}

@InProceedings{Michaeli2016Nonparametric,
  author    = {Tomer Michaeli and Weiran Wang and Karen Livescu},
  title     = {Nonparametric Canonical Correlation Analysis},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  year      = {2016},
  editor    = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {1967--1976},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/MichaeliWL16},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v48/michaeli16.html},
}

@InProceedings{Park2017Non,
  author    = {Dohyung Park and Anastasios Kyrillidis and Constantine Caramanis and Sujay Sanghavi},
  title     = {Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, {AISTATS} 2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}},
  year      = {2017},
  editor    = {Aarti Singh and Xiaojin (Jerry) Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  pages     = {65--74},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/ParkKCS17},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v54/park17a.html},
}

@InProceedings{Regalia2000higher,
  author    = {Phillip A. Regalia and Eleftherios Kofidis},
  title     = {The higher-order power method revisited: convergence proofs and effective initialization},
  booktitle = {{IEEE} International Conference on Acoustics, Speech, and Signal Processing. {ICASSP} 2000, 5-9 June, 2000, Hilton Hotel and Convention Center, Istanbul, Turkey},
  year      = {2000},
  pages     = {2709--2712},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icassp/RegaliaK00},
  doi       = {10.1109/ICASSP.2000.861047},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1109/ICASSP.2000.861047},
}

@Article{Schneider2015Convergence,
  author     = {Schneider, Reinhold and Uschmajew, Andr\'{e}},
  title      = {Convergence results for projected line-search methods on varieties of low-rank matrices via \L ojasiewicz inequality},
  journal    = {SIAM J. Optim.},
  year       = {2015},
  volume     = {25},
  number     = {1},
  pages      = {622--646},
  issn       = {1052-6234},
  doi        = {10.1137/140957822},
  fjournal   = {SIAM Journal on Optimization},
  mrclass    = {90C30 (65F99 90C52)},
  mrnumber   = {3323551},
  mrreviewer = {Kurt Marti},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.1137/140957822},
}

@Article{Sharma2006Deflation,
  author    = {Sanjay K. Sharma and Uwe Kruger and George W. Irwin},
  title     = {Deflation based nonlinear canonical correlation analysis},
  journal   = {Chemometrics and Intelligent Laboratory Systems},
  year      = {2006},
  volume    = {83},
  number    = {1},
  pages     = {34--43},
  month     = {jul},
  doi       = {10.1016/j.chemolab.2005.12.008},
  publisher = {Elsevier {BV}},
  timestamp = {2019.06.03},
}

@InProceedings{Sun2017decomposition,
  author    = {Chuangchuang Sun and Ran Dai},
  title     = {A decomposition method for nonconvex quadratically constrained quadratic programs},
  booktitle = {2017 American Control Conference, {ACC} 2017, Seattle, WA, USA, May 24-26, 2017},
  year      = {2017},
  pages     = {4631--4636},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/amcc/SunD17},
  doi       = {10.23919/ACC.2017.7963670},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.23919/ACC.2017.7963670},
}

@Article{Sun2007Class,
  author    = {Tingkai Sun and Songcan Chen},
  title     = {Class label versus sample label-based {CCA}},
  journal   = {Applied Mathematics and Computation},
  year      = {2007},
  volume    = {185},
  number    = {1},
  pages     = {272--283},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/amc/SunC07a},
  doi       = {10.1016/j.amc.2006.06.103},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1016/j.amc.2006.06.103},
}

@Article{Tan2018Sparse,
  author    = {Tan, Kean Ming and Wang, Zhaoran and Liu, Han and Zhang, Tong},
  title     = {Sparse generalized eigenvalue problem: optimal statistical rates via truncated {R}ayleigh flow},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2018},
  volume    = {80},
  number    = {5},
  pages     = {1057--1086},
  issn      = {1369-7412},
  doi       = {10.1111/rssb.12291},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62H12 (62H20 62H30)},
  mrnumber  = {3874310},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1111/rssb.12291},
}

@Article{Uschmajew2015new,
  author     = {Uschmajew, Andr\'{e}},
  title      = {A new convergence proof for the higher-order power method and generalizations},
  journal    = {Pac. J. Optim.},
  year       = {2015},
  volume     = {11},
  number     = {2},
  pages      = {309--321},
  issn       = {1348-9151},
  fjournal   = {Pacific Journal of Optimization. An International Journal},
  mrclass    = {65F15 (15A69 65K05 68W25 90C26)},
  mrnumber   = {3350202},
  mrreviewer = {Jos\'{e}-Javier Mart\'{\i}nez},
  timestamp  = {2019.06.03},
}

@InProceedings{Tang2017Acoustic,
  author    = {Qingming Tang and Weiran Wang and Karen Livescu},
  title     = {Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis},
  booktitle = {Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017},
  year      = {2017},
  editor    = {Francisco Lacerda},
  pages     = {1656--1660},
  publisher = {{ISCA}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/interspeech/TangWL17},
  timestamp = {2019.06.03},
  url       = {http://www.isca-speech.org/archive/Interspeech\_2017/abstracts/1581.html},
}

@Article{Wen2012Solving,
  author    = {Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
  title     = {Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
  journal   = {Math. Program. Comput.},
  year      = {2012},
  volume    = {4},
  number    = {4},
  pages     = {333--361},
  issn      = {1867-2949},
  doi       = {10.1007/s12532-012-0044-1},
  fjournal  = {Mathematical Programming Computation},
  mrclass   = {65F30 (68Q32 90C06)},
  mrnumber  = {3006618},
  timestamp = {2019.06.03},
  url       = {https://doi.org/10.1007/s12532-012-0044-1},
}

@Article{Wolfram1998Solving,
  author     = {Wolfram, D. A.},
  title      = {Solving generalized {F}ibonacci recurrences},
  journal    = {Fibonacci Quart.},
  year       = {1998},
  volume     = {36},
  number     = {2},
  pages      = {129--145},
  issn       = {0015-0517},
  fjournal   = {The Fibonacci Quarterly. The Official Journal of the Fibonacci Association},
  mrclass    = {11B39 (39A10)},
  mrnumber   = {1622060},
  mrreviewer = {Walter Carlip},
  timestamp  = {2019.06.03},
}

@Article{Sun2013survey,
  author    = {Shiliang Sun},
  title     = {A survey of multi-view machine learning},
  journal   = {Neural Computing and Applications},
  year      = {2013},
  volume    = {23},
  number    = {7-8},
  pages     = {2031--2038},
  month     = {feb},
  doi       = {10.1007/s00521-013-1362-6},
  publisher = {Springer Science and Business Media {LLC}},
  timestamp = {2019.06.03},
}

@InProceedings{Xu2018Accelerated,
  author    = {Peng Xu and Bryan D. He and Christopher De Sa and Ioannis Mitliagkas and Christopher R{\'{e}}},
  title     = {Accelerated Stochastic Power Iteration},
  booktitle = {International Conference on Artificial Intelligence and Statistics, {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain},
  year      = {2018},
  editor    = {Amos J. Storkey and Fernando P{\'{e}}rez{-}Cruz},
  volume    = {84},
  series    = {Proceedings of Machine Learning Research},
  pages     = {58--67},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/XuHSMR18},
  timestamp = {2019.06.03},
  url       = {http://proceedings.mlr.press/v84/xu18a.html},
}

@Article{Xu2013block,
  author     = {Xu, Yangyang and Yin, Wotao},
  title      = {A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion},
  journal    = {SIAM J. Imaging Sci.},
  year       = {2013},
  volume     = {6},
  number     = {3},
  pages      = {1758--1789},
  issn       = {1936-4954},
  doi        = {10.1137/120887795},
  fjournal   = {SIAM Journal on Imaging Sciences},
  mrclass    = {90C26 (94A08)},
  mrnumber   = {3105787},
  mrreviewer = {Jin Ling Zhao},
  timestamp  = {2019.06.03},
  url        = {https://doi.org/10.1137/120887795},
}

@Article{Yang2004Two,
  author    = {Jian Yang and David Zhang and Alejandro F. Frangi and Jing-Yu Yang},
  title     = {Two-Dimensional {PCA:} {A} New Approach to Appearance-Based Face Representation and Recognition},
  journal   = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  year      = {2004},
  volume    = {26},
  number    = {1},
  pages     = {131--137},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/pami/YangZFY04},
  doi       = {10.1109/TPAMI.2004.10004},
  timestamp = {Fri, 15 Sep 2017 13:06:49 +0200},
  url       = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2004.10004},
}

@InProceedings{Yang2008Two,
  author    = {Mao{-}Long Yang and Quan{-}Sen Sun and De{-}Shen Xia},
  title     = {Two-Dimensional Partial Least Squares and Its Application in Image Recognition},
  booktitle = {Advanced Intelligent Computing Theories and Applications. With Aspects of Contemporary Intelligent Computing Techniques, 4th International Conference on Intelligent Computing, {ICIC} 2008, Shanghai, China, September 15-18, 2008, Proceedings},
  year      = {2008},
  editor    = {De{-}Shuang Huang and Donald C. Wunsch II and Daniel S. Levine and Kang{-}Hyun Jo},
  volume    = {15},
  series    = {Communications in Computer and Information Science},
  pages     = {208--215},
  publisher = {Springer},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icic/YangSX08},
  doi       = {10.1007/978-3-540-85930-7\_28},
  timestamp = {Sat, 30 Dec 2017 13:59:02 +0100},
  url       = {https://doi.org/10.1007/978-3-540-85930-7\_28},
}

@InProceedings{Ye2004Two,
  author    = {Jieping Ye and Ravi Janardan and Qi Li},
  title     = {Two-Dimensional Linear Discriminant Analysis},
  booktitle = {Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, {NIPS} 2004, December 13-18, 2004, Vancouver, British Columbia, Canada]},
  year      = {2004},
  pages     = {1569--1576},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/YeJL04},
  timestamp = {Wed, 24 Jan 2018 13:31:06 +0100},
  url       = {http://papers.nips.cc/paper/2547-two-dimensional-linear-discriminant-analysis},
}

@InProceedings{Yger2012Adaptive,
  author    = {Florian Yger and Maxime Berar and Gilles Gasso and Alain Rakotomamonjy},
  title     = {Adaptive Canonical Correlation Analysis Based On Matrix Manifolds},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning, {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  year      = {2012},
  publisher = {icml.cc / Omnipress},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/YgerBGR12},
  timestamp = {Wed, 03 Apr 2019 17:43:34 +0200},
  url       = {http://icml.cc/2012/papers/565.pdf},
}

@Article{Zhang20052D2PCA,
  author    = {Daoqiang Zhang and Zhi{-}Hua Zhou},
  title     = {(2D)\({}^{\mbox{2}}\)PCA: Two-directional two-dimensional {PCA} for efficient face representation and recognition},
  journal   = {Neurocomputing},
  year      = {2005},
  volume    = {69},
  number    = {1-3},
  pages     = {224--231},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/ijon/ZhangZ05a},
  doi       = {10.1016/j.neucom.2005.06.004},
  timestamp = {Sat, 20 May 2017 00:25:05 +0200},
  url       = {https://doi.org/10.1016/j.neucom.2005.06.004},
}

@Article{Pearson1901lines,
  author    = {Karl Pearson},
  title     = {On lines and planes of closest fit to systems of points in space},
  journal   = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  year      = {1901},
  volume    = {2},
  number    = {11},
  pages     = {559--572},
  month     = {nov},
  doi       = {10.1080/14786440109462720},
  publisher = {Informa {UK} Limited},
}

@Book{Gupta2000Matrix,
  title      = {Matrix variate distributions},
  publisher  = {Chapman \& Hall/CRC, Boca Raton, FL},
  year       = {2000},
  author     = {Gupta, A. K. and Nagar, D. K.},
  volume     = {104},
  series     = {Chapman \& Hall/CRC Monographs and Surveys in Pure and Applied Mathematics},
  isbn       = {1-58488-046-5},
  mrclass    = {62H10 (62-02 62E15)},
  mrnumber   = {1738933},
  mrreviewer = {Yasuko Chikuse},
  pages      = {x+367},
  timestamp  = {2019.06.04},
}

@Book{Kollo2005Advanced,
  title      = {Advanced multivariate statistics with matrices},
  publisher  = {Springer, Dordrecht},
  year       = {2005},
  author     = {Kollo, T\~{o}nu and von Rosen, Dietrich},
  volume     = {579},
  series     = {Mathematics and Its Applications (New York)},
  isbn       = {978-1-4020-3418-3; 1-4020-3418-0},
  doi        = {10.1007/1-4020-3419-9},
  mrclass    = {62-01 (62E10 62E17 62H12 62J05)},
  mrnumber   = {2162145},
  mrreviewer = {Hengjian Cui},
  pages      = {xvi+489},
  timestamp  = {2019.06.04},
  url        = {https://doi.org/10.1007/1-4020-3419-9},
}

@Article{Werner2008estimation,
  author    = {Werner, Karl and Jansson, Magnus and Stoica, Petre},
  title     = {On estimation of covariance matrices with {K}ronecker product structure},
  journal   = {IEEE Trans. Signal Process.},
  year      = {2008},
  volume    = {56},
  number    = {2},
  pages     = {478--491},
  issn      = {1053-587X},
  doi       = {10.1109/TSP.2007.907834},
  file      = {:Werner2008estimation.pdf:PDF},
  fjournal  = {IEEE Transactions on Signal Processing},
  mrclass   = {94A12 (15A24 62H12)},
  mrnumber  = {2445531},
  timestamp = {2019.06.04},
  url       = {https://doi.org/10.1109/TSP.2007.907834},
}

@Article{Leng2012Sparse,
  author     = {Leng, Chenlei and Tang, Cheng Yong},
  title      = {Sparse matrix graphical models},
  journal    = {J. Amer. Statist. Assoc.},
  year       = {2012},
  volume     = {107},
  number     = {499},
  pages      = {1187--1200},
  issn       = {0162-1459},
  doi        = {10.1080/01621459.2012.706133},
  file       = {:Leng2012Sparse.pdf:PDF},
  fjournal   = {Journal of the American Statistical Association},
  mrclass    = {62H99 (62H12 62J07)},
  mrnumber   = {3010905},
  mrreviewer = {Emanuel H. Ben-David},
  timestamp  = {2019.06.04},
  url        = {https://doi.org/10.1080/01621459.2012.706133},
}

@Article{Yin2012Model,
  author    = {Yin, Jianxin and Li, Hongzhe},
  title     = {Model selection and estimation in the matrix normal graphical model},
  journal   = {J. Multivariate Anal.},
  year      = {2012},
  volume    = {107},
  pages     = {119--140},
  issn      = {0047-259X},
  doi       = {10.1016/j.jmva.2012.01.005},
  file      = {:Yin2012Model.pdf:PDF},
  fjournal  = {Journal of Multivariate Analysis},
  mrclass   = {62H12 (62-09 62J07 62P10)},
  mrnumber  = {2890437},
  timestamp = {2019.06.04},
  url       = {https://doi.org/10.1016/j.jmva.2012.01.005},
}

@Article{Zhao2014Structured,
  author     = {Zhao, Junlong and Leng, Chenlei},
  title      = {Structured {L}asso for regression with matrix covariates},
  journal    = {Statist. Sinica},
  year       = {2014},
  volume     = {24},
  number     = {2},
  pages      = {799--814},
  issn       = {1017-0405},
  fjournal   = {Statistica Sinica},
  mrclass    = {62J07 (62P05)},
  mrnumber   = {3235399},
  mrreviewer = {Girdhar G. Agarwal},
  timestamp  = {2019.06.04},
}

@Article{Zhou2014Regularized,
  author    = {Zhou, Hua and Li, Lexin},
  title     = {Regularized matrix regression},
  journal   = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  year      = {2014},
  volume    = {76},
  number    = {2},
  pages     = {463--483},
  issn      = {1369-7412},
  doi       = {10.1111/rssb.12031},
  fjournal  = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  mrclass   = {62J07 (62H12 62J12)},
  mrnumber  = {3164874},
  timestamp = {2019.06.04},
  url       = {https://doi.org/10.1111/rssb.12031},
}

@InProceedings{Li2016Stochastic,
  author    = {Xingguo Li and Tuo Zhao and Raman Arora and Han Liu and Jarvis Haupt},
  title     = {Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {ICML'16},
  pages     = {917--925},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {JMLR.org},
  abstract  = {We propose a stochastic variance reduced optimization algorithm for solving a class of large-scale nonconvex optimization problems with cardinality constraints, and provide sufficient conditions under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. Numerical experiments demonstrate the efficiency of our method in terms of both parameter estimation and computational performance.},
  acmid     = {3045488},
  file      = {:Li2016Stochastic.pdf:PDF},
  location  = {New York, NY, USA},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=3045390.3045488},
}

@Article{Ohlson2013multilinear,
  author     = {Ohlson, Martin and Rauf Ahmad, M. and von Rosen, Dietrich},
  title      = {The multilinear normal distribution: introduction and some basic properties},
  journal    = {J. Multivariate Anal.},
  year       = {2013},
  volume     = {113},
  pages      = {37--47},
  issn       = {0047-259X},
  doi        = {10.1016/j.jmva.2011.05.015},
  fjournal   = {Journal of Multivariate Analysis},
  mrclass    = {60E05 (62H12 62H99)},
  mrnumber   = {2984354},
  mrreviewer = {Apostolos Batsidis},
  timestamp  = {2019.06.04},
  url        = {https://doi.org/10.1016/j.jmva.2011.05.015},
}

@Article{Manceur2013Maximum,
  author    = {Manceur, Ameur M. and Dutilleul, Pierre},
  title     = {Maximum likelihood estimation for the tensor normal distribution: algorithm, minimum sample size, and empirical bias and dispersion},
  journal   = {J. Comput. Appl. Math.},
  year      = {2013},
  volume    = {239},
  pages     = {37--49},
  issn      = {0377-0427},
  doi       = {10.1016/j.cam.2012.09.017},
  fjournal  = {Journal of Computational and Applied Mathematics},
  mrclass   = {62H12 (62F12)},
  mrnumber  = {2991957},
  timestamp = {2019.06.04},
  url       = {https://doi.org/10.1016/j.cam.2012.09.017},
}

@Article{Lu2011survey,
  author    = {Haiping Lu and Konstantinos N. Plataniotis and Anastasios N. Venetsanopoulos},
  title     = {A survey of multilinear subspace learning for tensor data},
  journal   = {Pattern Recognition},
  year      = {2011},
  volume    = {44},
  number    = {7},
  pages     = {1540--1551},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/pr/LuPV11},
  doi       = {10.1016/j.patcog.2011.01.004},
  timestamp = {2019.06.04},
  url       = {https://doi.org/10.1016/j.patcog.2011.01.004},
}

@InProceedings{Geng2019Partially,
  author    = {Geng, Sinong and Yan, Minhao and Kolar, Mladen and Koyejo, Sanmi},
  title     = {Partially Linear Additive {G}aussian Graphical Models},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2180--2190},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  abstract  = {We propose a partially linear additive Gaussian graphical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model parameters are estimated using an $L_1$-regularized maximal pseudo-profile likelihood estimator (MaPPLE) for which we prove a $\sqrt{n}$-sparsistency. Importantly, our approach avoids parametric constraints on the effects of confounders on the estimated graphical model structure. Empirically, the PLA-GGM is applied to both synthetic and real-world datasets, demonstrating superior performance compared to competing methods.},
  file      = {geng19a.pdf:http\://proceedings.mlr.press/v97/geng19a/geng19a.pdf:PDF},
  timestamp = {2019.06.05},
  url       = {http://proceedings.mlr.press/v97/geng19a.html},
}

@Article{Geng2018Joint,
  author        = {Sinong Geng and Mladen Kolar and Oluwasanmi Koyejo},
  title         = {Joint Nonparametric Precision Matrix Estimation with Confounding},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1810.07147},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-07147},
  eprint        = {1810.07147},
  timestamp     = {2019.06.05},
  url           = {http://arxiv.org/abs/1810.07147},
}

@InProceedings{Karimi2016Linear,
  author    = {Hamed Karimi and Julie Nutini and Mark W. Schmidt},
  title     = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-{\L}ojasiewicz Condition},
  booktitle = {Machine Learning and Knowledge Discovery in Databases - European Conference, {ECML} {PKDD} 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part {I}},
  year      = {2016},
  editor    = {Paolo Frasconi and Niels Landwehr and Giuseppe Manco and Jilles Vreeken},
  volume    = {9851},
  series    = {Lecture Notes in Computer Science},
  pages     = {795--811},
  publisher = {Springer},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/pkdd/KarimiNS16},
  doi       = {10.1007/978-3-319-46128-1\_50},
  timestamp = {2019.06.11},
  url       = {https://doi.org/10.1007/978-3-319-46128-1\_50},
}

@Article{Yu2019Simultaneous,
  author      = {Ming Yu and Varun Gupta and Mladen Kolar},
  title       = {Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching},
  journal     = {arXiv 1905.06261},
  year        = {2019},
  abstract    = {Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyv\"arinen scoring rule. Hyv\"arinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is $\sqrt{n}$-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.},
  date        = {2019-05-15},
  eprint      = {http://arxiv.org/abs/1905.06261v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1905.06261v1:PDF},
  keywords    = {stat.ME},
}

@Article{Kim2019Two,
  author      = {Byol Kim and Song Liu and Mladen Kolar},
  title       = {Two-sample inference for high-dimensional Markov networks},
  journal     = {arXiv 1905.00466},
  year        = {2019},
  abstract    = {Markov networks are frequently used in sciences to represent conditional independence relationships underlying observed variables arising from a complex system. It is often of interest to understand how an underlying network differs between two conditions. In this paper, we develop methodology for performing valid statistical inference for difference between parameters of Markov network in a high-dimensional setting where the number of observed variables is allowed to be larger than the sample size. Our proposal is based on the regularized Kullback-Leibler Importance Estimation Procedure that allows us to directly learn the parameters of the differential network, without requiring for separate or joint estimation of the individual Markov network parameters. This allows for applications in cases where individual networks are not sparse, such as networks that contain hub nodes, but the differential network is sparse. We prove that our estimator is regular and its distribution can be well approximated by a normal under wide range of data generating processes and, in particular, is not sensitive to model selection mistakes. Furthermore, we develop a new testing procedure for equality of Markov networks, which is based on a max-type statistics. A valid bootstrap procedure is developed that approximates quantiles of the test statistics. The performance of the methodology is illustrated through extensive simulations and real data examples.},
  date        = {2019-05-01},
  eprint      = {http://arxiv.org/abs/1905.00466v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1905.00466v1:PDF},
  keywords    = {stat.ME},
}

@Article{Blei2003Latent,
  author    = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
  title     = {Latent Dirichlet Allocation},
  journal   = {Journal of Machine Learning Research},
  year      = {2003},
  volume    = {3},
  pages     = {993--1022},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/jmlr/BleiNJ03},
  timestamp = {2019.06.13},
  url       = {http://jmlr.org/papers/v3/blei03a.html},
}

@Article{Zhang2016Dynamics,
  author    = {Zhang, Zi-Ke and Liu, Chuang and Zhan, Xiu-Xiu and Lu, Xin and Zhang, Chu-Xu and Zhang, Yi-Cheng},
  title     = {Dynamics of information diffusion and its applications on complex networks},
  journal   = {Phys. Rep.},
  year      = {2016},
  volume    = {651},
  pages     = {1--34},
  issn      = {0370-1573},
  doi       = {10.1016/j.physrep.2016.07.002},
  fjournal  = {Physics Reports. A Review Section of Physics Letters},
  mrclass   = {68M10 (05C82 68M11)},
  mrnumber  = {3543858},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1016/j.physrep.2016.07.002},
}

@Book{Floudas2000Deterministic,
  title      = {Deterministic global optimization},
  publisher  = {Kluwer Academic Publishers, Dordrecht},
  year       = {2000},
  author     = {Floudas, Christodoulos A.},
  volume     = {37},
  series     = {Nonconvex Optimization and its Applications},
  isbn       = {0-7923-6014-1},
  note       = {Theory, methods and applications},
  doi        = {10.1007/978-1-4757-4949-6},
  mrclass    = {90-02 (90C26 90C57 90C59)},
  mrnumber   = {1746644},
  mrreviewer = {Harold P. Benson},
  pages      = {xviii+739},
  timestamp  = {2019.06.13},
  url        = {https://doi.org/10.1007/978-1-4757-4949-6},
}

@InProceedings{Hsu2016Online,
  author    = {Wei{-}Shou Hsu and Pascal Poupart},
  title     = {Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  year      = {2016},
  editor    = {Daniel D. Lee and Masashi Sugiyama and Ulrike von Luxburg and Isabelle Guyon and Roman Garnett},
  pages     = {4529--4537},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/HsuP16},
  timestamp = {2019.06.13},
  url       = {http://papers.nips.cc/paper/6077-online-bayesian-moment-matching-for-topic-modeling-with-unknown-number-of-topics},
}

@Article{Yu2018Recovery,
  author      = {Ming Yu and Varun Gupta and Mladen Kolar},
  title       = {Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach},
  journal     = {arxiv:1802.06967},
  year        = {2018},
  abstract    = {We study the problem of recovery of matrices that are simultaneously low rank and row and/or column sparse. Such matrices appear in recent applications in cognitive neuroscience, imaging, computer vision, macroeconomics, and genetics. We propose a GDT (Gradient Descent with hard Thresholding) algorithm to efficiently recover matrices with such structure, by minimizing a bi-convex function over a nonconvex set of constraints. We show linear convergence of the iterates obtained by GDT to a region within statistical error of an optimal solution. As an application of our method, we consider multi-task learning problems and show that the statistical error rate obtained by GDT is near optimal compared to minimax rate. Experiments demonstrate competitive performance and much faster running speed compared to existing methods, on both simulations and real data sets.},
  date        = {2018-02-20},
  eprint      = {http://arxiv.org/abs/1802.06967v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1802.06967v2:PDF},
  keywords    = {stat.ML},
  timestamp   = {2019.06.13},
}

@Book{Hastie2015Statistical,
  title      = {Statistical learning with sparsity},
  publisher  = {CRC Press, Boca Raton, FL},
  year       = {2015},
  author     = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  volume     = {143},
  series     = {Monographs on Statistics and Applied Probability},
  isbn       = {978-1-4987-1216-3},
  note       = {The lasso and generalizations},
  mrclass    = {62-02 (62F15 62G08 62H12 62J05 62J07 62J12 62M15)},
  mrnumber   = {3616141},
  mrreviewer = {Su-Yun Chen Huang},
  pages      = {xv+351},
  timestamp  = {2019.06.13},
}

@Article{Cai2018Comprehensive,
  author    = {HongYun Cai and Vincent W. Zheng and Kevin Chen{-}Chuan Chang},
  title     = {A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications},
  journal   = {{IEEE} Trans. Knowl. Data Eng.},
  year      = {2018},
  volume    = {30},
  number    = {9},
  pages     = {1616--1637},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tkde/CaiZC18},
  doi       = {10.1109/TKDE.2018.2807452},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1109/TKDE.2018.2807452},
}

@Article{Chen2017Fast,
  author        = {Siheng Chen and Sufeng Niu and Leman Akoglu and Jelena Kovacevic and Christos Faloutsos},
  title         = {Fast, Warped Graph Embedding: Unifying Framework and One-Click Algorithm},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1702.05764},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ChenNAKF17},
  eprint        = {1702.05764},
  timestamp     = {2019.06.13},
  url           = {http://arxiv.org/abs/1702.05764},
}

@InProceedings{Leskovec2009Meme,
  author    = {Jure Leskovec and Lars Backstrom and Jon M. Kleinberg},
  title     = {Meme-tracking and the dynamics of the news cycle},
  booktitle = {Proceedings of the 15th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009},
  year      = {2009},
  editor    = {John F. Elder IV and Fran{\c{c}}oise Fogelman{-}Souli{\'{e}} and Peter A. Flach and Mohammed Javeed Zaki},
  pages     = {497--506},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/LeskovecBK09},
  doi       = {10.1145/1557019.1557077},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/1557019.1557077},
}

@Article{Leskovec2016SNAP,
  author    = {Jure Leskovec and Rok Sosic},
  title     = {{SNAP:} {A} General-Purpose Network Analysis and Graph-Mining Library},
  journal   = {{ACM} {TIST}},
  year      = {2016},
  volume    = {8},
  number    = {1},
  pages     = {1:1--1:20},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tist/LeskovecS16},
  doi       = {10.1145/2898361},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/2898361},
}

@Article{Maaten2008Visualizing,
  author    = {van der Maaten, Laurens and Hinton, Geoffrey},
  title     = {Visualizing Data using {t-SNE}},
  journal   = jmlr_s,
  year      = {2008},
  volume    = {9},
  pages     = {2579--2605},
  added-at  = {2015-06-19T12:07:15.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopusz_kdd},
  interhash = {370ba8b9e1909b61880a6f47c93bcd49},
  intrahash = {8b9aebb404ad4a4c6a436ea413550b30},
  keywords  = {dimensionality_reduction tSNE visualization},
  timestamp = {2019.06.13},
  url       = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
}

@InProceedings{Leskovec2005Graphs,
  author    = {Jure Leskovec and Jon M. Kleinberg and Christos Faloutsos},
  title     = {Graphs over time: densification laws, shrinking diameters and possible explanations},
  booktitle = {Proceedings of the Eleventh {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August 21-24, 2005},
  year      = {2005},
  editor    = {Robert Grossman and Roberto J. Bayardo and Kristin P. Bennett},
  pages     = {177--187},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/LeskovecKF05},
  doi       = {10.1145/1081870.1081893},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/1081870.1081893},
}

@Article{Gehrke2003Overview,
  author    = {Johannes Gehrke and Paul Ginsparg and Jon M. Kleinberg},
  title     = {Overview of the 2003 {KDD} Cup},
  journal   = {{SIGKDD} Explorations},
  year      = {2003},
  volume    = {5},
  number    = {2},
  pages     = {149--151},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/sigkdd/GehrkeGK03},
  doi       = {10.1145/980972.980992},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/980972.980992},
}

@InProceedings{Gomez-Rodriguez2011Uncovering,
  author    = {Manuel Gomez{-}Rodriguez and David Balduzzi and Bernhard Sch{\"{o}}lkopf},
  title     = {Uncovering the Temporal Dynamics of Diffusion Networks},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning, {ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
  year      = {2011},
  editor    = {Lise Getoor and Tobias Scheffer},
  pages     = {561--568},
  publisher = {Omnipress},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/Gomez-RodriguezBS11},
  timestamp = {2019.06.13},
  url       = {https://icml.cc/2011/papers/354\_icmlpaper.pdf},
}

@InProceedings{Lozano2010Block,
  author    = {Aurelie C. Lozano and Vikas Sindhwani},
  title     = {Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference},
  booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.},
  year      = {2010},
  editor    = {John D. Lafferty and Christopher K. I. Williams and John Shawe{-}Taylor and Richard S. Zemel and Aron Culotta},
  pages     = {1486--1494},
  publisher = {Curran Associates, Inc.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/LozanoS10},
  timestamp = {2019.06.13},
  url       = {http://papers.nips.cc/paper/3993-block-variable-selection-in-multivariate-regression-and-high-dimensional-causal-inference},
}

@InProceedings{Netrapalli2012Learning,
  author    = {Praneeth Netrapalli and Sujay Sanghavi},
  title     = {Learning the graph of epidemic cascades},
  booktitle = {{ACM} {SIGMETRICS/PERFORMANCE} Joint International Conference on Measurement and Modeling of Computer Systems, {SIGMETRICS} '12, London, United Kingdom, June 11-15, 2012},
  year      = {2012},
  editor    = {Peter G. Harrison and Martin F. Arlitt and Giuliano Casale},
  pages     = {211--222},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/sigmetrics/NetrapalliS12},
  doi       = {10.1145/2254756.2254783},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/2254756.2254783},
}

@InProceedings{Gao2016Periodic,
  author    = {Zuguang Gao and Xudong Chen and Ji Liu and Tamer Basar},
  title     = {Periodic behavior of a diffusion model over directed graphs},
  booktitle = {55th {IEEE} Conference on Decision and Control, {CDC} 2016, Las Vegas, NV, USA, December 12-14, 2016},
  year      = {2016},
  pages     = {37--42},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/cdc/GaoC0B16},
  doi       = {10.1109/CDC.2016.7798243},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1109/CDC.2016.7798243},
}

@InProceedings{Gomez-Rodriguez2010Inferring,
  author    = {Manuel Gomez{-}Rodriguez and Jure Leskovec and Andreas Krause},
  title     = {Inferring networks of diffusion and influence},
  booktitle = {Proceedings of the 16th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, July 25-28, 2010},
  year      = {2010},
  editor    = {Bharat Rao and Balaji Krishnapuram and Andrew Tomkins and Qiang Yang},
  pages     = {1019--1028},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/Gomez-RodriguezLK10},
  doi       = {10.1145/1835804.1835933},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/1835804.1835933},
}

@InProceedings{Gomez-Rodriguez2012Influence,
  author    = {Manuel Gomez{-}Rodriguez and Bernhard Sch{\"{o}}lkopf},
  title     = {Influence Maximization in Continuous Time Diffusion Networks},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning, {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  year      = {2012},
  publisher = {icml.cc / Omnipress},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/Gomez-RodriguezS12},
  timestamp = {2019.06.13},
  url       = {http://icml.cc/2012/papers/189.pdf},
}

@InProceedings{Du2012Learning,
  author    = {Nan Du and Le Song and Alexander J. Smola and Ming Yuan},
  title     = {Learning Networks of Heterogeneous Influence},
  booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States.},
  year      = {2012},
  editor    = {Peter L. Bartlett and Fernando C. N. Pereira and Christopher J. C. Burges and L{\'{e}}on Bottou and Kilian Q. Weinberger},
  pages     = {2789--2797},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/DuSSY12},
  timestamp = {2019.06.13},
  url       = {http://papers.nips.cc/paper/4582-learning-networks-of-heterogeneous-influence},
}

@InProceedings{Kempe2003Maximizing,
  author    = {David Kempe and Jon M. Kleinberg and {\'{E}}va Tardos},
  title     = {Maximizing the spread of influence through a social network},
  booktitle = {Proceedings of the Ninth {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24 - 27, 2003},
  year      = {2003},
  editor    = {Lise Getoor and Ted E. Senator and Pedro M. Domingos and Christos Faloutsos},
  pages     = {137--146},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/KempeKT03},
  doi       = {10.1145/956750.956769},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/956750.956769},
}

@InProceedings{Gruhl2004Information,
  author    = {Daniel Gruhl and Ramanathan V. Guha and David Liben{-}Nowell and Andrew Tomkins},
  title     = {Information diffusion through blogspace},
  booktitle = {Proceedings of the 13th international conference on World Wide Web, {WWW} 2004, New York, NY, USA, May 17-20, 2004},
  year      = {2004},
  editor    = {Stuart I. Feldman and Mike Uretsky and Marc Najork and Craig E. Wills},
  pages     = {491--501},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/www/GruhlGLT04},
  doi       = {10.1145/988672.988739},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/988672.988739},
}

@Article{Guille2013Information,
  author    = {Adrien Guille and Hakim Hacid and C{\'{e}}cile Favre and Djamel A. Zighed},
  title     = {Information diffusion in online social networks: a survey},
  journal   = {{SIGMOD} Record},
  year      = {2013},
  volume    = {42},
  number    = {2},
  pages     = {17--28},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/sigmod/GuilleHFZ13},
  doi       = {10.1145/2503792.2503797},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/2503792.2503797},
}

@InProceedings{Pouget-Abadie2015Inferring,
  author    = {Jean Pouget{-}Abadie and Thibaut Horel},
  title     = {Inferring Graphs from Cascades: {A} Sparse Recovery Framework},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  year      = {2015},
  editor    = {Francis R. Bach and David M. Blei},
  volume    = {37},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {977--986},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/Pouget-AbadieH15},
  timestamp = {2019.06.13},
  url       = {http://proceedings.mlr.press/v37/pouget-abadie15.html},
}

@InProceedings{Du2013Scalable,
  author    = {Nan Du and Le Song and Manuel Gomez{-}Rodriguez and Hongyuan Zha},
  title     = {Scalable Influence Estimation in Continuous-Time Diffusion Networks},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.},
  year      = {2013},
  editor    = {Christopher J. C. Burges and L{\'{e}}on Bottou and Zoubin Ghahramani and Kilian Q. Weinberger},
  pages     = {3147--3155},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/DuSGZ13},
  timestamp = {2019.06.13},
  url       = {http://papers.nips.cc/paper/4857-scalable-influence-estimation-in-continuous-time-diffusion-networks},
}

@InProceedings{Du2013Uncover,
  author    = {Nan Du and Le Song and Hyenkyun Woo and Hongyuan Zha},
  title     = {Uncover Topic-Sensitive Information Diffusion Networks},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013},
  year      = {2013},
  volume    = {31},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {229--237},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/DuSWZ13},
  timestamp = {2019.06.13},
  url       = {http://proceedings.mlr.press/v31/du13a.html},
}

@Article{Eagle2009Inferring,
  author    = {N. Eagle and A. Pentland and D. Lazer},
  title     = {Inferring friendship network structure by using mobile phone data},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2009},
  volume    = {106},
  number    = {36},
  pages     = {15274--15278},
  month     = {aug},
  doi       = {10.1073/pnas.0900282106},
  publisher = {Proceedings of the National Academy of Sciences},
  timestamp = {2019.06.13},
}

@InProceedings{Zhou2013Learning,
  author    = {Ke Zhou and Hongyuan Zha and Le Song},
  title     = {Learning Social Infectivity in Sparse Low-rank Networks Using Multi-dimensional Hawkes Processes},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013},
  year      = {2013},
  volume    = {31},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {641--649},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/ZhouZS13},
  timestamp = {2019.06.13},
  url       = {http://proceedings.mlr.press/v31/zhou13a.html},
}

@InProceedings{Myers2012Information,
  author    = {Seth A. Myers and Chenguang Zhu and Jure Leskovec},
  title     = {Information diffusion and external influence in networks},
  booktitle = {The 18th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, {KDD} '12, Beijing, China, August 12-16, 2012},
  year      = {2012},
  editor    = {Qiang Yang and Deepak Agarwal and Jian Pei},
  pages     = {33--41},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/MyersZL12},
  doi       = {10.1145/2339530.2339540},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/2339530.2339540},
}

@InProceedings{Myers2012Clash,
  author    = {Seth A. Myers and Jure Leskovec},
  title     = {Clash of the Contagions: Cooperation and Competition in Information Diffusion},
  booktitle = {12th {IEEE} International Conference on Data Mining, {ICDM} 2012, Brussels, Belgium, December 10-13, 2012},
  year      = {2012},
  editor    = {Mohammed Javeed Zaki and Arno Siebes and Jeffrey Xu Yu and Bart Goethals and Geoffrey I. Webb and Xindong Wu},
  pages     = {539--548},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icdm/MyersL12},
  doi       = {10.1109/ICDM.2012.159},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1109/ICDM.2012.159},
}

@InProceedings{He2015HawkesTopic,
  author    = {Xinran He and Theodoros Rekatsinas and James R. Foulds and Lise Getoor and Yan Liu},
  title     = {HawkesTopic: {A} Joint Model for Network Inference and Topic Modeling from Text-Based Cascades},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  year      = {2015},
  editor    = {Francis R. Bach and David M. Blei},
  volume    = {37},
  series    = {{JMLR} Workshop and Conference Proceedings},
  pages     = {871--880},
  publisher = {JMLR.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icml/HeRFGL15},
  timestamp = {2019.06.13},
  url       = {http://proceedings.mlr.press/v37/he15.html},
}

@Article{Bonchi2011Influence,
  author    = {Francesco Bonchi},
  title     = {Influence Propagation in Social Networks: {A} Data Mining Perspective},
  journal   = {{IEEE} Intelligent Informatics Bulletin},
  year      = {2011},
  volume    = {12},
  number    = {1},
  pages     = {8--16},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/cib/Bonchi11},
  timestamp = {2019.06.13},
  url       = {http://www.comp.hkbu.edu.hk/\%7Eiib/2011/Dec/article1/iib\_vol12no1\_article1.pdf},
}

@InProceedings{Liu2012Time,
  author    = {Bo Liu and Gao Cong and Dong Xu and Yifeng Zeng},
  title     = {Time Constrained Influence Maximization in Social Networks},
  booktitle = {12th {IEEE} International Conference on Data Mining, {ICDM} 2012, Brussels, Belgium, December 10-13, 2012},
  year      = {2012},
  editor    = {Mohammed Javeed Zaki and Arno Siebes and Jeffrey Xu Yu and Bart Goethals and Geoffrey I. Webb and Xindong Wu},
  pages     = {439--448},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/icdm/LiuCXZ12},
  doi       = {10.1109/ICDM.2012.158},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1109/ICDM.2012.158},
}

@InProceedings{Gomez-Rodriguez2013Structure,
  author    = {Manuel Gomez{-}Rodriguez and Jure Leskovec and Bernhard Sch{\"{o}}lkopf},
  title     = {Structure and dynamics of information pathways in online media},
  booktitle = {Sixth {ACM} International Conference on Web Search and Data Mining, {WSDM} 2013, Rome, Italy, February 4-8, 2013},
  year      = {2013},
  editor    = {Stefano Leonardi and Alessandro Panconesi and Paolo Ferragina and Aristides Gionis},
  pages     = {23--32},
  publisher = {{ACM}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/wsdm/Gomez-RodriguezLS13},
  doi       = {10.1145/2433396.2433402},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1145/2433396.2433402},
}

@Article{Gomez-Rodriguez2016Estimating,
  author    = {Manuel Gomez{-}Rodriguez and Le Song and Hadi Daneshmand and Bernhard Sch{\"{o}}lkopf},
  title     = {Estimating Diffusion Networks: Recovery Conditions, Sample Complexity and Soft-thresholding Algorithm},
  journal   = {Journal of Machine Learning Research},
  year      = {2016},
  volume    = {17},
  pages     = {90:1--90:29},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/jmlr/Gomez-Rodriguez16},
  timestamp = {2019.06.13},
  url       = {http://jmlr.org/papers/v17/14-430.html},
}

@Article{Jiang2014Evolutionary,
  author    = {Chunxiao Jiang and Yan Chen and K. J. Ray Liu},
  title     = {Evolutionary Dynamics of Information Diffusion Over Social Networks},
  journal   = {{IEEE} Trans. Signal Processing},
  year      = {2014},
  volume    = {62},
  number    = {17},
  pages     = {4573--4586},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tsp/JiangCL14},
  doi       = {10.1109/TSP.2014.2339799},
  timestamp = {2019.06.13},
  url       = {https://doi.org/10.1109/TSP.2014.2339799},
}

@Article{Loh2017Statistical,
  author     = {Loh, Po-Ling},
  title      = {Statistical consistency and asymptotic normality for high-dimensional robust {$M$}-estimators},
  journal    = {Ann. Statist.},
  year       = {2017},
  volume     = {45},
  number     = {2},
  pages      = {866--896},
  issn       = {0090-5364},
  doi        = {10.1214/16-AOS1471},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62F12 (62F35 62H12)},
  mrnumber   = {3650403},
  mrreviewer = {Zaixing Li},
  timestamp  = {2019.06.14},
  url        = {https://doi.org/10.1214/16-AOS1471},
}

@Article{Loh2015Regularized,
  author    = {Loh, Po-Ling and Wainwright, Martin J.},
  title     = {Regularized {$M$}-estimators with nonconvexity: statistical and algorithmic theory for local optima},
  journal   = {J. Mach. Learn. Res.},
  year      = {2015},
  volume    = {16},
  pages     = {559--616},
  issn      = {1532-4435},
  fjournal  = {Journal of Machine Learning Research (JMLR)},
  mrclass   = {62H12 (62F07 62J07 90C90)},
  mrnumber  = {3335800},
  timestamp = {2019.06.14},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:[mkolar:]\;2\;1\;\;\;\;;
}
